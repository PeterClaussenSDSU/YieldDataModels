\documentclass[phdcss]{SDSUThesis}
%\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}

\usepackage{natbib}
\usepackage[nottoc,notbib]{tocbibind}

%to get the tilde in \texttt environments
\usepackage[T1]{fontenc}

%I keep getting a fancyhdr warning, this should suppress it
\setlength{\footskip}{4.35004pt}

\renewcommand{\cfttoctitlefont}{\hfill\bfseries}    %TOC Title Font
\renewcommand{\cftloftitlefont}{\hfill\bfseries}    %LOF Title Font
\renewcommand{\cftlottitlefont}{\hfill\bfseries}    %LOT Title font
\renewcommand{\cftaftertoctitle}{\hfill}            %%
\renewcommand{\cftafterloftitle}{\hfill}            %% make titles centered.
\renewcommand{\cftafterlottitle}{\hfill}            %%
\renewcommand{\cftpartfont}{\bfseries}              %Part font
\renewcommand{\cftpartpagefont}{\bfseries}          %Part page# font
\renewcommand{\cftchapfont}{\bfseries}              %Chapter font
\renewcommand{\cftchappagefont}{\bfseries}          %Chapter page# font
% vertical spacing overrides...the default gaps drove me nuts.
\renewcommand{\cftbeforechapskip}{0em}
\renewcommand{\cftbeforepartskip}{0em}
\chapterfont{\centering\nohang\normalsize\bfseries}
\sectionfont{\normalsize\bfseries}


% from https://tex.stackexchange.com/questions/521422/equation-with-numbering-in-table
% for CompFormula table
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{siunitx}

\usepackage{{makecell}}
\setcellgapes{3pt}\makegapedcells

\usepackage{array,collcell}
\newcommand\AddLabel[1]{%
  \refstepcounter{equation}% increment equation counter
  (\theequation)% print equation number
  \label{#1}% give the equation a \label
}
\newcolumntype{M}{>{\hfil$\displaystyle}X<{$\hfil}} % mathematics column
\newcolumntype{L}{>{\collectcell\AddLabel}r<{\endcollectcell}}


\title{Functional Data Analysis and Spatial Analysis for On-farm Strip Trials}
\author{Peter Claussen}
\docmonth{August}
\docyear{2024}
\docadvisor{Jung-Han Kimn}    % dissertation/thesis advisor name
\depthead{Eun Heui Kim}     % dept head name
\dept{Mathematics \& Statistics}    % department name

%\bibliographystyle{plain}

\begin{document}

\frontmatter

\maketitle

\acceptancepage

\begin{acknowledgments}
\end{acknowledgments}

\begin{abstract}
To be written
\end{abstract}

\renewcommand{\contentsname}{Table of Contents}
\clearpage
\phantomsection
\pdfbookmark[0]{\contentsname}{contents}
\tableofcontents

\clearpage

\listoftables

\clearpage
\listoffigures

\mainmatter

\clearpage
\phantomsection 
\addcontentsline{toc}{part}{Chapters}

\SweaveOpts{concordance=TRUE}

%This document draws from
%FunctionalForms.Rmd
%FunctionalAnalysisStripTrials.Rmd
%FunctionalAnalysisRegression.Rmd
%Spatial Analysis of a Strip Trial
%Spatial Analysis of a Strip Trial Trimmed

<<ImportLibraries, echo = FALSE, results=hide>>=
library(splines)
library(ggplot2)
library(gridExtra)
library(mgcv)
library(fda)
library(brms)
library(gstat)


cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#0072B2", "#D55E00", "#F0E442","#CC79A7","#000000","#734f80", "#2b5a74", "#004f39", "#787221", "#003959", "#6aaf00", "#663cd3")
#grey <- cbPalette[1] #"#999999"
#orange <- cbPalette[2] #"#E69F00"
#skyblue <- cbPalette[3] #"#56B4E9"
#bluishgreen <- cbPalette[4] #"#009E73"
yellow <- cbPalette[7] #"#F0E442"
blue <- cbPalette[5] #"#0072B2"
vermillion <- cbPalette[6] #"#D55E00"
pair.colors <- c(2,5)
@

%Begin FunctionalAnalysisStripTrials 


%% See ASA_CSSA_SSSA/2018/ThoughtsAboutPower -->

% Data used for this thesis
<<LoadandProcessExample1, echo=FALSE>>=
load(file="~/Work/git/ASA_CSSA_SSSA/2017/Workshop/Case Study 2/Strips.Rda")
#rename data table to match our naming for this code
Example1.dat <- EastQuarter.dat
Example1ForHistogram <- data.frame(Yield =Example1.dat[,'Yield'])
remove(EastQuarter.dat)
Example1.dat[,'Pass'] <- floor(Example1.dat[,'Northing']/6)+1
Example1.dat[,'Pair'] <- floor((Example1.dat[,'Pass']+1)/2)
#Example1.dat[,'Direction'] <- as.factor(Example1.dat[,'Direction'])

Example1.dat[,'PassNo'] <- Example1.dat[,'Pass']

Example1.dat[,'Pass'] <- as.factor(Example1.dat[,'Pass'])
Example1.dat[,'Pair'] <- as.factor(Example1.dat[,'Pair'])
@

% Read and process data for example 2
<<LoadandProcessExample2, echo=FALSE>>=
Example2.dat <- read.csv(file='../ManagementZoneML/data/B 2019 Soybean Treated.csv')
left = 1000
Example2.dat <- Example2.dat[Example2.dat[,'Longitude'] >left,]
#the original data was anonymized to the southwest corner of 'Home'. After trimming the odd oblong shape, we standardize back to zero
Example2.dat[,'Longitude'] <- Example2.dat[,'Longitude'] - min(Example2.dat[,'Longitude'])
Example2.dat[,'Latitude'] <- Example2.dat[,'Latitude'] - min(Example2.dat[,'Latitude'])
Example2ForHistOriginal <- data.frame(Yield=Example2.dat[,"Yield"])
#remove outliers
Example2.dat <- Example2.dat[Example2.dat[,'Yield']<80,]
Example2.dat <- Example2.dat[Example2.dat[,'Yield']>0,]

Example2ForHistCleaned <- data.frame(Yield=Example2.dat[,"Yield"])
#data were read into QGIS and points labelled with Block
#Blocks 1-8 should be analyzed as strips, with even numbered blocks being sprayed
Example2.dat <- Example2.dat[Example2.dat[,'Block'] %in% 1:8,]
#keep only those harvest strips that are marked as sample
Example2.dat <- Example2.dat[Example2.dat[,'Sample']>0,]
Example2.dat[,'Strip'] <- Example2.dat[,'Block']
Example2.dat[,'Sprayed'] <- Example2.dat[,'Block'] %in% c(2,4,6,8)
Example2.dat[,'Pass'] <- factor(Example2.dat[,'Block']):factor(Example2.dat[,'Sample'])
Example2.dat[,'Northing'] <- Example2.dat[,'Latitude']-min(Example2.dat[,'Latitude'])
Example2.dat[,'Easting'] <- Example2.dat[,'Longitude']-min(Example2.dat[,'Longitude'])
@

\chapter{Overview}\label{ChapOverview}

We consider the problem of on-farm participatory research. On farm trials have a long history, and one popular model is to provide a farmer-collaborator with a small amount of product (for example, fungicide) to test on a portion of crop lands. The work presented in this thesis works toward a simple goal - can we determine with some degree of statistical rigor if the product application was effective? The area of design and analysis of on-farm participatory research has not undergone the degree of development, with respect to experimental design and analysis, as the more traditional "small-plot" trials, which have been conducted worldwide in some form for more than a century. The material in this thesis is presented as a contribution toward the development of methods for analysis on-farm strip trials.

\section{Chapter Outline}

In Chapter \ref{ChapOnFarmStripTrials} we first review the literature of on-farm strip trial analysis and discuss the nature of yield monitor data, followed by a chapter (Chapter \ref{RepresentativeStripTrialsChapter}) describing two representative strip trials that will serve as sources for example data analyzed in this thesis. We introduce data before completing the relevant literature review so that we can use examples of functional data from representative trials to motivate later chapters.

Chapter \ref{UnivariateAndFunctionalDataAnalysisChapter} describes the similarity to traditional "univariate" data analysis to pointwise functional data analysis. We then consider in Chapter \ref{SmoothingFunctionsChapter}  how temporal data collected during harvest (harvest passes) can be modeled using spline bases functions. 

Chapter \ref{FunctionalDataAnalysisChapter} demonstrates the application of pointwise functional data analysis (paired sample $t$-tests) using the smoothed harvest passes as observational units. Chapter \ref{TrendAnalysisChapter} complements Chapter \ref{FunctionalDataAnalysisChapter} by applying a pointwise trend analysis in place of pointwise $t$-tests. Chapter \ref{SpatialAnalysisChapter} extends the pointwise trend analysis over smoothed yield monitor curves in Chapter \ref{TrendAnalysisChapter} to a two dimensional response surface analysis of strip trials, where smoothing criteria is based on the properties of spatial random fields. Finally, Chapter \ref{ConclusionsChapter} presents a summary of the analysis methods presented in this thesis.

\chapter{On-farm strip trials}\label{ChapOnFarmStripTrials}

\section{Overview of on-farm trials}

Collaborative research between agronomists and farmers has a long history \cite{doerge-07-1999, kyveryga-11-2019, riley-03-2001} In the internet era, collaboration with, and dissemination of information to farmers has greatly increased. For example the Nebraska On-Farm Research Network \cite{agriculture-2024} which allows users to browse on-farm trials dating to 1990.  Precision agriculture (PA) technologies have greatly increased opportunities for on-farm research (for example, \cite{lawes.r-03-2012, taylor-08-2011}).


In particular, we are interested in collaborative trials that are largely conducted by the farmer, independent of researcher intervention. In such cases, farmers are given a sample of a new technology - crop varieties, pesticides, etc - and encouraged to apply the new technology to a small portion of their acreage to demonstrate the effectiveness of the new technology. The farmer benefits by being able to see for himself the effectiveness (or lack thereof) of the new technology, while the researcher benefits from being able to pool the results from multiple collaborators. 


In this paper, we propose methods specifically to analyze single on-farm strip trials \cite{network-2023, hicks-1997, kyveryga-06-2018}. On-farm strip trials can have an advantage over more traditional small plot trials with respect to statistical power in detecting differences among certain kinds of treatments \cite{kandel-2018, li-09-2022, laurent-2022}. Our ultimate goal is to provide a presentation of the results of a strip trial to collaborators that can provide some statistical support to answer the question "Did the new product work well enough in the strip trial for the collaborator to adopt the new technology for application to a larger percent of his total acreage?"

For our purposes, we will define on-farm research as comparative experiments such that

\begin{enumerate}
 \item trials are conducted on farmer-managed crop-lands
 \item treatments are applied using farmer-owned equipment  
 \item data is collected using farmer-owned harvesters. 
\end{enumerate}

We will use the term crop-land for an area of ground planted and harvested by a farmer. The term "field" is more commonly used, but we will also be discussing mathematical entities known as "random fields", so we will be using "field" to imply the latter context, and "crop-land" for the former.

Further, items 2) and 3) imply the use of GPS-enabled technology to record placement of experimental treatments and the corresponding crop yields, 2) determines the size of the experimental unit, while 3) determines the size of the observational unit. Specifically, the dimensions of the experimental units will be determined by the width of farm implements used to apply treatment and by the length of a typical application pass. In many cases, the width of the application implement is greater than the width of the harvester, so there may be multiple harvest passes per application pass. 

We will consider a \textit{pass} as a simple complete farming operation, e.g. the interval between when a planter is lowered into the soil at one end of the field to when the planter is raised at the other end of the field, when a sprayer starts spraying at one of the field and continues to the opposite end of the field, or when a harvester enters unharvested crop at one end of the field and continues in a roughly linear motion to the opposite end. Because of differences in planter or sprayer and harvester widths, there may be multiple harvester passes per planter or sprayer pass. However, experimental products are usually applied at planting time, or at some interval during the growth season, so the basic experimental unit, the \textit{strip} consists of a single pass of a planter or sprayer. Thus, in our analysis of strip trials, we will make the distinction between harvest passes and experimental strips.

In practice, a pass as analyzed may be trimmed to exclude the end-of-row headlands, which may include overlapping applications \cite{bullock-11-2019}. Strips should also ideally be applied to inner rows only to avoid side of field effects \cite{bullock-11-2019}. We recognize that there is inherent spatial variability in the accuracy of yield monitors \cite{gauci-12-2023} and that yield monitor data frequently contains artifacts (\cite{marchant-2019}) but an appropriate choice of interpolation of yield points can minimize point-to-point variability.


Each strip may be thought of in terms of a traditional plot, albeit a very long and very thin plot, and in some cases the results of a strip trial is presented in a manner analogous to traditional small-plot trials \cite{kandel-2018, laurent-2022}. However, a strip in the shape a very long and very thin rectangle becomes increasingly similar to a single line, and we can easily model yield as function of position on this line. Thus, the analysis of yield monitor data from strip trials is a likely candidate 
for functional data analysis, and we will devote much of this thesis on the development of methods of analyzing strip trials as functional data.


Strictly speaking, on-farm strip trials do no necessarily require the use of GPS enabled harvesters. In the past, weigh wagons have been used to measure yield of strips in on-farm participatory testing - \cite{nelson-06-2015, wuest-1994, yan-03-2002} - for example. However, this method of determine strip yield is not suitable to analysis as functional data; while such methods have been in common use and are valid in context, we will ignore such experiments for our proposed analysis. 


We recognize that some on-farm experiment can be conducted as single strips; the methods discussed in this thesis assumes multiple strips. Other methods have been proposed for the analysis of single-strip trials \cite{hatfield-06-2018, cho-2021}. We recognize that on-farm strip trials may be unreplicated within a single location, but analyzed across multiple locations (\cite{schmidt-04-2018, yan-03-2002}). We also recognize that on-farm trials may be paired with research station-based traditional small-plot experiments, as with mother-baby trials \cite{virk-2008}.

Bramley, et. al \cite{bramley-10-2022}, discuss on-farm experimentation in the context of checkerboard pattern arrangements. They discuss pros and cons of rigorous statistical analysis of on-farm experiments, but in general are sympathetic to the interpretability of experimental results, particularly in the context of farmer-collaborator observations of the experimental treatments. As their title suggests, they emphasize the "farmer-centric" approach to on-farm experiments, and the software used to present and analyze research results. Tanaka, et. al, \cite{tanaka-05-2023} describe what might also be called a checkerboard pattern, but buffer space is included between the individual experimental units, unlike \cite{bramley-10-2022}. Tanaka, et. al, describe a fertilizer trial where response is analysed using regression models. While not in strips, thus not compatible with the functional data analysis methods described in this thesis, arrangements of this sort may be amenable to the spatial analysis described in the latter section.

\section{Overview of yield monitor data}\label{YieldMonitorOverview}

Yield is recorded in units of bushels per acre, but this requires the use of several sensors in the harvester \cite{luck-2014, whelan-2013}. Grain flow sensors measure the rate at which grain moves within the harvester. Typically, there are two types of sensors in used; impact-type which measures the force of grain hitting a metal plate connected to a strain gauge or potentiometer, and volumetric-type sensors which measure the volume of grain as it is lifted inside the harvester in an elevator. Flow rate is typically recorded at one second intervals as in our example data.

Yield in bushels per acre is usually standardized to a specific grain moisture content, so to properly record yield, harvesters are equipped with grain moisture sensors. There are many types of moisture sensors, but in general, grain moisture is not sampled at the same rate as grain flow; in our example data, moisture measurements are taken at 10-15 second intervals. Moisture is typically reported as a percent water. Yield flow rate volumes are recorded as "wet" weight, and yield monitor software uses to covert grain flow to a standardized "dry" weight.

Harvesters are equipped with ground speed sensors, which measure the speed of the harvester as it travels through the standing crops. This, combined with swath width or cut width (meters or feet, depending on the specific yield monitor), is used by yield monitor software to convert grain flow in weight per second to weight per area, resulting in a dry yield volume. Yield is converted according to the following formula  \cite{luck-2014} :
\begin{equation}
Yield \left( \frac{bushels}{acre} \right) = 43560 \left( \frac{m \times t}{d \times w \times p} \right) \left( \frac{100-MC_{harvest}}{100-MC_{market}} \right)
\end{equation}
where
\begin{itemize}
\item $m$ = mass flow rate ($lb/sec$)
\item $t$ = logging interval of the yield monitoring system ($sec$)
\item $d$ = distance traveled between logged data points ($ft$)
\item $w$ = header cut width setting ($ft$)
\item $p$ = grain density or test weight ($lb/bu$)
\item $MC_{harvest}$ = moisture content from yield monitor sensor (wet weight) ($\%$)
\item $MC_{market}$ = marketable moisture content (dry weight) ($\%$)
\item $43560$ = conversion from $ft^2$ to acres.
\end{itemize}

Yield is mapped to specific geographic coordinates through GPS sensors, which associated a longitude and latitude with each recorded grain flow data point. There is a lag between the time the grain enters the combine and when the grain reaches the grain flow sensor; it is usually possible for operators to adjust lag time to improve the accuracy of GPS coordinates associated with yield points. 

\section{A survey of methods to analyze on-farm trials}

Piepho, et al, \cite{piepho-11-2011} proposes a method of analysis of strip trials, according to the following algorithm:
\begin{enumerate}
  \item overall assessment of the treatments based on plot-averages
  \item overall assessment of the treatments based on subsamples without taking geo-referencing into account
  \item inspection of residuals from step (2) for associations with covariates
  \item overall assessment of the treatments based on spatial modelling of geo-reference subsamples with and without taking covariates into account
  \item spatial analysis of all geo-referenced subsamples based on categorized deflect angles with and without taking covariates into account.
\end{enumerate}

This method involved dividing the field into 30m grid, with each grid designated as a plot This method does not consider strips as experimental entities, instead the data were averaged and centered on the midpoint each grid, then geospatial methods were applied. This method does have the advantage of incorporation of geospatial information such as EC.

\cite{lawes.r-03-2012} produces graphic displays similar to those presented here. However, instead of finding functional forms for the data, the data were divided into a are regular 5 meter grid orf $n$ rows by $m$ columns, and pairwise $t$ tests was performed between two rows by subsetting the original $n \times m$ array into groups of 5 consecutive columns and two rows. The process was repeated by moving the position where the five columns were selected, for a total of $m-5$ total $t$-tests. This method was considered analogous to a moving window regression, with the exception that the windows were used for $t$-tests. This method seems fundamentally flawed when we consider the five columns used a replicates are not independent experimental units; as described, the analysis method used only two rows.

The Iowa Soybean Association On-Farm Research Network provides simple $t$-test based analysis of on-farm trials, but also provides a online, interactive tool (ISOFAST)\cite{laurent-11-2019} for summarizing results of multiple on-farm trials. They generally ignore spatial effects at the field level, but with ISOFAST allow broader geographical tests of the on-farm effectiveness of a variety of agricultural products and practices.


\cite{evans-11-2020} used geographically weighted regression for the analysis of strip trials. This method has the advantage of partitioning variance in strip trials into spatial variability and treatment components. \cite{stefanova-04-2023} used a clustering method to delineate strips into comparable pseudo-environments, then analyzed the resulting clusters as multi-environment trials using linear mixed models. As with ISOFAST, these methods sacrifice local spatial analysis for the good of a broader geographical spatial analysis.

On-farm participatory trials are not limited to strip trials. The Dataâ€Intensive Farm Management Project (DIFM) \cite{bullock-11-2019} combines randomized blocks of systematically randomized variable rates of inputs in grids. Spatial analysis of strip-plot and split-field trials has also been used with some success \cite{griffin-08-2008}. 


The examples used to present a method of functional analysis are better understood as systematic strip arrangements, as opposed to more traditionally randomized trials. Randomization is key if causal effects are to be investigated, but randomization may not always be practical, so in many cases farmers are recommended to use systematic strips. If trends are accounted for, systematic trials can be effective \cite{alesso-11-2019, cao-06-2022}. In the following chapter, we consider trend analysis incorporated into functional data analysis.

\chapter{Representative strip trials}\label{RepresentativeStripTrialsChapter}

\section{Overview}

We will be using two distinctly differ data sets as representative of on-farm strip trials. The first example was executed as a uniform rectangle with uniformly-sized experimental units and uniform crop stand throughout. This represents a best-case scenario for an on-farm trial. 

The second example was executed in a cropland that was not a simple rectangle, such that experimental units are not of the same length. This cropland also had areas that were not planted, making this experiment something of a worse-case scenario for obtaining meaningful results that can be communicated to the farmer-collaborator. 

\section{Data conversion from GPS formatted files to R compatible format}

We use data from two different trials to illustrate functional data analysis of strip trials. The two trials were conducted independently by different farmer-collaborators, using different harvest equipment. Different yield monitor manufacturers use different native file formats for GPS-tagged yield data. Ag Leader SMS software \cite{agleader} was used to convert the native data from Example 1 to GIS shapefile; the yield data for Example 2 was exported to GIS shapefile format from John Deere Operations Center \cite{myjohndeere}. 

Shapefiles were read into QGIS \cite{qgis} then exported to CSV format to be imported into R \cite{rcore}. Unless otherwise stated, all data manipulation and statistical analysis were performed using R. Before statistical analysis, the data were anonymized by converting GPS coordinates, which can be used to identify the specific collaborators field, to distance in meters from an origin point. The origin was found by subtracting the minimum latitude or longitude in corresponding data columns from each of the coordinates associated with a yield data point. 

GPS distance units were converted to meters based on the haversine formula \cite{robusto-01-1957} where degrees of latitude or longitude are converted to metric unit by first establishing a midpoint latitude ($lat_{mid}$)for the data to be converted, then calculating meters per degree (of latitude or longitude) according to
\begin{equation}
   \begin{aligned}
      m/deg_{lat} & = 111132.954 - 559.822 * cos( 2.0 * lat_{mid} ) + 1.175 * cos( 4.0 * lat_{mid}) \\
      m/deg_{lon} &= (\pi/180 ) * 6367449 * cos ( lat_{mid} )
   \end{aligned}
\end{equation}
Geotagged yield points in GPS units for latitude and longitude where then multiplied by the appropriate conversion factor to compute geographical tags denoted \texttt{Northing} and \texttt{Easting}. The conversion was computed using an R function adapted from code obtained from \texttt{https://stackoverflow.com/questions/639695/how-to-convert-latitude-or-longitude-to-meters}

%<<add.metric.function, eval = FALSE>>=
%add.metric <- function(data, origin=c(NA,NA)) {
%  #adapted from https://stackoverflow.com/questions/639695/how-to-convert-latitude-or-longitude-to-meters
%  if(any(is.na(origin))) {
%    origin[1] <- min(data[,'X'])
%    origin[2] <- min(data[,'Y'])
%  }
%  data[,'Easting'] <- data[,'X'] - origin[1]
%  data[,'Northing'] <- data[,'Y'] - origin[2]
%  latMid <- (min(data[,'Y']) + max(data[,'Y']))/2
%  m_per_deg_lat = 111132.954 - 559.822 * cos( 2.0 * latMid ) + 1.175 * cos( 4.0 * latMid)
%  m_per_deg_lon = (3.14159265359/180 ) * 6367449 * cos ( latMid )
%  data[,'Easting'] <- data[,'Easting']*m_per_deg_lon
%  data[,'Northing'] <- data[,'Northing']*m_per_deg_lat
%  return(data)
%}
%@

The final version of the data include three columns that are of interest for the proposed analysis:
\texttt{Easting} as the number of meters from the lower left (southwest) corner in an easterly direction of the cropland where the grain was harvested, \texttt{Northing} as the number of meters in a northerly direction, and \texttt{Yield}, which is yield in bushels per acre, renamed from the original data; this was done to make analysis consistent across yield data providers.

\section{Strip trial, Example 1}\label{Example1Section}

\begin{figure}\centering
<<Example1Histogram, fig=TRUE,echo=FALSE, width=8, height=3>>=
ggplot(Example1ForHistogram, aes(x=Yield)) + geom_histogram() + 
           scale_colour_manual(values=cbPalette[pair.colors]) +
           labs(title = "Example 1 Yield")
@
\caption{Histogram of yield samples for Example 1. The data are skewed, which is consistent for yield monitor data, as there are many low-yielding regions in the cropland. This increase the number of samples in the left tail. The right tail is typically limited by yield potential of the crop planted.}\label{Example1Histogram}
\end{figure}

\begin{figure}\centering
<<Example1Maps, fig=TRUE,echo=FALSE, width=8, height=6>>=
grid.arrange(
  arrangeGrob(
        ggplot(Example1.dat, aes(Easting, Northing)) + 
           geom_point(aes(colour = Product),size=1) + 
           scale_colour_manual(values=cbPalette[pair.colors]) +
           labs(colour = "Variety", x="Easting", y="Northing", title = "Seeding Map"),
    ggplot(Example1.dat, aes(Easting, Northing)) + 
           geom_point(aes(colour = Yield),size=1) + 
           scale_colour_gradient2(low=vermillion, mid=yellow, high=blue, midpoint = median(Example1.dat[,"Yield"])) +
           labs(colour = "Yield", x="Easting", y="Northing",title="Harvest Map"),
    nrow=2
  )
)
@
\caption{Seeding and yield monitor harvest maps for Example 1. The upper graph shows that varieties are planing in alternating strips with two harvest passes per strip. The lower graph shows instantaneous yield estimates colored by yield value. There are conspicuous low yielding regions at about 100 m and about 200 m \texttt{Easting}}\label{Example1Maps}
\end{figure}

In this first example, two varieties were tested using a split-planter comparison method \cite{doerge-07-1999}.
Two varieties were assigned to the left and to they right half of a standard corn planter. Variety names were entered in the original data as `Product`, these were anonymized to letters "B" and "E" - this was part of a larger strip trial with additional test varieties.

Grain was harvested  with a GSP enabled combined, with the width of the harvester one-half the planter width. Twelve passes  were planted, representing two planted strips, and 24 passes were harvested. The data were trimmed to excluded head rows and side rows to improve uniformity of the strips. The length of each harvest pass was approximately 400m. The harvest and yield maps are shown in Figure \ref{Example1Maps}. The data were not screened from outliers, after the endrows and edgerows were removed from the analysis there were no meaningful outliers in the remaining data points (Figure \ref{Example1Histogram}) 


\section{Strip trial, Example 2}\label{Example2Section}

\begin{figure}\centering
<<Example2Histograms, fig=TRUE,echo=FALSE, width=8, height=6>>=
grid.arrange(
  arrangeGrob(
    ggplot(Example2ForHistOriginal, aes(x=Yield)) + geom_histogram() + 
           scale_colour_manual(values=cbPalette[pair.colors]) +
           labs(title = "Example 2 before cleaning"),
    ggplot(Example2ForHistCleaned, aes(x=Yield)) + geom_histogram() + 
           scale_colour_manual(values=cbPalette[pair.colors]) +
           labs(title = "Example 2 after cleaning"),
    nrow=2
  )
)
@
\caption{Histogram of yield samples for Example 2, before and after cleaning. The the upper graph we see that there were some unreasonably large yield values, and an excess of 0 values for instantaneous yield estimates. The data were filtered to limit yield estimates from $0<y_i<80$ as show in the lower graph.}\label{Example2Histograms}
\end{figure}

\begin{figure}\centering
<<Example2SprayYieldMaps, fig=TRUE,echo=FALSE, width=8, height=6>>=
grid.arrange(
  arrangeGrob(
       ggplot(Example2.dat, aes(Easting, Northing)) + 
            geom_point(aes(colour = Sprayed),size=1) + 
            scale_colour_manual(values=cbPalette[pair.colors]) +
            labs(colour = "Sprayed", x="Easting", y="Northing", title = "Spray Map"),
    ggplot(Example2.dat, aes(Easting, Northing)) + 
           geom_point(aes(colour = Yield),size=1) + 
           scale_colour_gradient2(low=vermillion, mid=yellow, high=blue, midpoint = median(Example2.dat[,'Yield'])) +
           labs(colour = "Yield", x="Easting", y="Northing",title="Harvest Map"),
    nrow=1
  )
)
@
\caption{Spraying and yield monitor harvest maps for Example 2. In this example, the boom used to spray the field covered 4 harvest passes, so there are 4 harvest passes per experimental strip. There were also several wetlands that prevented planted in some regions of the cropland.}\label{Example2SprayYieldMaps}
\end{figure}

The second example is a fungicide trial where the collaborator was instructed to spray a fungicidal treatment in alternating strips in a field of his choosing; the number and location of strips was also chosen by the farmer. In this case, the collaborator started spraying at the gate leading into the field and moved to the east with each spray pass. The sprayer boom covered the width of four harvest passes, so each treated strip consisted of four harvest pass samples. Spray and harvest maps are show in Figure \ref{Example2SprayYieldMaps}. The harvest shape files were edited in QGIS, first by superimposing the spray plan layer, then tagging individual yield points with strip numbers, such blocks 2,4,6 and 8 were sprayed and strips 1,3,5 and 7 were not sprayed and selected to be paired with sprayed strips. Endrows and yield data in non-experimental parts of the field were trimmed for functional data analysis, so that only pairs of sprayed/unsprayed strips were retained. The western most strip of each pair was untreated, while the easternmost strip of each pair was sprayed. This excluded the far eastern edge row from the analysis.

The field contained several incipient wetlands that could not be planted; instead buffer rows were planted and harvested around each wetland (as shown by gaps in the yield map). Harvest values from these buffer rows were included in the yield associated with harvest strips, but may introduce error in yield estimates for some locations in the strips. The data were screened to remove outliers; yield values were limited to $\ge 0$ and $<80$. Histograms of the data before and after cleaning are shown in Figure \ref{Example2Histograms}.

%\begin{figure}\centering
%<<fig=TRUE,width=6,height=4>>=
%ggplot(Example1.dat, aes(Easting,Northing)) + 
%           geom_point(aes(colour = Block),size=2) + 
%           scale_colour_manual(values=cbPalette) +
%           labs(colour = "Block", x="Easting", y="Northing", title = "Seeding Map")
%@
%\caption{Block Map, Example 1}\label{blocks_sample1}
%\end{figure}

\chapter{Univariate and Functional Data Analysis}\label{UnivariateAndFunctionalDataAnalysisChapter}

\section{Overview}

In many cases, on-farm strip trials are analyzed as if each strip were a single plot, and the analysis proceeds as if the strip data could be summarized by a single point estimate - that is, there is only one yield observation per strip. We refer to this as univariate analysis, to differentiate this method from functional data analysis, where each strip is analyzed as a sequence of observations in space.

To introduce functional data analysis, we start with a simple strip trial for variety testing as described in Section \ref{Example1Section}. We will show how a simple univariate analysis is extended to an analysis of functional data. Strip means are summarized in Figure \ref{Example1StripYieldsPlot} and Table \ref{SummaryExample1Strips}. Unless otherwise noted, figures were produced using \texttt{ggplot} \cite{wickham-2016}.

\section{Univariate data analysis} \label{UnivariateDataAnalysis}

First, assume we have $N_1$ treated strips and $N_2$ untreated strips. We pooled all yield monitor data points associated with a single string by simple averaging. Thus, if a strip has $k=1,\cdots,K$ total yield monitor points, denoted  $Y_{ijk}$, then
\begin{equation}
y_{ij} = \frac{\sum_{k=1}^{K_{ij}} Y_{ijk}}{K_{ij}}
\end{equation}
We model yield data points as subsamples, so $y_{ij}$ represents a plot mean.

We will further simplify the analysis by modeling the experiment as an a comparison of two means with pooled  standard deviation, which we will refer to as the univariate analysis. We write the statistical model as
\begin{equation}
y_{ij} = \mu_i + e_{ij}
\end{equation}
where $i = 1, \cdots, I$ is the number of treatments and $j = 1, \cdots, J$ are the number of observations per treatment group.

We limit this analysis to the case where treated strips are paired with untreated strips. This suggests a paired comparison $t$ test, where we calculate the $j^{th}$ paired difference as
\begin{equation}
d_j = y_{1j} - y_{2j}
\end{equation}
where $y_{1j}$ and $y_{2j}$ are individual strip yield estimates for the $j^{th}$ pair of strips. 
%%We pair strips moving from south to north, so $y_{1j}$ corresponds to the southern most strip of a pair

Let $\bar{d} = \frac{1}{n} \sum_{j=1}^n d_j$, then the $t$ statistic takes the form
\begin{equation}
t_0 = \frac{\bar{d}}{S_d/\sqrt{n}}
\end{equation}
where $n$ is the number of pairs and the standard deviation of the paired differences is given by
\begin{equation}
S_d = \sqrt{\frac{\sum_{j=1}^n (d_j - \bar{d})^2}{n-1}}
\end{equation}

Alternatively, instead of reporting the results with the null hypothesis $t$-test, we can report a confidence interval for the paired mean difference. A 95\% confidence interval takes the form
\begin{equation}
   \bar{d} \pm t_{0.025,\nu} S_d / \sqrt{n}
\end{equation}
where $t_{0.025,\nu}$ is the 97.5\% quantile with $\nu$ degrees of freedom .

%We can then test this hypothesis with a two independent sample $t$ test \cite{montgomery-2019} of the form

%\begin{equation}
%t_0 = \frac{\bar{y}_1 - \bar{y}_2}{S_p \sqrt{\frac{1}{N_1}+\frac{1}{N_2}}}
%\end{equation}
%where $\bar{y}_1$ and $\bar{y}_2$ are sample means with $N_1$ and $N_2$ observations per mean. $S_p$ is a pooled standard %deviation given by

%\begin{equation}
%S_p = \sqrt{\frac{{(N_1-1)S_1^2 + (N_2 -1) S_2^2}}{N_1 + N_2 -2}}
%\end{equation}
%where $S_1^2$ and $S_2^2$ are the sample variances associated with each mean, given by

%\begin{equation}
%S_i^2 = \frac{\sum_{j=1}^{N_{i}} (y_{ij} - \bar{y}_i )^2}{N_i-1}
%\end{equation}
%for $i = 1,2$

\section{Functional data analysis}

Following \cite{ramsay-2005}, each formula from the univariate analysis has a functional equivalent (Table \ref{CompFormula}). For simplicity, we analyze data from fields such that strips have been planted in nearly east-west or north-south orientations  that is, parallel to either latitudinal or longitudinal lines. For example, in Example 1, the crops were planted in rows running east to west. The origin where $x=0$ is the western most or southern most border of the field, and $x$ represents a distance from this border.  We denote a single strip as $y_{ij}(x)$, for $i=1,2$ and $j=1,..,n$ where $n$ is the number of pairs of strips. We can proceed to a point-wise paired $t$-test by evaluating $y_{ij}$ at uniformly distributed points $x$ in the range of the data. This choice of example data does not necessitate rotating strips as in \cite{marchant-2019}.

%\begin{table}[h!]
%\centering
%\begin{aligned} 
% \text{Formula} & \text{Univariate} & \text{Functional} \\ 
% \hline
%\text{Means model}          & $y_{ij} = \mu_i + e_{ij}$
%                     & \begin{equation}\label{FunctionMeansModel}y_{ij}(x) = \mu_i(x) + e_{ij}(x)\end{equation}\\ 
%\text{Paired Difference}    & $d_j = y_{1j} - y_{2j}$ 
%                     & \begin{equation}\label{FunctionPairedDifference}d_j(x) = y_{1j}(x) - y_{2j}(x)\end{equation} \\
%\text{Mean Difference}      & $\bar{d} = \frac{1}{n} \sum_{j=1}^n d_j$ 
%                     & \begin{equation}\label{FunctionMeanDifference}\bar{d}(x) = \frac{1}{n(x)} \sum_{j=1}^n(x) d_j(x)\end{equation}  \\ 
%\text{Number of Pairs}      & $n$ 
%                     & \begin{equation}\label{NumberofPairs}n(x)\end{equation}\\ 
%\text{S.D. of paired differences}    & $S_d = \sqrt{\frac{\sum_{j=1}^n (d_j - \bar{d})^2}{n-1}}$ 
%                     & \begin{equation}S_d(x)\label{SDPairedDifferences} = \sqrt{\frac{\sum_{j=1}^n(x) (d_j(x) - \bar{d(x)})^2}{n(x)-1}}\end{equation}\\
%\text{$t$ statistic}        & $t_0 = \frac{\bar{d}}{S_d/\sqrt{n}}$ 
%                     & \begin{equation}t_0(x)\label{FunctionTStatistic} = \frac{\bar{d}(x)}{S_d(x)/\sqrt{n(x)}}\end{equation}\\ 
%\text{95\% Confidence Interval} & $\bar{d} \pm t_{0.025,\nu} S_d / \sqrt{n}$
%                         & \begin{equation}\label{FunctionCI}\bar{d}(x) \pm t_{0.025,\nu} S_d(x) / \sqrt{n(x)}\end{equation}
%\end{aligned} 
%\label{CompFormula}
%\end{table}

%\begin{table}[h!]
%\centering
%\begin{tabular}{r r r} 
% \hline
% Formula & Univariate & Functional \\ 
% \hline
%Means model          & $y_{ij} = \mu_i + e_{ij}$
%                     & \begin{equation}y_{ij}(x) = \mu_i(x) + e_{ij}(x)\label{FunctionMeansModel}\end{equation}\\ 
%Paired Difference    & $d_j = y_{1j} - y_{2j}$ 
%                     & \begin{equation}\label{FunctionPairedDifference}d_j(x) = y_{1j}(x) - y_{2j}(x)\end{equation} \\
%Mean Difference      & $\bar{d} = \frac{1}{n} \sum_{j=1}^n d_j$ 
%                     & \begin{equation}\label{FunctionMeanDifference}\bar{d}(x) = \frac{1}{n(x)} \sum_{j=1}^n(x) d_j(x)\end{equation}  \\ 
%Number of Pairs      & $n$ 
%                     & \begin{equation}\label{NumberofPairs}n(x)\end{equation}\\ 
%S.D. of paired differences    & $S_d = \sqrt{\frac{\sum_{j=1}^n (d_j - \bar{d})^2}{n-1}}$ 
%                     & \begin{equation}S_d(x)\label{SDPairedDifferences} = \sqrt{\frac{\sum_{j=1}^n(x) (d_j(x) - \bar{d(x)})^2}{n(x)-1}}\end{equation}\\
%$t$ statistic        & $t_0 = \frac{\bar{d}}{S_d/\sqrt{n}}$ 
%                     & \begin{equation}t_0(x)\label{FunctionTStatistic} = \frac{\bar{d}(x)}{S_d(x)/\sqrt{n(x)}}\end{equation}\\ 
%95\% Confidence Interval & $\bar{d} \pm t_{0.025,\nu} S_d / \sqrt{n}$
%                         & \begin{equation}\label{FunctionCI}\bar{d}(x) \pm t_{0.025,\nu} S_d(x) / \sqrt{n(x)}\end{equation}
%Sample Mean    & $\bar{y}_i = \frac{\sum_{j=1}^{N_i} y_{ij}}{N_i}$ & $\bar{y}_i(d) = \frac{\sum_{j=1}^{N_i(d)} y_{ij}(d)}{N_i(d)}$  \\ 
%Sample Variance    & $S_i^2 = \frac{\sum_{j=1}^{N_i} (y_{ij}-\bar{y}_i)^2}{N_1 - 1}$ &  $S_i^2(d) = \frac{\sum_{j=1}^{N_i(d)} (y_{ij}(d)-\bar{y}_i(d))^2}{N_1(d) - 1}$ \\ 
%Pooled Standard Deviation    & $S_p = \sqrt{\frac{(N_1 -1) S_1^2 + (N_2-1)S_2^2}{N_1+N_2-2}}$ & $S_p^2(d) = \frac{(N_1(d) -1) S_1^2(d) + (N_2(d)-1)S_2^2(d)}{N_1(d)+N_2(d)-2}$  \\ 
%$t$ statistic    & $t_0=\frac{\bar{y}_1-\bar{y}_2}{S_p \sqrt{\frac{1}{N_1}+ \frac{1}{N_2}}}$  & $t_0(d)=\frac{\bar{y}_1(d)-\bar{y}_2}{S_p(d) \sqrt{\frac{1}{N_1(d)} + \frac{1}{N_2(d)}}}$  \\ 
%\end{tabular}
%\caption{Univariate and Functional forms of statistics used to compare strips. $i = 1, 2$ is the number of treatments, $j = 1, \cdots, J$ are the number of strips per treatment and $x$ is the distance from one side of the field (\texttt{Northing} or \texttt{Easting}, depending on the example.)}
%\label{CompFormula}
%\end{table}

% see https://tex.stackexchange.com/questions/521422/equation-with-numbering-in-table
\begin{table}[]
 \begin{tabularx}\textwidth{@{}lML@{}}
 \toprule
 \textbf{Description} & \multicolumn{1}{l}{\textbf{Functional Form}}
                              & \multicolumn{1}{l}{}\\ \midrule
  Means model               & y_{ij}(x) = \mu_i(x) + e_{ij}(x)
                      & FunctionMeansModel \\
  Paired Difference    & d_j(x) = y_{1j}(x) - y_{2j}(x)
                      & FunctionPairedDifference \\
  Mean Difference        & \bar{d}(x) = \frac{1}{n(x)} \sum_{j=1}^n(x) d_j(x)
                      & FunctionMeanDifference \\
  Number of Pairs        &  n(x)
                      & NumberofPairs \\
  S.D. of paired differences  & S_d(x) = \sqrt{\frac{\sum_{j=1}^n(x) (d_j(x) - \bar{d(x)})^2}{n(x)-1}}
                      & SDPairedDifferences \\
  $t$ statistic      & t_0(x) = \frac{\bar{d}(x)}{S_d(x)/\sqrt{n(x)}}
                      & FunctionTStatistic \\
  95 Percent C. I.   & \bar{d}(x) \pm t_{0.025,\nu} S_d(x) / \sqrt{n(x)} 
                      & FunctionCI \\
  \bottomrule
  \end{tabularx}
 \caption{Functional forms of statistics used to compare strips. $i = 1, 2$ is the number of treatments, $j = 1, \cdots, J$ are the number of strips per treatment and $x$ is the distance from one side of the field (\texttt{Northing} or \texttt{Easting}, depending on the example.)}
  \label{CompFormula}
\end{table}


In general, $y_{ij}(x)$ will take a functional form
\begin{equation}
y_{ij}(x) = F_{ij}(x) + e_{ij}(x)
\end{equation}
where $F_{ij}$ is the data generating process (e.g. yield potential) and $e_{ij}$ is random error associated with measurement. Factors contributing to $e$ is discussed in more detail in Section \ref{YieldMonitorOverview}. In practice, the exact function $F$ is unknown, but can be approximated by function $f$. With respect to yield monitor data, $f$ serves two purposes. It is used to smooth noisy yield monitor data and to interpolate between observation points. The second part is necessary to provide a common basis for functional data analysis. Yield monitor samples are collected at one second intervals, but the speed of the combine varies as it moves along a harvest pass, so the distance between each sample varies. We wish to analyze functional data using distance as a basis, so the function $f$ converts raw data from a time basis to a distance basis.

\chapter{Smoothing Functions for Functional Data}\label{SmoothingFunctionsChapter}

\section{Overview}

In this section, we explore different methods of finding functional forms of yield monitor data, using as an example a single harvest pass. The raw yield measurements for a single harvest pass are fit to smoothing functions using different libraries in R, in order to determine a function that is flexible enough to allow for some control of the smoothing but that also is sophisticated enough to find suitable smoothing curves without extensive supervision.

\begin{figure}\centering
<<RawSwaths, fig=TRUE, echo=FALSE, width=12, height=10>>=
ggplot(Example1.dat, aes(Easting, Yield)) + 
   geom_point(size=1,col=cbPalette[1]) + 
   #geom_line(linewidth=1,col=cbPalette[1]) + 
   labs(x="Easting", y="Yield", title = "Harvest passes from Example 1") + facet_wrap(~Pass)
@
\caption{Harvest passes with no smoothing. For the first example data set, there are 24 sets of instantaneous yield points to be smoothed an interpolated by functional forms. See Figure \ref{RawSwath} for more details}\label{RawSwaths}. 
\end{figure}

To provide a context for the extent of the problem of finding functional forms for observational units (harvest passes) in a single strip trial, we consider the yield observations from Example 1, plotted by position of the observation (\texttt{Easting}) grouped by harvest pass number (\ref{RawSwaths}). As we will see in following sections, key parts of finding a functional form for these data is choosing a functional basis for data of this type, then identifying an appropriate degree of smoothness for the functional basis. If we had no time constraints, we could attempt to fit each harvest pass individually, finding the unique combination of basis functions and smoothing parameters optimal for each harvest pass. However, we assume that obtaining unique solutions for each harvest pass is impractical, and we will, in this section, attempt to determine if there is an implementation of smoothing functions such that the default behavior of the fitting library provides a solution sufficient for all harvest passes in these data without fine-tuning smoothing parameters.

\section{Smoothing functions for a single observational unit}

<<SubsetASinglePass, echo=FALSE>>=
passes <- unique(Example1.dat[,'Pass'])
SinglePass.dat <- Example1.dat[Example1.dat[,'Pass']==passes[1],]
@

\begin{figure}\centering
<<RawSwath, fig=TRUE, echo=FALSE, width=8, height=3>>=
ggplot(SinglePass.dat, aes(Easting,Yield)) + 
  geom_point(size=1,col=cbPalette[1]) + 
#  geom_line(linewidth=1,col=cbPalette[1]) + 
#  scale_colour_manual(values=cbPalette) + 
labs(x="Easting", y="Yield", title = "Single Harvest Pass, No Smoothing")
@
\caption{Representative harvest pass with no smoothing. These data are the 24$^{th}$ or northern most of data shown in Graph \ref{RawSwaths}. Instantaneous yield estimates are shown as gray points. Yield estimates are taken a 1 second intervals, but we are interested in the location of estimates, so points are plotted versus meters \texttt{Easting}}\label{RawSwath}
\end{figure}

We will use as an example a single harvest pass taken from Example 1 (Section \ref{Example1Section}) data set, a split-planter variety trial. We take the last harvest pass of the data (pass number 24). The original yield observations for this harvest pass are shown in Figure \ref{RawSwath}

Figure \ref{RawSwath} shows the raw data from one harvest swath. Consider this as a single specific instance of some $y_{ij}$ We wish to find an smoothing function $f$ that minimizes deviations from the true data generating process $F$ and maps data samples at uniform time intervals to a uniform set of distance points, while preserving the features of the original data. Specifically, in the case of \ref{RawSwath}, inspection of the data leads to the following features: A decrease from approximately 240 $bu/ac$ over the first 10m followed by a rise that appears to end at about 80m. There may be a slight decrease in yield from 80m to 110m, but yield appears stable from 110m to 160m. At 160m, there is an abrupt decrease in yield which rises to a more stable yield at about 210m. There may be small fluctuations in yield from 210m to 300m, where there appears to be a small decrease in yield from 300m to 330m, then yield appears stable until a possible decrease at 400m. A functional representation of the data should reflect the features we find by visual inspection of the raw data.

Choosing an appropriate function $f$ is part of the art of functional analysis. The choice of function is largely going to depend on the relative importance placed on two components of the functional form - smoothing and interpolation.

Functional data are in theory continuous values, but data are commonly collected at discrete intervals, and measurement errors occur during when functions are sampled. Smoothing occurs when functional forms are chosen to minimize variance between the function values at sample points and the sampled values.

Discrete functional data units frequently cannot be sampled at identical independent values (i.e. the same $x$). Instead, values at non-sampled $x$ are inferred from sample points $y_{ij}(x)$ by a process of interpolation. The number of sampled points used for each interpolated value characterizes the choice of functional forms.

%A generalized additive model following from our previous examples may take the form (adapted from \cite{wood-2017} )

%\begin{equation}
%g(y_i) = \mathbf{A_i} \mathbf{\theta} + f(x_{1i}) +f(x_{2i},x_{3i}) + e_i
%\end{equation}

%where $\mathbf{A_i} \mathbf{\theta}$ are the strictly parametric components of the model and  $f(\circ)$ is a function form of the covariate $x_1, \dots$. In the functional analysis of strip trials, $\mathbf{A_i} \mathbf{\theta}$ would represent the treatment structure while $f(x_{1i})$ represents a smooth function of yield over distance.

%In this case, we can assume $g(y_i)=y_i$ and $\mathbf{A_i} \mathbf{\theta} = \mu_i$, so this simplifies to a linear model with respect to the smoothing function. We will use distance from the a fixed end of the field as covariate $x = x_1$

%\begin{equation}
%y_i = f(x) + e_i
%\end{equation}

%\section{Forms of $f(x)$}

%The choice of $f(x)$ depends largely on a smoothing parameter $\lambda$ that determines how 'smooth' or 'wiggly' the resulting interpolating function fits the data. Too smooth and the data are under fit, losing information. If the fit is too rough and the data are over fit, including too much error variance in the model. We use the R library \texttt{mgcv} to perform generalized cross-validation (\cite{wood-2017}, p 169) of the fitted function $f_i(x)$ to obtain an appropriately smooth $f_i(x)$. Fitted values are then taken as the true yield at point $x$, and this value is used to compute summary statistics for functional data analysis. 


Before we can attempt a functional analysis of strip trial data, we need a method to converted raw yield monitor data to an appropriate functional form. Yield monitor data is both inherently noisy, and while measurements are taken uniform one second intervals, we wish to analysis functional forms of yield with a basis in the spatial position of the measured yields. That is, we need to convert yield monitor data that has a basis in time to a basis function that can be uniformly sampled in space.

\section{Representing $f$}

To simplify discussion in this next section, we'll consider only a single functional variable $y$ and drop the $ij$ subscript notation. We follow \cite{wood-2017}, and represent a simple smoothing function of one covariate as
\begin{equation}
y_i = f(x_i) + e_i
\end{equation}\label{SimpleFunctionalForm}
where $y$ is the response variable, $x$ is a covariate, $f$ is a smooth function and the $e_i$ are normally distributed random variables. In this context, we are looking to represent the true function $f$ that generates the data in Figure \ref{RawSwath}.

Several mathematical models exist that are used to approximate $f$. In this section we review some possible functional forms, again following \cite{wood-2017}. We define a *basis* as a space of functions which includes $f$ (or, at least, a close approximation) as an element. A basis is defined by some set of functions $b_j(x)$, $j = 1, \cdots, k$. We then represent $f$ as a sum of the basis functions,
\begin{equation}
f(x) = \sum_{j=1}^k \beta_j b_j(x)
\end{equation}\label{GenericBasisFunctions}
with $\beta_j$ being unknown parameters to be found from data. We can rewrite this in matrix form as
\begin{equation}
f(\mathbf{x}) = \mathbf{X} \mathbf{\beta}
\end{equation}\label{BasisFunctionsMatrixForm}
where $X_{ij} = b_{j}(x_{i})$ and $\mathbf{\beta} = \left\{\beta_1, \beta_2, \cdots \right\}$

\subsection{Polynomial functions}

Polynomial functions are frequently used as functional forms due to their simplicity and flexibility. Unlike power or exponential functions, polynomial functions can be fit to data using ordinary least squares (polynomial regression), while any number of data points can be fit exactly using polynomials of sufficiently high order.

Polynomial functions take the form
\begin{equation}
f(x_i) = \beta_0 + \beta_{1} x + \dots + \beta_{k} x_i^k
\end{equation}
where $k$ is the order of the polynomial. Clearly, a polynomial function forms a basis, where we can write the polynomial function as
\begin{equation}
f(x) = \sum_{j=1}^{k+1} \beta_{j} x^{j-1}
\end{equation}

 
\begin{figure}\centering
<<PolynomialSmoothing, fig=TRUE, echo=FALSE, width=8, height=3>>=
current.lm <- lm(Yield ~ poly(Easting,12), data=SinglePass.dat)
Polynomial <- data.frame(
  Easting = seq(min(SinglePass.dat[,'Easting']), max(SinglePass.dat[,'Easting']) ,len = 10000)
)
Polynomial[,'Yield'] <- predict(current.lm, newdata = Polynomial)
     
ggplot(SinglePass.dat, aes(Easting, Yield)) + 
   geom_point(size=1,col=cbPalette[1]) + 
#   geom_line(linewidth=1,color=cbPalette[1]) + 
   geom_line(aes(Easting, Yield), linewidth=1,color=cbPalette[2],data=Polynomial)
@
\caption{Interpolation of yield points using a linear model of degree 12. Instantaneous yield estimates are shown in gray lines, while a polynomial curve of degree twelve smoothing the points is shown as a line.}\label{PolynomialSmoothing}
\end{figure}

Figure \ref{PolynomialSmoothing} show a polynomial of order 12 superimposed over the original data. This function in general smoothly interpolates the data, but does over-smooth the key feature at 180m. 

\subsection{Local smoothing}

\begin{figure}\centering
<<SeveralPolynomials, fig=TRUE, echo=FALSE, width=8,height=3>>=
#For some reason, I'm getting an error when poly(..., k>26)
#for(i in 1:length(knots)) {
knots = c(3,6,12,18,24,30,36) #redefined later in the code
PredictionsLM.dat <- data.frame(
  Easting = rep(SinglePass.dat[,'Easting'], length(knots))
)
PredictionsLM.dat[,'Knots'] <- factor(rep(knots, each = length(SinglePass.dat[,'Easting'])))
PredictionsLM.dat[,'Yield'] <- NA

for(i in 2:length(knots[1:5])) {
  current.lm <- lm(Yield ~ poly(Easting, knots[i]), data=SinglePass.dat)
  PredictionsLM.dat[PredictionsLM.dat[,'Knots']==knots[i], 'Yield'] <- predict(current.lm)
}
PredictionsLM.dat <- PredictionsLM.dat[!is.na(PredictionsLM.dat[,'Yield']),]

ggplot(PredictionsLM.dat, aes(Easting, Yield)) + 
  geom_point(aes(Easting, Yield),size = 1, data=SinglePass.dat, color=cbPalette[1]) + 
#  geom_line(aes(Easting, Yield),size = 0.5, data=SinglePass.dat, color=cbPalette[1]) + 
  geom_line(linewidth=1, aes(color=Knots)) + 
  scale_colour_manual(values=cbPalette[2:11]) + 
  labs(colour = "Order", x="Easting", y="Yield", title = "Single harvest pass, polynomial smoothing")
@
\caption{Smoothing the raw data with polynomials of increasing order. Instantaneous yield estimates are shown by gray points, and polynomial functions interpolating and smoothing these points are shown as lines}\label{SeveralPolynomials}
\end{figure}

A problem with polynomial regression is the regression is by definition global; the entire range of $x_i$ is fit by a single function This may allow for non-local influences. Data in one region, particularly points with unusual leverage, can influence the fit in other regions of the data \cite{fox-1997}. The order $k=12$ for Figure \ref{PolynomialSmoothing} was chosen arbitrarily. If we increase the order of the polynomial, we see dramatic oscillations in what appear by inspection to be relatively smooth parts of the original data, as we see in Figure \ref{SeveralPolynomials}.

%Thus, any midpoint value between two arbitrary points $x_i$ and $x_j$ can be interpolated exactly with a polynomial of order 1. i.e $y = a +bx_i$. One possible functional form for $y_{ij}$, then, computes $y(d)$ for any $d$ by identifying the $y_{ikK}$ bounding $d$ and finding the straight line interpolation between the points.

An alternative approach, piecewise linear regression, partitions the data into "bins", then fit a linear model restricted to the bin \cite{fox-1997}. The points at which the bins join are referred to as knots \cite{wood-2017}, while the basis for this interpolation is a series of simple linear function.

Let the knots be represented as ${x^{*}_j} : j = 1, \cdots, k$ such that $x^{*}_j>x^{*}_{j-1}$. For $j=2,\cdots, k-1$, we represent the basis functions as \cite{wood-2017}
\begin{equation}
b_j(x) = 
  \begin{cases} 
      (x-x^{*}_{j-1})/(x^{*}_j-x^{*}_{j-1}) &  x^{*}_{j-1} < x \le x^{*}_{j} \\
      (x^{*}_{j+1}-x)/(x^{*}_{j+1}-x^{*}_{j}) & x^{*}_{j} < x \le x^{*}_{j+1} \\
      0 & \text{otherwise}
   \end{cases}
\end{equation}\label{PiecewiseLinearBasis}
with 
\begin{equation}
b_1(x) =
\begin{cases}
(x^{*}_2-x)/(x^{*}_2 - x^{*}_1) & x < x^{*}_2 \\
0 & \text{otherwise}
\end{cases}
\end{equation}
and
\begin{equation}
b_k(x) =
\begin{cases}
(x - x^{*}_{k-1})/(x^{*}_k - x^{*}_{k-1} ) & x > x^{*}_{k-1} \\
0 & \text{otherwise}
\end{cases}
\end{equation}

\begin{figure}\centering
<<PiecewiseLinear, fig=TRUE, echo=FALSE, width=8, height=3>>=
#code adapted from wood-2017, p165/6
tf <- function(x, xj, j) {
  dj <- xj*0; dj[j] <- 1
  approx(xj, dj, x)[[2]]
}
tf.X <- function(x, xj) {
  nk=length(xj);n <- length(x)
  X <- matrix(NA, n, nk)
  for (j in 1:nk) X[,j] = tf(x, xj, j)
  X
}
Easting <- SinglePass.dat[,'Easting']
Yield <- SinglePass.dat[,'Yield']
sj <- seq(min(Easting),max(Easting),length=12)
X <- tf.X(Easting, sj)
b <- lm(Yield ~ X-1)
s <- seq(min(Easting),max(Easting),length=length(Easting))
Xp <- tf.X(s, sj)
#plot(Easting, Yield)
#lines(s,Xp %*% coef(b))
Piecewise <- data.frame(
  Easting = s
)
Piecewise[,'Yield'] <- Xp %*% coef(b)
ggplot(SinglePass.dat, aes(Easting, Yield)) + 
   geom_point(size=1, color=cbPalette[1]) + 
#   geom_line(linewidth=1, color=cbPalette[1]) + 
   geom_line(aes(Easting, Yield), linewidth=1, color=cbPalette[2],data=Piecewise)
@
\caption{Piecewise linear interpolation. Instantaneous yield estimates (shown as gray points) are interpolated by a set of short linear curves. There are 12 knots specified for this interpolation, resulting in 11 linear basis functions.}\label{PiecewiseLinear}
\end{figure}

Figure \ref{PiecewiseLinear} shows piecewise linear regression superimposed on the observed harvest values for the example strip data. With 12 knots, piecewise linear regression appears to preserve key features of the original data. However, the choice of knots was be made by inspection; there is no automatic method for choosing knots. Code for this figure was adapted from \cite{wood-2017}.

%Piecewise <- data.frame(
%  Easting = seq(min(x), max(x) ,len = 10000)
%)
%Piecewise[,'Yield'] <- predict(model, newdata = data.frame(x = Piecewise[,'Easting']))
%Knots = data.frame(
%  Easting = seq(min(x), max(x), len = K + 2)[-c(1, K + 2)]
%)
%Knots[,'Yield']  = predict(model, newdata = data.frame(x = Knots[,'Easting']))
                           
%ggplot(SinglePass.dat, aes(Easting,Yield)) + 
%   geom_line(linewidth=1,color=cbPalette[1]) + 
%   #geom_point(size=1,color=cbPalette[1]) + 
%   geom_line(aes(Easting,Yield),linewidth=1,color=cbPalette[2],data=Piecewise) +
%   geom_point(size=2,color=cbPalette[2],data=Knots) 

\subsection{Spline basis functions}

Splines functions are commonly chosen as basis functions \cite{perperoglou-2019}.  As with the piece wise regression model, spline models are commonly constrained to be continuous at the knots. Most splines also have the constraint that the first and second derivatives are also continuous; and in most cases cubic splines are used, such that a cubic polynomial interpolates the region between knots \cite{perperoglou-2019}.

%Following \cite{perperoglou-2019}, a cubic spline function with three knots $x^{*}_1, x^{*}_2$ and $x^{*}_3$ can be represented as
%\begin{equation}
%f(x) = \beta_0 + \beta_1 x + \beta_2 x^2 + \beta_3 x^3 + \beta_4(x-x^{*}_1)^3 + \beta_5(x-x^{*}_2)^2 + \beta_6(x-x^{*}_3)^3
%\end{equation}

\cite{wood-2017} p. 201 defines a cubic spline function $f(x)$ with $k$ knots, $x_1, \cdots x_k$, as
\begin{equation}
f(x) = a^-_j(x)f(x_j) + a^+_j(x) f(x_{j+1}) + c^-_j(x) f''(x_j) + c^+_j f''(x_{j+1}) \text{ if } x_j \le x \le x_{j+1}
\end{equation}
where 
\begin{equation}
\begin{aligned}
a^-_j(x) &= (x_{j+1}-x)/(x_{j+1}-x_{j}) \\
a^+_j(x) &= (x-x_j)/(x_{j+1}-x_{j}) \\
c^-_j(x) &= \left[ (x_{j+1} - x)^3/(x_{j+1}-x_{j}) - (x_{j+1}-x_{j})^2 \right] / 6\\
c^+_j(x) &= \left[ (x-x_j)^3/(x_{j+1}-x_{j}) - (x_{j+1}-x_{j})(x-x_j)\right] / 6\\
\end{aligned}
\end{equation}

Suppose we have knots defined by ${x_i, y_i : i=1,..,n}$. The natural cubic spline $g(x)$ is defined as a function over the bins defined by $[x_i, x_{i+1}]$ where a single cubic polynomial interpolates over single bin \cite{wood-2017}. The knots are joined in such a way as the whole spline is continuous to the second derivative, where $g(x_i)=y_i$, while $g''(x_1) = g''(x_n) = 0$. Natural splines with $k$ knots will have $k+1$ degrees of freedom \cite{perperoglou-2019}.

B-splines are an alternative representation of a spline basis \cite{wood-2017, perperoglou-2019}. We define $m+1$ as the order of the basis (so, for a cubic spline $m=2$). B-splines are non-zero over the interval between $m+3$ knots, so are more local that ordinary splines. A $k$ parameter b-spline is defined over $k+m+2$ knots\cite{wood-2017}, so b-splines have $p+k$ degrees of freedom \cite{perperoglou-2019}, where for cubic splines $p=3$. 
An $(m+1)^{th}$ order b-spline takes the form 
\begin{equation}
f(x) = \sum_{i=1}^k B^m_i(x) \beta_i
\end{equation}

The basis functions for b-splines are typically defined recursively \cite{wood-2017,perperoglou-2019}, taking the form \cite{wood-2017}
\begin{equation}
   B_i^m(x) = \frac{x-x_i}{x_i+m+1}B^{m-1}(x) + \frac{x_{i+m+2}-x}{x_{i+m+2}-x_i+1}B^{m-1}{i+1}(x), i = 1, \cdots, k
\end{equation}
with
\begin{equation}
   B^{-1}_i(x) = 
   \begin{cases} 
      1 & x_i \leq x < x_{i+1} \\
      0 & otherwise
   \end{cases}
\end{equation}

Extending the b-spline, we find p-splines \cite{eilers-1996}. First, we note that b-splines are usually fit using ordinary least squares regression, by minimizing an objective function taking the form
\begin{equation}
S = \sum_{i=1}^m \left\{  y_i - \sum_{j=1}^n \beta_j B_j(x_i) \right\}^2
\end{equation}

P-splines add a finite difference penalty, so that the function to be minimized takes the form \cite{eilers-1996}
\begin{equation}
S = \sum_{i=1}^m \left\{  y_i - \sum_{j=1}^n \beta_j B_j(x_i) \right\}^2 + \lambda \sum_{j=k+1}^n (\Delta^k \beta_j)^2
\end{equation}
where $\lambda$ is a tuning parameter to control the degree of penalization, and ultimately the "wiggliness" allowed when fitting the splines.

A further extension to penalizes splines allows the penalty to be weighted according to the covariate $x$. We refer to these as adaptive splines, following \cite{wood-2017}.

\section{Fitting splines in R}

There exist a variety of libraries and functions in R that may be used to interpolate or smooth noisy data using spline functions \cite{perperoglou-2019}. We will consider the applicability of some of the functions to the example harvest strip data in Figure \ref{RawSwath}

\subsection{Spline interpolation using simple linear regression}

Let $\mathbf{X}$ be a design matrix encoding spline coefficients ($b_j$) from equation \ref{GenericBasisFunctions}, such that the problem of fitting a set of spline basis functions reduces to the problem of minimizing 
\begin{equation}
\lVert\mathbf{y} - \mathbf{X} \mathbf{\beta} \rVert ^2,
\end{equation}\label{OLSModel}
 
where $\mathbf{y}$ is the vector of $y_i$ from Equation \ref{SimpleFunctionalForm} while $\mathbf{X}$ and $\mathbf{beta}$ are from Equation \ref{BasisFunctionsMatrixForm}. The solution vector $\widehat{\beta}$ can be found using ordinary least squares. We use the \texttt{lm} function in R to compute a least squares solution to problems of form of Equation \ref{OLSModel}.

We fit splines to the data using the \texttt{spline} \cite{rcore} library in R to generate appropriate $\mathbf{X}$ matrices. The \texttt{spline} library includes two functions that expand to spline bases, \texttt{ns} and \texttt{bs}. \texttt{ns} produces the basis matrix for natural splines, while \texttt{bs} produces the basis matrix for b-splines. 

The \texttt{ns} function accepts \texttt{df} as an argument to controlling smoothness and the function chooses \texttt{df - 1 - intercept} knots. For illustration, we choose instead the argument \texttt{df=12+1}, so the resulting spline bases has 12 knots, comparable to the piecewise linear fit in Figure \ref{PiecewiseLinear}

\texttt{bs} chooses \texttt{df-degree} knots if \texttt{df} is passed an a argument. We illustrate the \texttt{bs} function by using the argument \texttt{df=15}. B-splines with 15 knots identifies similar features as with \texttt{ns} with 12 knots.

<<SplineSmoothingCode, echo=FALSE>>=
spline.knots = seq(0,1, length.out=12)
ns12_1.lm <- lm(Yield ~ ns(Easting, df=12+1), data=SinglePass.dat)
#current.lm <- lm(Yield ~ ns(Easting, knots=spline.knots), data=SinglePass.dat)
NSplines <- data.frame(
  Easting = seq(min(SinglePass.dat[,'Easting']), max(SinglePass.dat[,'Easting']),
                len = length(SinglePass.dat[,'Yield']))
)
NSplines[,'Yield'] <- predict(ns12_1.lm, newdata = NSplines)

bs12_3.lm <- lm(Yield ~ bs(Easting, df=12+3), data=SinglePass.dat)
#current.lm <- lm(Yield ~ bs(Easting, knots=spline.knots), data=SinglePass.dat)
BSplines <- data.frame(
  Easting = seq(min(SinglePass.dat[,'Easting']), max(SinglePass.dat['Easting']),
                len = length(SinglePass.dat[,'Yield']))
)
BSplines[,'Yield'] <- predict(bs12_3.lm, newdata = BSplines)
@

%## Smoothing Penalty

%In the previous examples, coefficients $\beta_i$ are estimated using ordinary least squares; that is, coefficients are found that minimize \cite{craven-1979]
%$$
%\frac{1}{n} \sum_{j=1}^n (f(x_j) -y_j)^2
%$$
%To estimate an optimal amount of smoothing, a penalty is introduced \cite{craven-1979]. The frequently takes the form
%$$
%\frac{1}{n} \sum_{j=1}^n (f(x_j) -y_j)^2 + \lambda \int_0^1(f^{(m)}(u))^2 du
%$$
%\cite{craven-1979] uses the notation $g_{n, \lambda}$ where $g \in W_2^(m)= {f: f,f', \cdots, f^{m-1} abs. cont., f^{(m)} \in \mathcal{L}_2[0,1]}$.

\subsection{Fitting splines using generalized additive models}

We would prefer to use the \texttt{mgcv} library, or "Mixed GAM Computation Vehicle with Automatic Smoothness Estimation" \cite{wood-2017} to find smoothing functions for our harvest pass data. \texttt{mgcv} is an attractive package to use for these analysis. This package is a standard part of the of R installation, and thus can be considered stable. The driver function \texttt{gam} provides automatic smoother estimation (\texttt{s} in \texttt{mgcv} notation), although it is relatively easy to fine-tune smoothing parameters. \cite{perperoglou-2019}. The automatic nature of smoother selection is of importance to analysis of strip trial data - strips trial themselves may consist of several individual measurement units (i.e. harvest passes) and it is not always practical to write code to optimize each measurement unit, particularly if there are multiple trials to be analyzed in a short period of time. \texttt{mgcv} also has options to smooth in two dimensions, which will be of use later when we consider analysis of problematic strip trials.


The library \texttt{mgcv} is one of several packages that fit generalized additive models in R \cite{perperoglou-2019} .In the broad sense, a generalized additive model takes the form (adapted from \cite{wood-2017})
\begin{equation}
g(y_i) = \mathbf{A_i} \mathbf{\theta} + f(x_{1}) + f(x_2, x_3) + e_i
\end{equation}\label{GAMFormula}
where $\mathbf{A_i} \mathbf{\theta}$ are the strictly parametric components of the model and  $f(x_i,\cdots)$ is a function form of the covariate(s) $x_i, \cdots$. 

In this case, we can assume $g(y_i)=y_i$ and $\mathbf{A_i} \mathbf{\theta} = \mu_i$, so this simplifies to a linear model with respect to the smoothing function, thus is of the form $y_i = f(x) + e_i$

Now, reconsider the piecewise linear basis in Equation \ref{PiecewiseLinearBasis}. Instead of minimizing $\lVert\mathbf{y} - \mathbf{X} \mathbf{\beta} \rVert ^2$, we may instead choose to control the wiggliness of the resulting fit model by minimizing \cite{wood-2017}
\begin{equation}
   \lVert\mathbf{y} - \mathbf{X} \mathbf{\beta} \rVert ^2 + \lambda \sum_{j=2}^{k-1} \left\{f(x^*_{j-1})-f(x^*_j) + f(x^*_{j+1})  \right\}^2
\end{equation}\label{PiecewisePenalized}

This adds as sum of squared second differences of the function at the knots; when the basis functions are splines this is represented by the second integral of the smoothing function,
\begin{equation}
   \sum_{i=1}^n\left\{ y_i -f(x_i) \right\}^2 + \lambda \int f''(x)^2 dx
\end{equation}

In either case, the wiggliness of the smoothing function relative to the data can be controlled by the selection of $\lambda$. That is, given an estimate $\widehat{f}$ of the true data generative function $f$, high values of $\lambda$ over-smooth the data, while low values of $\lambda$ under-smooth the data, either resulting in an estimated $\widehat{f}$ that is not close to the true value $f$ \cite{wood-2017}. Ideally, we would choose $\widehat{f}$ such that the value
\begin{equation}
   M = \frac{1}{n} \sum_{i=1}^n (\widehat(f)(x_i)-f(x_i))^2
\end{equation}
is minimized. Clearly, we cannot directly measure $f$, so $M$ cannot be computed directly. However, the expected squared error of predicting a new variable, $\mathbb{E}(M)+\sigma^2$, can be estimated. Let $\widehat{f}^{[-i]}$ be the value of $\widehat{f}$ fit to all the $\mathbf{y}$  except the single value $y_i$. Wood (\cite{wood-2017}) then defines the ordinary cross validation score as

\begin{equation}
\mathcal{V}_0 = \frac{1}{n} \sum_{i=1}^n (\widehat{f}^{[-i]}(x_i) - y_i)^2
\end{equation}\label{OrdinaryCrossValidation}

Now, $\widehat{f}^{[-i]} \approx \widehat{f}$ when $n$ is sufficient large, and it can be shown \cite{wood-2017} that $\mathbb{E}(\mathcal{V}_0) \approx \mathbb{M}+\sigma^2$, again for sufficiently large $n$. This leads to the conclusion that $M$ can be minimized by choosing a $lambda$ that minimizes $\mathcal{V}_0$. However, especially for large $n$ (or correspondingly $\widehat{f}$ with a large number of free parameters) computing $\mathcal{V}_0$ directly is computationally expensive. However, is can also be shown \cite{wood-2017} that

\begin{equation}
\mathcal{V}_0 = \frac{1}{n} \sum_{i=1}^n (y_i - \widehat{f}(x_i))^2 / (1-A_{ii})^2
\end{equation}\label{OrdinaryCrossValidationInfluence}

where $\widehat{f}$ is fit to the entire data set, and $\left\{ A_{ii} \right\} = \mathbf{A}$ is the influence matrix for $\widehat{f}$. This leads to a generalized cross validation score

\begin{equation}
\mathcal{V}_g = \frac{n\sum_{i=1}^n (y_i - \widehat{f}(x_i))^2}{[n-tr(\mathbf{A})]^2}
\end{equation}\label{GeneralizedCrossValidation}

The takeaway from this is that when a model is fit using \texttt{mgcv}, the user does not need to select $\lambda$, instead \texttt{mgcv} finds a solution that minimized the generalized cross validation score.

%$$
%y_i = f_i(x) + e_i
%$$

%where $y_i$ is a response variable, $x$ is a covariate, $f$ is a smooth function the $e_i$ are independent $\mathcal{N}(0,\sigma^2)$ random variables \cite{wood-2017}. (Much of the next discussion is taken from \cite{wood-2017})

%We require that $f$ can be represented as a linear model. The problem then is choose a *basis* for $f$, which requires selecting an appropriate set of *basis functions*. Suppose $b_j$ is the $j^th$ basis function, then we write $f$ as

%$$
%f = \sum_{j=1}^k b_j(x) \beta_j
%$$
%where $\beta_j$ are unknown parameters. Then 
%$$
%y_i = \sum_{j=1}^k b_j(x) \beta_j + e_i
%$$
%is a linear model.

The \texttt{mgcv} \texttt{gam} function uses the \texttt{s(x, ...)} syntax to define a smoothing function over a covariate, and \texttt{...} are additional arguments, such as \texttt{bs='bs'} that specify the basis functions. The \texttt{s(x, ...)} syntax also accepts an argument \texttt{k}, which specifies the dimension of the basis used to represent the smooth term \cite{wood-2017}.  

Related to \texttt{k} is the concept of effective degrees of freedom, where the upper bound for effective degrees of freedom is \texttt{k-1} - the basis dimension minus of d.f. due to identifiability constraints on the smooth terms. Effective degrees of freedom is usually computed from the data and is not easily defined; a simple way to think of the effective degrees of freedom is to consider a mixed effects model with $M$ fixed effect parameters and $p$ random effect parameters. The effective degrees of freedom $\tau$ will be constrained by $M \le \tau \le M+p$ where in the equality $M=\tau$ the model behaves as if there were no random effects, while when $\tau = M+p$ the random effects behave as if they were fixed effects.

In practice, it is recommended that \texttt{k} be set to be much larger than the effective degrees of freedom; in other words, when using \texttt{mgcv} it is best practice to fit a model using an arbitrary \texttt{k} to determine effective degrees of freedom for the smoothing terms, then refit the model with an appropriately large \texttt{k}. That is an undesirable practice in applying \texttt{mgcv} to smooth multiple harvest passes to provide bases for functional data analysis, so we explore in this section whether the default \texttt{k} values (which vary depending on the choice of basis, but in general default to \texttt{k=10}) is sufficiently flexible to fit a single harvest pass, and to compare \texttt{mgcv} fits to unpenalized spline fits using the \texttt{lm} function and the \texttt{splines} library.

<<GAMSmoothing, echo=FALSE>>=
library(mgcv)
bs16.gam <- gam(Yield ~ s(Easting, bs='bs',k=12+4), data=SinglePass.dat)
GAMSmooth <- data.frame(
  Easting = seq(min(SinglePass.dat[,'Easting']), max(SinglePass.dat[,'Easting']) ,
                len = length(SinglePass.dat[,'Yield']))
)
GAMSmooth[,'Yield'] <- predict(bs16.gam, newdata = GAMSmooth)
SinglePass.dat[,'Method'] = "Original Data"
NSplines[,'Method'] = "lm ns, df=12+1"
BSplines[,'Method'] = "lm bs, df=12+3"
GAMSmooth[,'Method'] = "gam bs, k=16"
Comparison1.dat <- rbind(SinglePass.dat[,c("Easting","Yield","Method")],
                         NSplines,
                         BSplines,
                         GAMSmooth)
Comparison1.dat[,'Method'] <- factor(Comparison1.dat[,'Method'], levels=c("Original Data","lm ns, df=12+1","lm bs, df=12+3","gam bs, k=16"))
Comparison1.dat <- Comparison1.dat[Comparison1.dat[,'Method']!="Original Data",]
#summary(bs16.gam)
#anova(bs16.gam)
#anova(ns12_1.lm)
#anova(bs12_3.lm)
@

\begin{figure}\centering
<<ComparingSplineFits, fig=TRUE, echo=FALSE, width=8, height=3>>=
ggplot(Comparison1.dat, aes(Easting, Yield)) + 
   geom_point(aes(Easting, Yield),size = 1, data=SinglePass.dat, color=cbPalette[1]) + 
   geom_line(linewidth=1, aes(color=Method, linetype=Method)) +
   scale_colour_manual(values=cbPalette[2:11])
@
\caption{Comparable smoothing interpolation using \texttt{lm} and \texttt{mgcv} \texttt{gam}. Arguments to the spline and \texttt{mgcv} functions where chosen to produce number of basis functions comparable to Figure \ref{PiecewiseLinear}. Arguments to \texttt{ns} and \texttt{bs} have are specified as \texttt{df} and require larger values to produce a number of basis function as Figure \ref{PiecewiseLinear} while the \texttt{k} argument specifies a maxim degrees of freedom for the smoothing component of an \texttt{mgcv} fit. Values we chosen by inspection to produce similar curves. Note that the curves almost perfectly overlap in the center of the graph; constraints on the bases functions affect how the functions behave at the endpoints.} \label{ComparingSplineFits}
\end{figure}

In Figure \ref{ComparingSplineFits}, we compare \texttt{ns} and \texttt{bs} fit to the single swath example and and equivalent b-spline fit using \texttt{mgcv} \texttt{gam}. We see that the \texttt{ns} fit with \texttt{df=12+1}, \texttt{bs} fit with \texttt{df=12+3} and \texttt{mgcv} fit using the arguments \texttt{s(Easting,bs='bs',k=12+4)} all produce interpolating functions that are nearly identical except small differences at the edges. The \texttt{mgcv} model reports an effective degrees for freedom for the smooth term of 14.34, while anova of the \texttt{bs} model with \texttt{df=12+3} has, as expected, a model degrees of freedom of 15. 

Why is there a difference in degrees of freedom for the basis splines model, \texttt{lm} plus \texttt{bs} and  \texttt{mgcv} with \texttt{bs='ba,k=12+4}. From \cite{wood-2017}, we consider the bias corrected fitted values 
\begin{equation}
   \tilde{\mathbf{\mu}} = \widehat{\mathbf{\mu}} + \widehat{\mathbf{b}} = 2 \widehat{\mathbf{\mu}} - \mathbf{A} \widehat{\mathbf{\mu}} = \left( 2 \mathbf{A} - \mathbf{A} \mathbf{A}\right) \mathbf{y}.
   \label{EffectiveDF}
\end{equation}

with $\mathbf{A} = \mathbf{X} \left(\mathbf{X}^t \mathbf{X} + \mathbf{S}_{\lambda}\right)^{-1} \mathbf{X}^t$, while $\mathbf{b}=\mathbf{\mu}-\mathbf{A}\mathbf{\mu}$ represents a smoothing bias, with estimate $\widehat{\mathbf{b}}=\widehat{\mathbf{\mu}}-\mathbf{A}\widehat{\mathbf{\mu}}$. In this case, $$\mathbf{A}$$ is the influence matrix for a Gaussian additive model, and $2 \mathbf{A} - \mathbf{A}\mathbf{A}$ is a 'bias corrected' influence matrix. The trace of this matrix can be taken as the degrees of freedom for the associated bias corrected model. Let $\mathbf{D}$ be the matrix of basis function coefficients, such that $\mathbf{S} = \mathbf{D}^t\mathbf{D}$

In the case of generalized additive models, the problem to be solved is essentially an penalized regression model \cite{wood-2017}, minimizing
\begin{equation}
   || \mathbf{y} - \mathbf{X} \mathbf{\beta} ||^2 + \lambda \mathbf{\beta}^t \mathbf{S} \mathbf{\beta}
\end{equation}
with respect to $\widehat{\beta}$. When $\lambda=0$, this is an unpenalized least squares estimate; in such a case, we would expect to find \texttt{mgcv} with \texttt{(bs='bs',k=15)} to be equal to \texttt{lm} plus \texttt{bs(...,df=15)} - that is, we have an unpenalized generalized additive model. In the case of \texttt{mgcv} we find $\lambda$ to be estimated by minimizing the generalized cross-validation score (\cite{wood-2017}) 
\begin{equation}
   \mathcal{V}_g = \frac{n \sum_{i=1}^n \left( y_i - \widehat{f}_i \right)^2}{\left[n - tr\left( \mathbf{A} \right)^2 \right]}
\end{equation}
where $\widehat{f}$ is estimated from the data.


In our examples, we provide dimension bases parameters such as \texttt{df} or \texttt{k}. But for a single functional data analysis that might require smoothing a large number (say, dozens) of harvest passes, an automatic smoothing method would be preferred. It may not be practical to find an optimal smoothing parameters for each of dozens of harvest passes, if multiple trials are to be analyzed. In our example data, we have 24 harvest passes for the first example, and 24 harvest passes for the second example. We would prefer to automate smoothing as much as possible to make analysis of multiple trials practical.

<<ComputeMGCVBases, echo=FALSE>>=
default.gam <- gam(Yield ~ s(Easting),data=SinglePass.dat)
MGCVDefault <- data.frame(
  Easting = seq(min(SinglePass.dat[,'Easting']), max(SinglePass.dat[,'Easting']) ,
                len = length(SinglePass.dat[,'Yield']))
)
MGCVDefault[,'Yield'] <- predict(default.gam, newdata = MGCVDefault)

default.gambs <- gam(Yield ~ s(Easting, bs='bs'),data=SinglePass.dat)
MGCVBSDefault <- data.frame(
  Easting = seq(min(SinglePass.dat[,'Easting']), max(SinglePass.dat[,'Easting']) ,
                len = length(SinglePass.dat[,'Yield']))
)
MGCVBSDefault[,'Yield'] <- predict(default.gambs, newdata = MGCVBSDefault)

default.gamps <- gam(Yield ~ s(Easting, bs='ps'),data=SinglePass.dat)
MGCVPSDefault <- data.frame(
  Easting = seq(min(SinglePass.dat[,'Easting']), max(SinglePass.dat[,'Easting']) ,
                len = length(SinglePass.dat[,'Yield']))
)
MGCVPSDefault[,'Yield'] <- predict(default.gamps, newdata = MGCVPSDefault)

default.gamad <- gam(Yield ~ s(Easting, bs='ad'),data=SinglePass.dat)
MGCVADDefault <- data.frame(
  Easting = seq(min(SinglePass.dat[,'Easting']), max(SinglePass.dat[,'Easting']) ,
                len = length(SinglePass.dat[,'Yield']))
)
MGCVADDefault[,'Yield'] <- predict(default.gamad, newdata = MGCVADDefault)

SinglePass.dat[,'Method'] = "Original Data"
MGCVDefault[,'Method'] = "mgcv default"
MGCVBSDefault[,'Method'] = "mgcv bs"
MGCVPSDefault[,'Method'] = "mgcv ps"
MGCVADDefault[,'Method'] = "mgcv ad"
ComparisonMGCV.dat <- rbind(SinglePass.dat[,c("Easting","Yield","Method")],
                         MGCVDefault,
                         MGCVBSDefault,
                         MGCVPSDefault,
                         MGCVADDefault)
ComparisonMGCV.dat[,'Method'] <- factor(ComparisonMGCV.dat[,'Method'], levels=c("Original Data",
                                                                 "mgcv default",
                                                                 "mgcv bs",
                                                                 "mgcv ps",
                                                                 "mgcv ad"))
ComparisonMGCV.dat <- ComparisonMGCV.dat[ComparisonMGCV.dat[,'Method']!="Original Data",]
@

\begin{figure}\centering
<<ComparingGAMSplines, fig=TRUE, echo=FALSE, width=8, height=3>>=
ggplot(ComparisonMGCV.dat, aes(Easting, Yield)) + 
   geom_point(aes(Easting, Yield),size = 1, data=SinglePass.dat, color=cbPalette[1]) + 
   geom_line(linewidth=1, aes(color=Method, linetype=Method)) +
   scale_colour_manual(values=cbPalette[2:11])
@
\caption{Different spline smoothers using \texttt{gam} with default \texttt{k} values. The default for  \texttt{mgcv} is to use "thin-plate" splines. Fits using "bs" and "ns" splines are similar to the default, but not exactly overlapping, Similar \texttt{k} value used as default for the first three bases, but each bases has different constraints on the endpoints. The  "ad" option uses a larger default \text{k} value and results in a curve that hews more closely to the original data, to the point of overfitting.}\label{ComparingGAMSplines}
\end{figure}

In Figure \ref{ComparingGAMSplines} we compare default smoothing bases functions fit using \texttt{mgcv} \texttt{gam} with different arguments to \texttt{s(..., bs = "*")}, where \texttt{*} is one of \texttt{"bs"} for ordinary basis splines, \texttt{"ps"} for penalized splines and \texttt{"ad"} for adaptive splines. The default basis for \texttt{s(...)} with no arguments are thin-plate splines\cite{wood-2017}. Thin-plate splines \cite{wood-2003} are a form of penalized splines that provide a general solution to smoothing functions of multiple variables \cite{wood-2017}. No other arguments are supplied to the \texttt{s} function, other than the covariate \texttt{Easting}, that is, the function invocation takes the form \texttt{gam(Yield \textasciitilde s(Easting))} or \texttt{gam(Yield \textasciitilde s(Easting,bs='bs'))}, etc..

We should note in Figure \ref{ComparingGAMSplines} there appears to be no line for \texttt{bs='bs'}. This is because the smooth line produced using \texttt{bs='bs'} is identical to the smooth line produced by \texttt{bs='ps'}; the penalty associated with penalized splines appears to be subsumed by the smoothness penalty $\lambda$ applied in the general \texttt{mgcv} model.

<<ComputeBRMBases, echo=FALSE>>=
if(!file.exists("ComputeBRMBases")) {
  default.brm <- brm(Yield ~ s(Easting),silent=2, refresh = 0, data=SinglePass.dat)
  BRMDefault <- data.frame(
    Easting = seq(min(SinglePass.dat[,'Easting']), max(SinglePass.dat[,'Easting']) ,
                  len = length(SinglePass.dat[,'Yield']))
  )
  BRMDefault[,'Yield'] <- predict(default.brm, newdata = BRMDefault)
  #Note that BRM predict gives us upper and lower confidence bands.
  names(BRMDefault)[2] <- "Yield"

  default.brmbs <- brm(Yield ~ s(Easting, bs='bs'),silent=2, refresh = 0, data=SinglePass.dat)
  BRMBSDefault <- data.frame(
    Easting = seq(min(SinglePass.dat[,'Easting']), max(SinglePass.dat[,'Easting']) ,
                  len = length(SinglePass.dat[,'Yield']))
  )
  BRMBSDefault[,'Yield'] <- predict(default.brmbs, newdata = BRMBSDefault)
  names(BRMBSDefault)[2] <- "Yield"

  default.brmps <- brm(Yield ~ s(Easting, bs='ps'),silent=2, refresh = 0, data=SinglePass.dat)
  BRMPSDefault <- data.frame(
    Easting = seq(min(SinglePass.dat[,'Easting']), max(SinglePass.dat[,'Easting']) ,
                  len = length(SinglePass.dat[,'Yield']))
  )
  BRMPSDefault[,'Yield'] <- predict(default.brmps, newdata = BRMPSDefault)
  names(BRMPSDefault)[2] <- "Yield"

  #we get an error, Can not convert this smooth class to a random effect
  #default.brmad <- brm(Yield ~ s(Easting, bs='ad'),silent=2, refresh = 0, data=SinglePass.dat)
  #BRMADDefault <- data.frame(
  #  Easting = seq(min(SinglePass.dat[,'Easting']), max(SinglePass.dat[,'Easting']) ,
  #                len = length(SinglePass.dat[,'Yield']))
  #)
  #BRMADDefault[,'Yield'] <- predict(default.brmad, newdata = BRMADDefault)
  #names(BRMADDefault)[2] <- "Yield"

  BRMDefault[,'Method'] = "brm default"
  BRMBSDefault[,'Method'] = "brm bs"
  BRMPSDefault[,'Method'] = "brm ps"
  #BRMADDefault[,'Method'] = "brm ad"
  ComparisonBRM.dat <- rbind(SinglePass.dat[,c("Easting","Yield","Method")],
                           BRMDefault[,c("Easting","Yield","Method")],
                           BRMBSDefault[,c("Easting","Yield","Method")],
                           BRMPSDefault[,c("Easting","Yield","Method")])
#                           BRMADDefault[,c("Easting","Yield","Method")])
  ComparisonBRM.dat[,'Method'] <- factor(ComparisonBRM.dat[,'Method'],levels=c("Original Data",
                                                                   "brm default",
                                                                   "brm bs",
                                                                   "brm ps"))
#                                                                   "brm ad"))
  save(ComparisonBRM.dat, file ="ComputeBRMBases")
} else {
  load(file="ComputeBRMBases")
}
@

\begin{figure}\centering
<<ComparingBRMSplines, fig=TRUE, echo=FALSE, width=8, height=3>>=
ComparisonBRM.dat <- ComparisonBRM.dat[ComparisonBRM.dat[,'Method']!="Original Data",]
ggplot(ComparisonBRM.dat, aes(Easting,Yield)) + 
   geom_point(aes(Easting, Yield),size = 1, data=SinglePass.dat, color=cbPalette[1]) + 
   geom_line(linewidth=1, aes(color=Method, linetype=Method)) +
   scale_colour_manual(values=cbPalette[2:11])
@
\caption{Different spline smoothers using \texttt{brm} default \texttt{k} values. \texttt{brm} uses \texttt{mgcv} to code the bases functions, so we expect to see a great similarity between this graph and Figure \ref{ComparingGAMSplines}. However, we find an error when trying to implement the "ad" basis in \texttt{brm}, so that curve is not included in this graph.}\label{ComparingBRMSplines}
\end{figure}

The \texttt{brms} library provides an interface for specifying multilevel Bayesian model, implemented in the \texttt{stan} programming language \cite{burkner_2017}. While we do not necessarily intend to analyze strip trials in a fully Bayesian manner, the \texttt{brm} function also supports the \texttt{s} smoothing syntax; where the \texttt{s} functionality is derived from \texttt{mgcv}. Figure \ref{ComparingBRMSplines} compares different spline bases when computed used \texttt{brm} as opposed to \texttt{mgcv}. We should note that when we use the \texttt{bs='ad'} syntax in a \texttt{brm} model, we generate an error
\begin{verbatim}
   Can not convert this smooth class to a random effect.
\end{verbatim}

<<CreatePredictionDataFrames, echo=FALSE, include=FALSE>>=
#Create a data frame to hold the predictions from different linear models.
#k = c(1,6,11,16,21,26)
knots = c(3,6,12,18,24,30,36)
#k = c(11,16,26)
#k = c(1,6,11,16,21,26,31,36,41,46)
PredictionsNS.dat <- data.frame(
  Easting = rep(SinglePass.dat[,'Easting'], length(knots))
)
PredictionsNS.dat[,'Knots'] <- factor(rep(knots, each = length(SinglePass.dat[,'Easting'])))
PredictionsNS.dat[,'Yield'] <- NA

PredictionsBS.dat <- data.frame(
  Easting = rep(SinglePass.dat[,'Easting'], length(knots))
)
PredictionsBS.dat[,'Knots'] <- factor(rep(knots, each = length(SinglePass.dat[,'Easting'])))
PredictionsBS.dat[,'Yield'] <- NA

PredictionsGAM.dat <- data.frame(
  Easting = rep(SinglePass.dat[,'Easting'], length(knots))
)
PredictionsGAM.dat[,'Knots'] <- factor(rep(knots, each = length(SinglePass.dat[,'Easting'])))
PredictionsGAM.dat[,'Yield'] <- NA

PredictionsGAMBS.dat <- data.frame(
  Easting = rep(SinglePass.dat[,'Easting'], length(knots))
)
PredictionsGAMBS.dat[,'Knots'] <- factor(rep(knots, each = length(SinglePass.dat[,'Easting'])))
PredictionsGAMBS.dat[,'Yield'] <- NA

PredictionsGAMPS.dat <- data.frame(
  Easting = rep(SinglePass.dat[,'Easting'], length(knots))
)
PredictionsGAMPS.dat[,'Knots'] <- factor(rep(knots, each = length(SinglePass.dat[,'Easting'])))
PredictionsGAMPS.dat[,'Yield'] <- NA

PredictionsGAMAD.dat <- data.frame(
  Easting = rep(SinglePass.dat[,'Easting'], length(knots))
)
PredictionsGAMAD.dat[,'Knots'] <- factor(rep(knots, each = length(SinglePass.dat[,'Easting'])))
PredictionsGAMAD.dat[,'Yield'] <- NA

PredictionsBRM.dat <- data.frame(
  Easting = rep(SinglePass.dat[,'Easting'], length(knots))
)

PredictionsBRM.dat[,'Knots'] <- factor(rep(knots, each = length(SinglePass.dat[,'Easting'])))
PredictionsBRM.dat[,'Yield'] <- NA

PredictionsFDA.dat <- data.frame(
  Easting = rep(SinglePass.dat[,'Easting'], length(knots))
)

PredictionsFDA.dat[,'Knots'] <- factor(rep(knots, each = length(SinglePass.dat[,'Easting'])))
PredictionsFDA.dat[,'Yield'] <- NA
@

<<OCVDataFrame, echo=FALSE, include=FALSE>>=
#create a data frame to hold AdjRSqr values
SplitsComp.dat <- data.frame(
  Knots = rep(knots,7),
  Method = rep(c("lm ns", "lm bs", "mgcv default", "mgcv bs", "mgcv ps","mgcv ad", "fda"), each= length(knots))
)
SplitsComp.dat[,'Method'] <- factor(SplitsComp.dat[,'Method'], 
                                levels=c("lm ns", "lm bs", "mgcv default", "mgcv bs", "mgcv ps", "mgcv ad", "fda"))

SplitsComp.dat[,'OCV'] <- NA
@

<<NSRegressionSmoothingCode, echo=FALSE, include=FALSE>>=
# ns Regression Smoothing
for(i in 2:length(knots)) {
  current.lm <- lm(Yield ~ ns(Easting, df=knots[i]+1), data=SinglePass.dat)
  PredictionsNS.dat[PredictionsNS.dat[,'Knots']==knots[i],'Yield'] <- predict(current.lm)
  mask <- SplitsComp.dat[,'Knots'] == knots[i] & SplitsComp.dat[,'Method'] == "lm ns"
  
  sum <- 0
  for(j in 1:dim(SinglePass.dat)[1]) {
    current.d <- SinglePass.dat[-j,]
    current.model <- lm(Yield ~ ns(Easting, df=knots[i]+1), data=current.d)
    new.dat <- SinglePass.dat[j,]
    predicted <- predict(current.model, newdata = new.dat)
    diff <- predicted - new.dat[,'Yield']
    sum  <- sum+(diff)^2
  }
  SplitsComp.dat[mask,'OCV'] <- sum/dim(SinglePass.dat)[1]
}
@

<<BSRegressionCode, echo=FALSE, include=FALSE>>=
# bs Regression Smoothing
for(i in 1:length(knots)) {
  #df degrees of freedom; one can specify df rather than knots; bs() then chooses df-degree (minus 
  #one if there is an intercept) knots at suitable quantiles of x (which will ignore missing values).
  #The default, NULL, takes the number of inner knots as length(knots). If that is zero as per 
  #default, that corresponds to df = degree - intercept.
  
  current.lm <- lm(Yield ~ bs(Easting, df=knots[i]+3), data=SinglePass.dat)
  PredictionsBS.dat[PredictionsBS.dat[,'Knots']==knots[i],'Yield'] <- predict(current.lm)
  mask <- SplitsComp.dat[,'Knots'] == knots[i] & SplitsComp.dat[,'Method'] == "lm bs"
  current.summary <- summary(current.lm)
  sum <- 0
  for(j in 1:dim(SinglePass.dat)[1]) {
    current.d <- SinglePass.dat[-j,]
    current.model <- lm(Yield ~ bs(Easting, df=knots[i]+3), data=current.d)
    new.dat <- SinglePass.dat[j,]
    predicted <- predict(current.model, newdata = new.dat)
    diff <- predicted - new.dat[,'Yield']
    sum  <- sum+(diff)^2
  }
  SplitsComp.dat[mask,'OCV'] <- sum/dim(SinglePass.dat)[1]
}
@

<<GAMSmoothingCode, echo=FALSE, include=FALSE>>=
# mgcv gam Smoothing
#From `mgcv:gam` help, `choose.k`
#In practice k-1 (or k) sets the upper limit on the degrees of freedom associated with an s smooth (1 degree of freedom is usually lost to the identifiability constraint on the smooth).
for(i in 1:length(knots)) {
  current.lm <- gam(Yield ~ s(Easting, k=knots[i]+4), data=SinglePass.dat)
  #gam.check(current.lm)
  PredictionsGAM.dat[,'Yield'][PredictionsGAM.dat[,'Knots']==knots[i]] = predict(current.lm)
  mask <- SplitsComp.dat[,'Knots'] == knots[i] & SplitsComp.dat[,'Method'] == "mgcv default"
  sum <- 0
  for(j in 1:dim(SinglePass.dat)[1]) {
    current.d <- SinglePass.dat[-j,]
    current.model <- gam(Yield ~ s(Easting, k=knots[i]+4), data=current.d)
    new.dat <- SinglePass.dat[j,]
    predicted <- predict(current.model, newdata = new.dat)
    diff <- predicted - new.dat[,'Yield']
    sum  <- sum+(diff)^2
  }
  SplitsComp.dat[mask,'OCV'] <- sum/dim(SinglePass.dat)[1]
}
@


<<GAMBSSmoothingCode, echo=FALSE, include=FALSE>>=
# GAM bs
#I get an error with k=3 with GAM ps
for(i in 1:length(knots)) {
  current.lm <- gam(Yield ~ s(Easting, bs='bs', k=knots[i]+4), data=SinglePass.dat)
  #gam.check(current.lm)
  PredictionsGAMBS.dat[,'Yield'][PredictionsGAMBS.dat[,'Knots']==knots[i]] = predict(current.lm)
  mask <- SplitsComp.dat[,'Knots'] == knots[i] & SplitsComp.dat[,'Method'] == "mgcv bs"
  sum <- 0
  for(j in 1:dim(SinglePass.dat)[1]) {
    current.d <- SinglePass.dat[-j,]
    current.model <- gam(Yield ~ s(Easting, bs='bs',k=knots[i]+4), data=current.d)
    new.dat <- SinglePass.dat[j,]
    predicted <- predict(current.model, newdata = new.dat)
    diff <- predicted - new.dat[,'Yield']
    sum  <- sum+(diff)^2
  }
  SplitsComp.dat[mask,'OCV'] <- sum/dim(SinglePass.dat)[1]
}
@

<<GAMPSSmoothingCode, echo=FALSE, include=FALSE>>=
# GAM ps
#I get an error with k=3 with GAM ps
for(i in 2:length(knots)) {
  current.lm <- gam(Yield ~ s(Easting, bs='ps', k=knots[i]+4), data=SinglePass.dat)
  #gam.check(current.lm)
  PredictionsGAMPS.dat[,'Yield'][PredictionsGAMPS.dat[,'Knots']==knots[i]] = predict(current.lm)
  mask <- SplitsComp.dat[,'Knots'] == knots[i] & SplitsComp.dat[,'Method'] == "mgcv ps"
  sum <- 0
  for(j in 1:dim(SinglePass.dat)[1]) {
    current.d <- SinglePass.dat[-j,]
    current.model <- gam(Yield ~ s(Easting, bs='ps',k=knots[i]+4), data=current.d)
    new.dat <- SinglePass.dat[j,]
    predicted <- predict(current.model, newdata = new.dat)
    diff <- predicted - new.dat[,'Yield']
    sum  <- sum+(diff)^2
  }
  SplitsComp.dat[mask,'OCV'] <- sum/dim(SinglePass.dat)[1]
}
@

<<GAMADSmoothingCode, echo=FALSE, include=FALSE>>=
# GAM ad
#I get an error with k<12
for(i in 3:length(knots)) {
  current.lm <- gam(Yield ~ s(Easting, bs='ad', k=knots[i]+4), data=SinglePass.dat)
  #gam.check(current.lm)
  PredictionsGAMAD.dat[,'Yield'][PredictionsGAMAD.dat[,'Knots']==knots[i]] = predict(current.lm)
  mask <- SplitsComp.dat[,'Knots'] == knots[i] & SplitsComp.dat[,'Method'] == "mgcv ad"
  sum <- 0
  for(j in 1:dim(SinglePass.dat)[1]) {
    current.d <- SinglePass.dat[-j,]
    current.model <- gam(Yield ~ s(Easting, bs='ad',k=knots[i]+4), data=current.d)
    new.dat <- SinglePass.dat[j,]
    predicted <- predict(current.model, newdata = new.dat)
    diff <- predicted - new.dat[,'Yield']
    sum  <- sum+(diff)^2
  }
  SplitsComp.dat[mask,'OCV'] <- sum/dim(SinglePass.dat)[1]
}
@

<<FDASmoothingCode, echo=FALSE, include=FALSE>>=
# FDA Smoothing
times_basis = seq(min(SinglePass.dat[,'Easting']),max(SinglePass.dat[,'Easting']),length.out=200)
for(i in 1:length(knots)) {
  Knots = seq(min(SinglePass.dat[,'Easting']),max(SinglePass.dat[,'Easting']),length.out=knots[i]) #Location of knots
  n_knots = length(Knots) #Number of knots
  n_order = 4 # order of basis functions: cubic bspline: order = 3 + 1
  n_basis = length(Knots) + n_order - 2;
  basis = create.bspline.basis(c(min(times_basis),max(times_basis)),n_basis, n_order, Knots)
  single.smooth <- smooth.basis(argvals = SinglePass.dat[,'Easting'], y = SinglePass.dat[,'Yield'], fdParobj = basis)
  
  PredictionsFDA.dat[,'Yield'][PredictionsFDA.dat[,'Knots']==knots[i]] = predict(single.smooth)
  mask <- SplitsComp.dat[,'Knots'] == knots[i] & SplitsComp.dat[,'Method'] == "fda"
  sum <- 0
  for(j in 1:dim(SinglePass.dat)[1]) {
    current.d <- SinglePass.dat[-j,]
    single.smooth <- smooth.basis(argvals = current.d[,'Easting'], y = current.d[,'Yield'], fdParobj = basis)
    new.dat <- SinglePass.dat[j,]
    predicted <- predict(single.smooth, newdata = new.dat[,c('Easting')])
    diff <- predicted - new.dat[,'Yield']
    sum  <- sum+(diff)^2
  }
  SplitsComp.dat[mask,'OCV'] <- sum/dim(SinglePass.dat)[1]
}
@

<<BRMSmoothingCode, include=FALSE,echo=FALSE>>=
if(!file.exists("PredictionsBRM")) {
  # BRM Smoothing
  for(i in 1:length(knots)) {
    k=knots[i]+4
    #silent=2, refresh=0 to suppress messages
    current.lm <- brm(Yield ~ s(Easting, k=k),silent=2, refresh = 0, data=SinglePass.dat)
    PredictionsBRM.dat[,'Yield'][PredictionsBRM.dat[,'Knots']==knots[i]] = predict(current.lm)
  }
  save(PredictionsBRM.dat, file="PredictionsBRM")
} else {
  load(file="PredictionsBRM")
}
@

%One measure of the "spread" of the data is given by the sum of squared deviation from the mean, or the corrected sum of squares.
%$$
%SS_{total} = \sum (y(x_j) - \bar{y(x_j)})^2
%$$
%In general, $SS_{total}$ can be decomposed into two components; the variance explained by the function or model $f$ and the residual or error variance. We write these as
%$$
%\begin{aligned}
%SS_{model} &= \sum (f(x_j) - \bar{y(x_j)})^2 \\
%SS_{residual} &= sum (y(x_j) = f(x_j))^2
%\end{aligned}
%$$
%These two quantities combined to equal the total sums of squares, so
%$$
%SS_{total} = SS_{model} + SS_{residual}
%$$

%When working with linear models, the ratio of model sum of squares to the total sum of squares is written as
%$$
%r^2 = \frac{SS_{model}}{SS_{total}}
%$$ 

%where $r$ is the correlation coefficient. Generally, $r^2$ can be taken as the proportion of total variation of $Y$ that is explained by the function or model.

%By itself, $r^2$ is a poor measure for model comparison. $r^2$ will generally increase as the number of parameters in a function $f$ is increased, to the point that if the data contain $n$ data points, a linear model with $n-1$ parameters will result in a perfect fit, with $r^2 = 1$.

%Frequently, $r^2$ is adjusted for the number of parameters is incorporated into the calculation of $r^2$. We write
%$$
%r_{adjusted}^2 = 1-\frac{SS_{residual}/(n-p)}{SS_{total}(n-1)}
%$$
%where $p$ is the number of parameter in the model and $n$ is the total number of observations. \cite{faraway-2000}. 

%In the following sections, we will use $r_{adjusted}^2$ for comparing different forms of $f$.



%Ordinary or leave-on-out cross validation can be computationally intensive, since it leaves out each data point and fitting a new model for each data point. In strip trials with yield monitor data, the number of data rows can easily be on the order of several thousand data points. A compromise is $k$-fold cross validation. The data are divided into "folds" and for each fold, the data corresponds to the fold is left out of the fit as testing data; the remaining data are used to fit a model. The fitted model from the training data is used to product the observations left out in the test data and the actual observations are compared to the predicted values.

\subsection{Comparing different smoothing implementations}

The data from Figure \ref{RawSwath} is used to compare accuracy of interpolation of different smoothing functions, using ordinary cross validation as a scoring function. Wood \cite{wood-2017} p. 170 defines the ordinary cross validation score \ref{OrdinaryCrossValidation}
as a measure of goodness of fit of a functional representation. The proposed model $f$ is fit to the original data minus a single point $x_j$ and the value predicted by the model fit the to reduced data set $\widehat{f}_i^{[-j]}$ is compared to the observed value $y_i(x_j)$. This process is repeated for each $d_j$ in the original data. This is sometimes referred to as leave-one-out cross validation (LOOCV). This score provides a basis for comparing the ability of different models to interpolate the original data.

We use \texttt{lm} in R to fit polynomials of degrees \texttt{(3,6,12,18,24,30,36)} using the \texttt{poly} function as part of the model formula (e.g. \texttt{Yield \textasciitilde poly(Easting,3)}). Splines are also fit to the data using \texttt{lm} in conjunction with \texttt{ns} (natural splines) and \texttt{bs} (basis splines) from the \texttt{splines} library \cite{perperoglou-2019, rcore}, with arguments defining the number of knots \texttt{k} in the set \texttt{(3,6,12,18,24,30,36)}; we use \texttt{ns(df=k+1)} and \texttt{bs(df=k+3)}. 

We also smooth the data using \texttt{gam} from the \texttt{mgcv} library \cite{wood-2017} with \texttt{k} in the set \texttt{(3,6,12,18,24,30,36)+4}, following the results from \ref{ComparingSplineFits}. We include \texttt{mgcv} fits with arguments \texttt{bs='bs'}, \texttt{bs='ps'} and \texttt{bs='ad'} to fit b-splines, p-splines and adaptive splines, respectively. We find that the \texttt{bs='ps'} fails for \texttt{k+4=7} and \texttt{bs='ad'} fails for \texttt{k} < 10. 

We also fit using the \texttt{brm} function from the \texttt{brms} \cite{burkner_2017} library with \texttt{s} argument \texttt{k} in the set \texttt{(3,6,12,18,24,30,36)}. We do not compute ordinary cross-validation for \texttt{brm} due to time constraints; each call to \texttt{brm} invokes a C++ compiler call and numerous MCMC iterations. 

We also fit interpolation curves using the \texttt{fda} \cite{ramsay-2024} package. The \texttt{fda} package is useful for various forms of functional data analysis and includes interpolating functions. The \texttt{fda} package requires more function calls than the other packages considered, First, the user must create an explicit set of points to evaluate the function, plus an explicit vector of knots. After these were created, we called the the \texttt{create.bspline.basis} to create a b-spline basis and \texttt{smooth.basis} for compute coefficients. We use for the number of knots in the \texttt{fda} smooth basis values in \texttt{(3,6,12,18,24,30,36)}.

%For each model fit, we obtain an adjusted $R^2$. This is preferable to unadjusted $R^2$ as a method for comparing the fit of linear and related models because it accounts for the number of terms in the model. The unadjusted $R^2$ increases with each term added to a model, to the point that a model with sufficient parameters can be made to perfectly fit any data. Adjusted $R^2$ also tends to increase with additional model terms, but the increase becomes incrementally smaller with the number of parameters and adjusted $R^2$ can become asymptotic or even decrease with increasing model complexity.


%The PRESS statistic offers another option for model selection. Briefly, PRESS is a leave-one-out-and-refit methodology that find the error of a model by leaving one element out, one at a time, and measures the prediction error of the left-out element. The PRESS statistic decreases with model complexity, and, similar to adjusted $R^2$, becomes asymptotic as model complexity increases.

%$$
%PRESS = \sum_{i=1}^n (y_i - \widehat{y}_{i, -i})^2
%$$


%<<SplitsCompAdjR2,eval=FALSE,echo=FALSE>>=
%ggplot(SplitsComp.dat, aes(Knots,AdjRSqr)) + 
%  geom_line(linewidth=1,aes(color=Method)) +
%geom_point(size=3,aes(color=Method,shape=Method)) + 
%scale_colour_manual(values=cbPalette) + 
%labs(colour = "Method", x="Knots", y="Adjusted R Squared", title = "Comparing smoothing methods")
%@
\begin{figure}\centering
<<SplitsCompOCV, fig=TRUE, echo=FALSE, width=8, height=6>>=
ggplot(SplitsComp.dat, aes(Knots,OCV)) + 
  geom_line(linewidth=1, aes(color=Method)) +
geom_point(size=3, aes(color=Method, shape=Method)) + 
scale_colour_manual(values=cbPalette) + 
labs(colour = "Method", x="Knots", y="Ordinary Cross-Validation", title = "Comparing smoothing methods")
@
\caption{Ordinary cross validation for increasing number of degrees of freedom specified for select smoothing functions. Arguments to the smoothing functions where chosen to provide similar numbers of knots (see Figure \ref{ComparingSplineFits}). The models compared include splines fitted using \texttt{lm} as well as splines fitted using \texttt{mgcv}. Curves fit using the \texttt{fda} package to illustrate an additional option for finding bases functions for these data.}\label{SplitsCompOCV}
\end{figure}

Figure \ref{SplitsCompOCV} shows the ordinary cross-validation results for a range of selected smoothing functions fit as described above. We see that OCV scores reach a minimum at about 24 knots, while there is a point of diminishing returns at about 12 knots. This suggests our choice of 12 knots for illustration purposes in Figures \ref{PiecewiseLinear} and \ref{ComparingSplineFits} is close to an optimal compromise between smoothness and roughness. There is little difference in the interpolative accuracy of bases functions fit using the \texttt{fda} library when compared to \texttt{lm} with \texttt{splines} and with the \texttt{mcgv} library, so this package with not be further explored in this section, given that the other two methods are much easer to program.

\subsection{Comparing different implementations of the \texttt{s} syntax}

We wish to choose a library that uses the \texttt{s} syntax to specify a generic smoothing function that defaults to an acceptable degree of smoothing for any arbitrary noisy curve. That is, for automated smoothing of strip trial, we wish to be able to simplify specify R model formula using syntax of the form \texttt{Yield \textasciitilde s(Easting)} and have some confidence in the appropriate degree of smoothing provided, without being required to compute fit metrics such as ordinary cross-validation for each harvest pass. R functions that support the \texttt{s(...)} syntax includes the \texttt{gam} from the \texttt{mgcv} library  , the \texttt{brm} from the \texttt{brms} library  and \texttt{gam} from the \texttt{gam} library. 


<<GAMAndBRMDefaults, echo=FALSE, include=FALSE>>=
library(gam)
default.gam <- gam(Yield ~ s(Easting),data=SinglePass.dat)
GAMDefault <- data.frame(
  Easting = seq(min(SinglePass.dat[,'Easting']), max(SinglePass.dat[,'Easting']) ,
                len = length(SinglePass.dat[,'Yield']))
)
GAMDefault[,'Yield'] <- predict(default.gam, newdata = GAMDefault)
#gam conflicts with mgcv, so unload gam and reload mgcv
detach("package:gam", unload=TRUE)
library(mgcv)

default.brm <- brm(Yield ~ s(Easting), silent=2, refresh = 0, data=SinglePass.dat)
BRMDefault <- data.frame(
  Easting = seq(min(SinglePass.dat[,'Easting']), max(SinglePass.dat[,'Easting']) ,
                len = length(SinglePass.dat[,'Yield']))
)
BRMDefault[,'Yield'] <- predict(default.brm, newdata = BRMDefault)
@

<<BuildComparisonTable, include=FALSE, echo=FALSE>>=
#SinglePass.dat[,'Method'] = "Original Data"
#MGCVDefault[,'Method'] = "mgcv default"
GAMDefault[,'Method'] = "gam default"
BRMDefault[,'Method'] = "brm default"
Comparison2.dat <- rbind(SinglePass.dat[,c("Easting","Yield","Method")],
                         MGCVDefault,
                         GAMDefault,
                         BRMDefault)
Comparison2.dat[,'Method'] <- factor(Comparison2.dat[,'Method'],levels=c("Original Data",
                                                                 "mgcv default",
                                                                 "gam default",
                                                                 "brm default"))
Comparison2.dat <- Comparison2.dat[Comparison2.dat[,'Method']!="Original Data",]
@

\begin{figure}\centering
<<ComparingSSplines, fig=TRUE, echo=FALSE, width=8, height=3>>=
ggplot(Comparison2.dat, aes(Easting,Yield)) + 
   geom_point(aes(Easting, Yield),size = 1, data=SinglePass.dat, color=cbPalette[1]) + 
   geom_line(linewidth=1, aes(color=Method, linetype=Method)) +
   scale_colour_manual(values=cbPalette[2:11])
@
\caption{Different implementations of the \texttt{s()} specifier. When the \texttt{s} specifier is used without any arguments, \texttt{mgcv} defaults to thin-plate splines with \texttt{k=9} as an upper limit for the degrees of freedom for the smooth curve. \texttt{brm} uses \texttt{mgcv} to set up bases functions, so the curves for \texttt{mgcv} and \texttt{brm} are very similar. The \texttt{gam} library defaults to a smaller degrees of freedom for smoothing.}\label{ComparingSSplines}
\end{figure}

Figure \ref{ComparingSSplines} shows the default smoothing functions \texttt{s} from the \texttt{mgcv}, \texttt{brm} and \texttt{gam} libraries. There is little difference between \texttt{mgcv} and \texttt{brm}. This is not surprising, given that the \texttt{brm} \texttt{s} function is largely a wrapper for the \texttt{mgcv} \texttt{s} function \cite{brms_s}.  The default smoother is much more "smooth" for the \texttt{gam} function. It's not clear from the help page for \texttt{gam} what exactly is the default number of basis functions, but as we see in a later chapter (Figure \ref{Example1TrendAnalysisGAMdf}) that \texttt{gam} defaults to 3 degrees of freedom for the smooth term. This is much lower than the default for \texttt{mgcv}.

\subsection{Additional graphs}

\begin{figure}\centering
<<PredictionsNS, fig=TRUE, echo=FALSE, width=8, height=3>>=
ggplot(PredictionsNS.dat, aes(Easting,Yield)) + 
  #geom_line(aes(Easting,Yield),size = 0.5, data=SinglePass.dat, color=cbPalette[1]) +
  geom_point(aes(Easting, Yield),size = 1, data=SinglePass.dat, color=cbPalette[1]) + 
  geom_line(linewidth=1, aes(color=Knots, linetype=Knots)) + 
  scale_colour_manual(values=cbPalette[2:11]) + 
  labs(colour = "Knots", x="Easting", y="Yield", title = "Single swath, lm ns smoothing")
@
\caption{Smoothing curves for a single harvest swath using \texttt{lm} with \texttt{ns, df = Knots+1}}\label{PredictionsNS}
\end{figure}

\begin{figure}\centering
<<PredictionsBS, fig=TRUE, echo=FALSE, width=8, height=3>>=
ggplot(PredictionsBS.dat, aes(Easting,Yield)) + 
  #geom_line(aes(Easting,Yield),size = 0.5, data=SinglePass.dat, color=cbPalette[1]) + 
  geom_point(aes(Easting, Yield),size = 1, data=SinglePass.dat, color=cbPalette[1]) + 
  geom_line(linewidth=1, aes(color=Knots, linetype=Knots)) + 
  scale_colour_manual(values=cbPalette[2:11]) + 
  labs(colour = "Knots", x="Easting", y="Yield", title = "Single Swath, lm bs smoothing")
@
\caption{Smoothing curves for a single harvest swath using \texttt{lm} with \texttt{bs, df = Knots+3}}\label{PredictionsBS}
\end{figure}

\begin{figure}\centering
<<PredictionsGAM, fig=TRUE, echo=FALSE, width=8, height=3>>=
ggplot(PredictionsGAM.dat, aes(Easting,Yield)) + 
  #geom_line(aes(Easting,Yield),size = 0.5, data=SinglePass.dat, color=cbPalette[1]) +
  geom_point(aes(Easting, Yield),size = 1, data=SinglePass.dat, color=cbPalette[1]) +  
  geom_line(linewidth=1, aes(color=Knots, linetype=Knots)) + 
  scale_colour_manual(values=cbPalette[2:11]) + 
  labs(colour = "Knots", x="Easting", y="Yield", title = "Single Swath, mgcv  default smoothing")
@
\caption{Smoothing curves for a single harvest swath using \texttt{mgcv} with the default smoother and \texttt{k = Knots+4}}\label{PredictionsGAM}
\end{figure}

\begin{figure}\centering
<<PredictionsGAMBS, fig=TRUE, echo=FALSE, width=8, height=3>>=
ggplot(PredictionsGAMBS.dat, aes(Easting,Yield)) + 
  #geom_line(aes(Easting,Yield),size = 0.5, data=SinglePass.dat, color=cbPalette[1]) + 
  geom_point(aes(Easting, Yield),size = 1, data=SinglePass.dat, color=cbPalette[1]) + 
  geom_line(linewidth=1, aes(color=Knots, linetype=Knots)) + 
  scale_colour_manual(values=cbPalette[2:11]) + 
  labs(colour = "Knots", x="Easting", y="Yield", title = "Single Swath, mcgv bs Smoothing")
@
\caption{Smoothing curves for a single harvest swath using \texttt{mgcv} with the \texttt{bs='bs'} smoother, \texttt{k = Knots+4}}\label{PredictionsGAMBS}
\end{figure}

\begin{figure}\centering
<<PredictionsGAMPS, fig=TRUE, echo=FALSE, width=8, height=3>>=
ggplot(PredictionsGAMPS.dat, aes(Easting,Yield)) + 
  #geom_line(aes(Easting,Yield),size = 0.5, data=SinglePass.dat, color=cbPalette[1]) + 
  geom_point(aes(Easting, Yield),size = 1, data=SinglePass.dat, color=cbPalette[1]) + 
  geom_line(linewidth=1, aes(color=Knots, linetype=Knots)) + 
  scale_colour_manual(values=cbPalette[2:11]) + 
  labs(colour = "Knots", x="Easting", y="Yield", title = "Single Swath, mcgv ps Smoothing")
@
\caption{Smoothing curves for a single harvest swath using \texttt{mgcv gam} with the \texttt{bs='ps'} smoother, \texttt{k = Knots+4}}\label{PredictionsGAMPS}
\end{figure}

\begin{figure}\centering
<<PredictionsGAMAD, fig=TRUE, echo=FALSE, width=8, height=3>>=
ggplot(PredictionsGAMAD.dat, aes(Easting,Yield)) +
  #geom_line(aes(Easting,Yield),size = 0.5, data=SinglePass.dat, color=cbPalette[1]) + 
  geom_point(aes(Easting, Yield),size = 1, data=SinglePass.dat, color=cbPalette[1]) + 
  geom_line(linewidth=1, aes(color=Knots, linetype=Knots)) + 
  scale_colour_manual(values=cbPalette[2:11]) + 
  labs(colour = "Knots", x="Easting", y="Yield", title = "Single Swath, mgcv ad smoothing")
@
\caption{Smoothing curves for a single harvest swath using \texttt{mgcv gam} with the \texttt{bs='ad'} smoother and \texttt{k = Knots+4}}\label{PredictionsGAMAD}
\end{figure}

\begin{figure}\centering
<<PredictionsBRM, fig=TRUE, echo=FALSE, width=8, height=3>>=
ggplot(PredictionsBRM.dat, aes(Easting,Yield)) + 
  #geom_line(aes(Easting,Yield),size = 0.5, data=SinglePass.dat, color=cbPalette[1]) + 
  geom_point(aes(Easting, Yield),size = 1, data=SinglePass.dat, color=cbPalette[1]) + 
  geom_line(linewidth=1, aes(color=Knots, linetype=Knots)) + 
  scale_colour_manual(values=cbPalette[2:11]) + 
  labs(colour = "Knots", x="Easting", y="Yield", title = "Single swath, brm smoothing")
@
\caption{Smoothing curves for a single harvest swath using \texttt{brm} with the default smoother and \texttt{k = Knots+4}}\label{PredictionsBRM}
\end{figure}

\begin{figure}\centering
<<PredictionsFDA, fig=TRUE, echo=FALSE, width=8, height=3>>=
ggplot(PredictionsFDA.dat, aes(Easting,Yield)) + 
  #geom_line(aes(Easting,Yield),size = 0.5, data=SinglePass.dat, color=cbPalette[1]) + 
  geom_point(aes(Easting, Yield),size = 1, data=SinglePass.dat, color=cbPalette[1]) + 
  geom_line(linewidth=1, aes(color=Knots, linetype=Knots)) + 
  scale_colour_manual(values=cbPalette[2:11]) + 
  labs(colour = "Knots", x="Easting", y="Yield", title = "Single Swath, fda Smoothing")
@
\caption{Smoothing curves for a single harvest pass using \texttt{fda} with \texttt{create.bspline.basis}}
\label{PredictionsFDA}
\end{figure}

In Figure \ref{SplitsCompOCV} we compared the ordinary cross validation scores of different smoothing functions to consider how to choose an optimal number of knots. In the figures that follow we show the results of increasing knots on the wiggliness of the resulting basis functions. In all figures, the original harvest swath data is represented as a gray line in the background.

Figures \ref{PredictionsNS} and \ref{PredictionsBS} show more wiggliness at higher numbers of knots than do the curves fit using \texttt{mgcv}, \texttt{brms} and \texttt{fda}. Using \texttt{lm}, curves are not penalized; the fit criteria simply minimizes sums of squared deviations.

There is little visual difference in the curves fit using \texttt{mgcv} with different spline basis, in Figures \ref{PredictionsGAM}, \ref{PredictionsGAMBS}, \ref{PredictionsGAMPS} and \ref{PredictionsGAMAD}. The similarity of \texttt{mgcv} with \texttt{bs='ad'} is somewhat surprising, in light of the results in Figure \ref{ComparingGAMSplines}, where the curve for the default parameters associated with \texttt{bs='ad'} was noticeably different than the curves for other spline bases.

Figure \ref{PredictionsBRM} shows the multiple curves fit using the \texttt{brms} package. These curves are qualitatively different than the equivalent curves fit using \texttt{mgcv}, even though the \texttt{s} function is built upon the \texttt{s} function from \texttt{mgcv}. \texttt{brms} uses Bayesian methodology built upon the \texttt{stan} C++ library. Thus, to fit a single curve using \texttt{brm} requires additional compile time, along with the time associated with Markov chain Monte Carlo computational engine implemented by \texttt{stan}. For this reason, we did not calculate ordinary cross validation scores for the curves in \ref{PredictionsBRM}. The extra computational overhead makes the \texttt{brms} library less desirable for fitting multiple harvest passes for strip trial analysis.

The wiggliness of the curves in Figure \ref{PredictionsFDA} suggest that more steps are required for penalizing function fitting using this library. The \texttt{fda} package has powerful tools for working with function data, but does not offer the simplicity of packages like \texttt{mgcv} and \texttt{brms}.

\section{Smooth functions summary}

Spline basis in general seem appropriate for representing a single harvest swath as a function form. Of the R packages considered, the \texttt{mgcv} \texttt{gam} function offers considerable flexibility combined with relative speed of computations, which are desirable features when considering finding functional representations of harvest passes to be used for further functional data analysis. There is little difference in the choice of spline bases considered, with the exception of the \texttt{bs='ad'} option. This option, with the default number of knots, finds more features than the default number of knots associated with other \texttt{mgcv} bases considered. 

<<SmoothHarvestStripsExample1, echo=FALSE>>=
Example1.basis <- rep(seq(2,398, by=2))
total.passes <- max(Example1.dat[,'PassNo'])
Example1SmoothHarvestPasses.dat <- data.frame(
  Yield = rep(0, total.passes*length(Example1.basis)),
  Easting = rep(Example1.basis, total.passes),
  PassNo = rep(1:total.passes, each=length(Example1.basis)),
  Pair = rep(0, total.passes*length(Example1.basis)),
  Product = rep(levels(Example1.dat[,'Product']),total.passes*length(Example1.basis)/2),
  Northing = rep(0, total.passes*length(Example1.basis))
)
for(i in 1:total.passes) {
  current.pass <- Example1.dat[Example1.dat[,'PassNo']==i,]
  #current.loess <- loess(Yield ~ Easting, data=current.pass)
  #current.gam <- gam(Yield ~ s(Easting, bs="cr"),data=current.pass)
  #current.gam <- gam(Yield ~ s(Easting, bs='ps',k=30),data=current.pass)
  current.gam <- gam(Yield ~ s(Easting),data=current.pass)
  #current.gam <- gam(Yield ~ s(Easting, bs='ad'),data=current.pass)
  #current.smoothed <- predict(current.loess, data.frame(Easting = Example1.basis))
  current.smoothed <- predict(current.gam, data.frame(Easting = Example1.basis))
  Example1SmoothHarvestPasses.dat[,'Yield'][Example1SmoothHarvestPasses.dat[,'PassNo']==i] <- current.smoothed
  Example1SmoothHarvestPasses.dat[,'Pair'][Example1SmoothHarvestPasses.dat[,'PassNo']==i] <- current.pass[,'Pair'][1]
  Example1SmoothHarvestPasses.dat[,'Product'][Example1SmoothHarvestPasses.dat[,'PassNo']==i] <- as.character(current.pass[,'Product'][1])
  Example1SmoothHarvestPasses.dat[,'Northing'][Example1SmoothHarvestPasses.dat[,'PassNo']==i] <- mean(current.pass[,'Northing'])
}
Example1SmoothHarvestPasses.dat[,'Pass'] <- as.factor(Example1SmoothHarvestPasses.dat[,'PassNo'])
Example1SmoothHarvestPasses.dat[,'Product'] <- as.factor(Example1SmoothHarvestPasses.dat[,'Product'])
#Example1SmoothHarvestPasses.dat[,'Product'] <- levels(Example1.dat[,'Product'])[Example1SmoothHarvestPasses.dat[,'Product']]

Example1SmoothHarvestStrips.dat <- aggregate(Yield ~ Easting + Product + Pair,FUN=mean, data=Example1SmoothHarvestPasses.dat)
#Example1SmoothHarvestStrips.dat[,'Product'] <- levels(Example1.dat[,'Product'])[Example1SmoothHarvestStrips.dat[,'Product']]
@

\begin{figure}\centering
<<SmoothSwaths, fig=TRUE, echo=FALSE, width=12, height=10>>=
ggplot(Example1.dat, aes(Easting,Yield)) + 
 #geom_line(linewidth=1, col=cbPalette[1]) + 
 geom_point(size=1, col=cbPalette[1]) + 
 geom_line(linewidth=1, col=cbPalette[2],data=Example1SmoothHarvestPasses.dat) + 
 labs(x="Easting", y="Yield", title = "Harvest passes from Example 1 with associated functional forms") + facet_wrap(~Pass)
@
\caption{Harvest passes with \texttt{mgcv} smoothing. This duplicates the data from Figure \ref{RawSwaths} with the additional of functional forms of the data. The functional forms are found using \texttt{mgcv} with default arguments to the \texttt{s} function.}\label{SmoothSwaths}
\end{figure}

To summarize the magnitude of the problem we are faced with as we desire to undertake a functional data analysis of strip trials, consider Figure \ref{SmoothSwaths}. This duplicates the harvest passes as shown at the start of this chapter (\ref{RawSwaths}), with functional forms found using the default smoothing parameters for the \texttt{mgcv} library. While for some harvest passes the functional forms are over-smoothed (by visual inspection), as a full data set the defaults for \texttt{mgcv} are sufficient for functional data analysis as described in following chapters. More precisely, unless otherwise noted, we will be using \texttt{mgcv} to find functional forms for harvest pass data.

\chapter{Functional data analysis of strip trials}\label{FunctionalDataAnalysisChapter}

\section{Overview}

In this chapter, we first consider a univariate analysis of strip means, using a simple paired sampled $t$-test. We then proceed to show a pointwise $t$-test, where the experimental units are derived from harvest passes using smoothing methods considered in Chapter \ref{SmoothingFunctionsChapter}. We then show how the data can be presented in such a way as to allow a more detailed interpretation of the results of a strip trial than available from simple univariate analysis.

\section{Univariate analysis, Example 1}

<<AggregateHarvestPassesExample1, echo=FALSE, include=FALSE>>=
#aggregate by harvest pass number. Pass is a factor, PassNo is numeric.
Example1HarvestPasses.dat <- aggregate(Yield ~ PassNo, Example1.dat, mean, na.rm=TRUE)
#Convert Product to numeric and aggregate, then use aggregated value as index into factor levels
Example1HarvestPasses.dat[,'Product'] <- aggregate(as.numeric(Product) ~ PassNo, Example1.dat, mean, na.rm=TRUE)[,2]
Example1HarvestPasses.dat[,'Product'] <- levels(Example1.dat[,'Product'])[Example1HarvestPasses.dat[,'Product']]
Example1HarvestPasses.dat[,'Product'] <- as.factor(Example1HarvestPasses.dat[,'Product'])
#summarize the number of yield observations per pass
Example1HarvestPasses.dat[,'Points'] <- aggregate(Yield ~ PassNo,Example1.dat[!is.na(Example1.dat[,'Yield']),],length)[,2]
#summarize the position in the field. We can use this later for regression analysis.
Example1HarvestPasses.dat[,'Northing'] <- aggregate(Northing ~ PassNo, Example1.dat, mean, na.rm=TRUE)[,2]
#compute min and max northing values so we can estimate deviations from a straight line
Example1HarvestPasses.dat[,'DeviationNorthing'] = aggregate(Northing ~ PassNo, Example1.dat, max, na.rm=TRUE)[,2] - aggregate(Northing ~ Pass,Example1.dat, min, na.rm=TRUE)[,2]
#compute min and max easting to determine strip length.
Example1HarvestPasses.dat[,'Length'] = aggregate(Easting ~ PassNo, Example1.dat, max, na.rm=TRUE)[,2]-aggregate(Easting ~ Pass,Example1.dat, min, na.rm=TRUE)[,2]
@

\begin{table}[h!]
\centering
\begin{tabular}{r r r r r r r} 
 \hline
 Pass Number & Yield & Variety & Points & Northing & Deviation & Length\\ 
 \hline
 1 & 165.32 & E & 262 &   0.78 & 1.63 & 397.35 \\
 2 & 160.28 & E & 256 &   6.86 & 1.60 & 397.75 \\
 3 & 171.33 & B & 260 &  12.99 & 1.54 & 397.79 \\
 4 & 179.71 & B & 267 &  19.10 & 1.58 & 397.41 \\
 5 & 165.30 & E & 257 &  25.18 & 1.62 & 398.08 \\
 6 & 162.45 & E & 262 &  31.31 & 1.51 & 398.84 \\
 7 & 180.67 & B & 266 &  37.32 & 1.55 & 397.98 \\
 8 & 196.67 & B & 260 &  43.46 & 1.50 & 398.35 \\
 9 & 175.93 & E & 265 &  49.50 & 1.51 & 397.33 \\
10 & 174.60 & E & 265 &  55.64 & 1.46 & 397.85 \\
11 & 193.54 & B & 269 &  61.64 & 1.43 & 398.61 \\
12 & 191.35 & B & 259 &  67.74 & 1.47 & 397.83 \\
13 & 187.40 & E & 268 &  73.76 & 1.48 & 398.42 \\
14 & 183.05 & E & 267 &  79.91 & 1.40 & 397.82 \\
15 & 200.40 & B & 264 &  86.01 & 1.46 & 398.83 \\
16 & 194.22 & B & 266 &  92.09 & 1.56 & 398.65 \\
17 & 184.24 & E & 270 &  98.16 & 1.58 & 398.61 \\
18 & 177.72 & E & 259 & 104.31 & 1.66 & 398.31 \\
19 & 189.07 & B & 271 & 110.34 & 1.73 & 398.81 \\
20 & 194.28 & B & 271 & 116.43 & 1.60 & 398.91 \\
21 & 185.21 & E & 273 & 122.45 & 1.63 & 399.36 \\
22 & 200.70 & E & 278 & 128.54 & 1.63 & 398.20 \\
23 & 203.62 & B & 258 & 134.53 & 1.63 & 397.80 \\
24 & 213.13 & B & 276 & 140.68 & 1.63 & 397.99 \\
\end{tabular}
\caption{Summary of harvest passes for Example 1. Pass number is the order of the harvest pass from the bottom (southern-most) strip in the harvest map in Figure \ref{Example1Maps}. Yield is the arithmetic average of the harvest data points recorded along each strip. Variety is the variety planted in each pass, according to the seeding map in Figure \ref{Example1Maps}. Points are the number of yield points recorded for each pass. Northing is the mean over all \texttt{Northing} coordinates associated with each pass, while Deviation is the difference between the minimum and maximum \texttt{Northing} values recorded for each pass. Length is the difference between the minimum and maximum \texttt{Easting} values recorded for each pass.}
\label{SummaryExample1Passes}
\end{table}

A simple analysis, which we will refer to as univariate analysis, may proceed as follows. We first compute a yield for each harvest pass by averaging over the yield monitor data points as shown in Figure \ref{Example1Maps}. The means and other summary statistics are shown in Table \ref{SummaryExample1Passes}. The number of yield observations per pass is relatively large and varies relatively little (256-278), so we will take the simple arithmetic average of yield data points as a measure of the yield for each pass and won't consider weighted means. Length of the harvest passes varies from 397.33m to 399.36m, so we can consider the harvest passes as subplots of the same length. The harvest passes deviate from a straight east-west line by less than 2 meters (the maximum \texttt{Northing} value minus the minimum \texttt{Northing} value for the observations in each pass), so for functional analysis we can simplify and use the variable \texttt{Easting} to represent distance or $x$. Similarly, we can take the average value of \texttt{Northing} to represent position of the harvest pass in the field.



<<AggregateStripsExample1, echo=FALSE, include=FALSE>>=
#create a strip number
Example1HarvestPasses.dat[,'Strip'] <- ceiling(Example1HarvestPasses.dat[,'PassNo']/2)
#aggregate by strips
Example1Strips.dat <- aggregate(Yield ~ Strip, Example1HarvestPasses.dat, mean, na.rm=TRUE)
#Convert Product to numeric and aggregate, then use aggregated value as index into factor levels as before
Example1Strips.dat[,'Product'] <- aggregate(as.numeric(Product) ~ Strip, Example1HarvestPasses.dat, mean, na.rm=TRUE)[,2]
Example1Strips.dat[,'Product'] <- levels(Example1HarvestPasses.dat[,'Product'])[Example1Strips.dat[,'Product']]
Example1Strips.dat[,'Product'] <- as.factor(Example1Strips.dat[,'Product'])
#keep Northing for plots and later regression analysis.
Example1Strips.dat[,'Northing'] <- aggregate(Northing ~ Strip, Example1HarvestPasses.dat, mean, na.rm=TRUE)[,2]
@

\begin{table}[h!]
\centering
\begin{tabular}{r r r r} 
 \hline
 Strip & Yield & Variety & Pair \\ 
 \hline
 1 & 162.80 & E & 1 \\
 2 & 175.52 & B & 1 \\
 3 & 163.87 & E & 2 \\
 4 & 188.67 & B & 2 \\
 5 & 175.27 & E & 3 \\
 6 & 192.44 & B & 3 \\
 7 & 185.22 & E & 4 \\
 8 & 197.31 & B & 4 \\
 9 & 180.98 & E & 5 \\
10 & 191.67 & B & 5 \\
11 & 192.95 & E & 6 \\
12 & 208.38 & B & 6 \\
\end{tabular}
\caption{Strip data for Example 1. Yield is the mean of the harvest pass yields in Table \ref{SummaryExample1Passes}. Pair is the designation used for a paired $t$-test described later in this section.}
\label{SummaryExample1Strips}
\end{table}

We now consider strips as a plot composed of two harvest passes, each planted with the same variety. We can then compute strip yield by taking the arithmetic average of the two harvest passes per strip. These results are show in Table \ref{SummaryExample1Strips}

\begin{figure}\centering
<<Example1StripYieldsPlot, fig=TRUE,echo=FALSE, width=8, height=3>>=
ggplot(Example1Strips.dat, aes(x=Northing, y=Yield, color=Product, fill=Product)) +
   geom_bar(stat="identity") + scale_colour_manual(values=cbPalette[pair.colors], name="Variety") +     
   scale_fill_manual(values=cbPalette[pair.colors], name="Variety")
@
\caption{Average strip yields from Table \ref{SummaryExample1Strips} plotted by strip position in meters \texttt{Northing}. }\label{Example1StripYieldsPlot}
\end{figure}

\begin{table}[h!]
\centering
\begin{tabular}{r r} 
 \hline
 Pair & Yield Difference \\ 
 \hline
 1 & 12.72 \\
 2 & 24.80 \\
 3 & 17.17 \\
 4 & 12.08 \\
 5 & 10.69 \\
 6 & 15.42 \\
\end{tabular}
\caption{Differences used for paired $t$-test. Yield difference is calculated for each pair by computing the observe yield for variety B minus the observed yield for variety E, where observed yields are taken from Table \ref{SummaryExample1Strips}}
\label{DifferencesExample1}
\end{table}

<<PairedTTestExample1, echo=FALSE, include=FALSE>>=
MeansB <- Example1Strips.dat[Example1Strips.dat[,'Product']=="B",]
MeansE <- Example1Strips.dat[Example1Strips.dat[,'Product']=="E",]
Differences <- MeansB[,'Yield'] - MeansE[,'Yield']
d <- mean(Differences)
S <- sd(Differences)
n <- length(Differences)
t0 <- d/(S *(sqrt(1/n)))
prt <- 2*(1-pt(t0, n-1))
CI.bound <- pt(0.025,n-1)*S*sqrt(n)
Differnce.CI <- c(d-CI.bound, d+CI.bound)
@

Side-by-side strips of different varieties for pairs in \ref{SummaryExample1Strips} are next used to compute a paired $t$-test. The differences between yield for variety B minus the yield for variety E are show in Table \ref{DifferencesExample1}. Mean yield difference is $15.48$, with a standard deviation of $5.13$ and $n=6$ This yields a $t$ statistic of $7.39$ with a $p$-value of $0.0007$, and a 95\% confidence interval of the difference between means of $(9.07 21.89)$. Statistics were computed described in Section \ref{UnivariateDataAnalysis}.

<<SegmentYieldsExample1, echo=FALSE, include=FALSE>>=
Example1.dat[,"Segment"] <- ceiling(Example1.dat[,"Easting"]/20)
Example1Segments.dat <- aggregate(Yield ~ Segment, data=Example1.dat,FUN=mean, na.rm=TRUE)
#the first data point is segment 0, we'll discard it
Example1Segments.dat <- Example1Segments.dat[Example1Segments.dat[,"Segment"]>0,]
Example1Segments.dat[,"Easting"] <- Example1Segments.dat[,"Segment"]*20
Example1SegmentCount.dat <- aggregate(Yield ~ Segment, data=Example1.dat,FUN=length)
@

\begin{figure}\centering
<<Example1SegmentYieldsPlot, fig=TRUE,echo=FALSE, width=8, height=3>>=
ggplot(Example1Segments.dat, aes(x=Easting, y=Yield)) +
  geom_bar(stat="identity") + scale_colour_manual(values=cbPalette[pair.colors]) + scale_fill_manual(values=cbPalette[pair.colors])
@
\caption{Yield means for 20m segments from Example 1. While the westernmost 20m segment was over 200 bu/acre, in general the western two-thirds of the field is lower yielding than the eastern-most third. As seen in Figure \ref{Example1Maps}, this is related to relatively low two yielding zones in the southern portion of the field, centered at 100m and 200m easting.}
\label{Example1SegmentYieldsPlot}
\end{figure}

Before we continue to pointwise functional data analysis, we compute the averages of yield monitor observations over discrete segments at 20 meter intervals. We choose 40 because the length of the strips are 400m, this provides us yield means that are all of equal size. The number of yield observations per segment ranged from 315 to 322 yield points. The resulting averages are plotted in Figure \ref{Example1SegmentYieldsPlot}. Yield tended to be higher in the most eastern quarter of the field. As we see in Figure \ref{Example1Maps} there are two low-yielding regions in the southern portion of the field, one centered at about 100 meters \texttt{Easting} and the other at roughly 250 meters \texttt{Easting}. This is reflected in the mean yield by segment. As we will see in following sections (e.g. Figure \ref{Example1Differences}), estimates of yield differences tend to be greater in the eastern portion of the field and are generally more statistically significant in this region, largely because yield is also less variable.

\section{Pointwise functional data analysis , Example 1}

We now proceed to a functional data analysis of Example 1. This analysis is analogous to the paired $t$-test in the preceding section, but instead of computing the arithmetic average of yield points per harvest pass, we find a functional representation for each harvest pass.

\begin{figure}\centering
<<Example1RawHarvestPasses, fig=TRUE, echo=FALSE, width=8, height=3>>=
ggplot(Example1.dat, aes(Easting,Yield)) + 
geom_point(aes(colour = Product),size=.2) + 
#geom_line(aes(colour = Product),linewidth=.5) + 
scale_colour_manual(values=cbPalette[pair.colors]) + 
labs(colour = "Variety", x="Easting", y="Yield", title = "Example 1 Harvest Passes") +
  ylim(80, 240)
@
\caption{Instantaneous yield monitor estimates for the harvest strips in Example 1. Data are shown as discrete points to emphasize the difference between the original data and later functional forms fit to data.}
\label{Example1RawHarvestPasses}
\end{figure}

\begin{figure}\centering
<<Example1SmoothedHarvestPasses, fig=TRUE,echo=FALSE, width=8, height=3>>=
ggplot(Example1SmoothHarvestPasses.dat, aes(Easting,Yield)) + 
#geom_point(aes(colour = Product),size=.2) + 
geom_line(aes(colour = Product, group=Pass),linewidth=.5) +
scale_colour_manual(values=cbPalette[pair.colors]) + 
labs(colour = "Variety", x="Easting", y="Yield", title = "Example 1 Smoothed Harvest Passes") +
  ylim(80, 240)
@
\caption{\texttt{mgcv} smoothed harvest passes from Figure \ref{Example1RawHarvestPasses}. The R library \text{mgcv} as used to find functional forms for the original data in Figure \ref{Example1RawHarvestPasses}. Default bases functions and degrees of freedom where used for these data. This figure is analogous to Table \ref{DifferencesExample1}.}
\label{Example1SmoothedHarvestPasses}
\end{figure}

\begin{figure}\centering
<<Example1AnalyzedStrips, fig=TRUE,echo=FALSE, width=8, height=3>>=
ggplot(Example1SmoothHarvestStrips.dat, aes(Easting,Yield)) + 
#geom_point(aes(colour = Product),size=.2) + 
geom_line(aes(colour = Product, group=Pair),linewidth=.5) +
scale_colour_manual(values=cbPalette[pair.colors]) + 
labs(colour = "Variety", x="Easting", y="Yield", title = "Example 1 Strips") +
  ylim(80, 240)
@
\caption{Smoothed harvest passes pooled into experimental strips for Example 1. Functional forms in Figure \ref{Example1SmoothedHarvestPasses} were pooled pointwise by pairs for form functional forms of the experimental strips. This figure is analogous to Table \ref{DifferencesExample1}.}
\label{Example1AnalyzedStrips}
\end{figure}

%\begin{figure}\centering
%<<Example2Strips,echo=FALSE, width=8, height=6>>=
%ggplot(Example2.dat, aes(Longitude,Latitude)) + 
%geom_point(aes(colour = Sample),size=1) + 
%scale_colour_gradient2(low=vermillion, mid=yellow, high=blue, midpoint = 0.5) +
%labs(colour = "Relative Rank", x="Easting", y="Northing", title = "Strip Trial")
%@
%\caption{}\label{blocks_sample1}
%\end{figure}

<<TTestsExample1, echo=FALSE>>=
Example1MeanDifferences <- data.frame(
  Easting = rep(Example1.basis, each=6),
  Pair = factor(rep(1:6, length(Example1.basis)))
)
TTestExample1 <- data.frame(
  Easting = Example1.basis
)
n <- 6
for(i in 1:length(Example1.basis)) {
  l <- Example1.basis[i]
  obs <- Example1SmoothHarvestStrips.dat[Example1SmoothHarvestStrips.dat[,'Easting']==l,]
  MeansB <- obs[obs[,'Product']=="B",]
  MeansE <- obs[obs[,'Product']=="E",]
  Differences <- MeansB[,'Yield'] - MeansE[,'Yield']
  Example1MeanDifferences[Example1MeanDifferences[,'Easting']==l,'Difference'] <- Differences
  TTestExample1[i,'Delta'] <- mean(Differences)
  TTestExample1[i,'SD'] <- sd(Differences)
}
TTestExample1[,'t'] <- TTestExample1[,'Delta']/(TTestExample1[,'SD']*sqrt(1/n))
TTestExample1[,'P'] <- 2*(1-pt(TTestExample1[,'t'],n-1))
TTestExample1[,'Significant'] = TTestExample1[,'P']<0.05
@

\begin{figure}\centering
<<Example1PairwiseDifferences, fig=TRUE,echo=FALSE, width=8, height=3>>=
ggplot(Example1MeanDifferences, aes(Easting, Difference)) + 
#geom_point(aes(colour = Pair),size=.2) + 
geom_line(aes(colour = Pair),linewidth=.5) +
scale_colour_manual(values=cbPalette) + 
labs(colour = "Pair", x="Easting", y="Yield Difference", title = "Example 1 Pairwise Differences")
@
\caption{Yield difference for each pair of smoothed strip data from \ref{Example1AnalyzedStrips}. These are the functional forms of pairwise differences, and are analogous to Table \ref{DifferencesExample1}.}
\label{Example1PairwiseDifferences}
\end{figure}

Each harvest pass was smoothed in R using the \texttt{gam} function from the \texttt{mgcv} library by first fitting yield values to distance from the west edge of the field, using code of the form \texttt{gam(Yield \textasciitilde s(Easting),...)}. The original data are shown in Figure \ref{Example1RawHarvestPasses}. The fitted model was used to predict yield on a common basis of $[2,4,\dots, 398]$ measured in meters from the western edge of the field. (Figure \ref{Example1SmoothedHarvestPasses}. Since each harvest pass represents half of a planter pass, two harvest passes were merged by taking the average of the two passes at each distance point to form a single strip (Figure \ref{Example1AnalyzedStrips}. This provided 12 experimental units with two treatments and six replicates for functional data analysis. From this, paired comparison $t$-statistics and $p$-values were computed and plotted. Figure \ref{Example1PairwiseDifferences} shows the pointwise difference from pairs of strips - strips 1 and 2 from \ref{Example1AnalyzedStrips} form pair 1 in figure \ref{Example1PairwiseDifferences}, strips 3 and 4 form pair 2, etc. We consider Figure \ref{Example1AnalyzedStrips} to be the functional data analog of Table \ref{DifferencesExample1}. From these strips we compute a pairwise $t$ test for each \texttt{Easting} point, thus we are performing (in rather tortured language) a pointwise pairwise $t$-test functional data analysis.

%\begin{figure}\centering
%<<ResultsExample1,fig=TRUE,echo=FALSE, width=8, height=6>>=
%CombinedT <- data.frame(
%  Easting=rep(TTestExample1[,'Easting'],4),
%  Measure = c(rep('Treatment Differences',length(TTestExample1[,'Delta'])),
%              rep('Standard Deviation',length(TTestExample1[,'SD'])),
%              rep('t Ratio',length(TTestExample1$t)),
%              rep('Probability>t',length(TTestExample1$P))),
%  Value = c(TTestExample1[,'Delta'],TTestExample1[,'SD'],TTestExample1$t,TTestExample1$P),
%  Significant = rep(TTestExample1$P<0.05,4)
%)
%CombinedT$Measure <- factor(CombinedT$Measure,levels=c('Treatment Differences','Standard Deviation','t Ratio','Probability>t'))
%ggplot(CombinedT, aes(Easting,Value)) + 
%geom_point(aes(color=Significant)) + #cbPalette[1]) + 
%scale_colour_manual(values=cbPalette) +
%labs(x="Easting", title = "Functional Statistics") + facet_wrap(~ Measure,nrow = 4,scales="free_y")
%@
%\caption{Pointwise paired t-tests of variety strips from Example 1}\label{ResultsExample1}
%\end{figure}

\begin{figure}\centering
<<Example1Differences, fig=TRUE,echo=FALSE, width=8, height=3>>=
ggplot(TTestExample1, aes(Easting, Delta)) + 
geom_point(aes(colour = Significant),size=1) + 
scale_colour_manual(values=cbPalette[pair.colors]) + 
labs(colour = "Significant", x="Easting", y="Yield (bu/acre)", title = "Example 1 Yield Difference")
@
\caption{Pointwise yield differences $\bar{d})(x)$ for Example 1. This is the functional form of Equation \ref{FunctionMeanDifference} based on the data in Figure \ref{Example1PairwiseDifferences}}
\label{Example1Differences}
\end{figure}

\begin{figure}\centering
<<Example1SD, fig=TRUE,echo=FALSE, width=8, height=3>>=
ggplot(TTestExample1, aes(Easting, SD)) + 
geom_point(aes(colour = Significant),size=1) + 
scale_colour_manual(values=cbPalette[pair.colors]) + 
labs(colour = "Significant", x="Easting", y="Standard Deviation", title = "Example 1 Standard Deviation")
@
\caption{Pointwise standard deviations $S_d(x)$ for Example 1. This is the functional form of Equation \ref{SDPairedDifferences}, based on the data in Figure \ref{Example1PairwiseDifferences}}
\label{Example1SD}
\end{figure}

\begin{figure}\centering
<<Example1T, fig=TRUE,echo=FALSE, width=8, height=3>>=
ggplot(TTestExample1, aes(Easting, t)) + 
geom_point(aes(colour = Significant),size=1) + 
scale_colour_manual(values=cbPalette[pair.colors]) + 
labs(colour = "Significant", x="Easting", y="t ratio", title = "Example 1 t-test")
@
\caption{Pointwise $t(x)$ ratio values for Example 1. This is the functional form of Equation \ref{FunctionTStatistic} based on the data in Figure \ref{Example1PairwiseDifferences}} 
\label{Example1T}
\end{figure}

\begin{figure}\centering
<<Example1P, fig=TRUE,echo=FALSE, width=8, height=3>>=
ggplot(TTestExample1, aes(Easting, P)) + 
geom_point(aes(colour = Significant),size=1) + 
scale_colour_manual(values=cbPalette[pair.colors]) + 
labs(colour = "Significant", x="Easting", y="P(>t)", title = "Example 1 p-values")
@
\caption{Pointwise $P(>t)(x)$ values for Example 1. These are the pointwise $p$-values associated with the $t$ statistics in Figure \ref{Example1T} based on the data in Figure \ref{Example1PairwiseDifferences}}
\label{Example1P}
\end{figure}

Pointwise mean of differences between variety yields from Figure \ref{Example1PairwiseDifferences} is presented as a continuous function of distance $x$ from the western edge of the field over the interval [2,398] meters (Figure \ref{Example1Differences}). In this example the difference is greatest in the eastern half of the field, but is consistently greater than 12 bu/acre. There are four regions where the $t$-test of mean differences is statistically significant, these correspond to regions where the standard deviation of variety differences is lower; that is, where yield is more consistent in strips from south to north. Figure \ref{Example1SD} shows the pointwise standard deviations calculated from the strips in Figure \ref{Example1AnalyzedStrips}, while Figure \ref{Example1T}, shows the results of pointwise paired $t$-test based, $t(x) = S_d(x)/\sqrt(n(x))$, where in this example since the strips are of equal length, $n(x)=6$ for $x \in [2,398]$. Finally, Figure \ref{Example1P} shows the pointwise $p$-values associated each $t(x)$. The values in Figure \ref{Example1P} were used to determine the color of points in Figures \ref{Example1Differences},\ref{Example1SD}, \ref{Example1T} and \ref{Example1P}.

\begin{figure}\centering
<<CIPlotExample1, fig=TRUE,echo=FALSE, width=8, height=3>>=
CIExample1 <- data.frame(
  Easting = Example1.basis,
  Difference = TTestExample1[,'Delta'],
  CI.Upper = TTestExample1[,'Delta']+qt(0.025, n-1, lower.tail=FALSE)*TTestExample1[,'SD']/sqrt(n),
  CI.Lower = TTestExample1[,'Delta']-qt(0.025, n-1, lower.tail=FALSE)*TTestExample1[,'SD']/sqrt(n)
)
CIExample1[,'Significant'] <- sign(CIExample1[,'CI.Upper']) == sign(CIExample1[,'CI.Lower'])
ggplot(CIExample1, aes(Easting, Difference)) + 
geom_point(aes(col=Significant),size=2) +
ylim(c(min(CIExample1[,'CI.Lower']), max(CIExample1[,'CI.Upper'])))+
geom_line(aes(Easting, CI.Upper),col=cbPalette[3],data=CIExample1) +
geom_line(aes(Easting, CI.Lower),col=cbPalette[3],data=CIExample1) +
scale_colour_manual(values=cbPalette) +
labs(x="Easting",y = "Difference (bu.acre)", title = "Variety Yield Differences")
@
\caption{Pointwise mean difference and confidence intervals among variety strips from Example 1. This is the functional form of Equation \ref{FunctionCI} based on the data in Figure \ref{Example1PairwiseDifferences} }
\label{CIPlotExample1}
\end{figure}

Figure \ref{CIPlotExample1} shows the same information as Figure \ref{Example1Differences}, but included pointwise confidence intervals. The four Figures \ref{Example1Differences},\ref{Example1SD}, \ref{Example1T} and \ref{Example1P} are intended to be presented as a group and show the elements of a pairwise $t$-test in graphical form. This is similar to the output of standard statistical software such as the R $t.test$ function. Figure \ref{CIPlotExample1} is intended to be a stand-alone graph that provides a simpler, smaller presentation of results.

%Standard deviation also varies with position, reflecting the lowering yield patches at approximately 100m and 200m in the lower (southern) portion of the field. Since $t$ is a function of both difference and standard deviation (in this example, replicates are constant across the field) the test of significance also varies with position and the p-value associated with this test is more often significant in the western part of the field.-->

%We should note that the largest region of significant differences between varieties occurs in a higher yield part of the field; the differences in low fertility zones are smaller and not statistically significant. This provides information about the physical cause of varietal differences. We might infer from this that variety # is a superior performer in high-yield zones, but still performs well in low-yield zones.-->

\section{Pointwise functional data analysis, Example 2}


<<CountStrips, echo=FALSE>>=
strips <- unique(Example2.dat[,'Strip'])
@

%\begin{figure}\centering
%<<Example2SprayYieldMaps,fig=TRUE,echo=FALSE, width=10, height=6>>=
%Example2.dat$SprayedRank <- rank(Example2.dat$Sprayed)
%Example2.dat$SprayedRank <- Example2.dat$SprayedRank/max(Example2.dat$SprayedRank)

%Example2.dat$YieldRank <- rank(Example2.dat[,'Yield'])
%Example2.dat$YieldRank <- Example2.dat$YieldRank/max(Example2.dat$YieldRank)

%MapsSprayed <- data.frame(Longitude=c(Example2.dat[,'Longitude'],
%                                Example2.dat[,'Longitude']),
%                   Latitude=c(Example2.dat[,'Latitude'],
%                              Example2.dat[,'Latitude']),
%                   Value=c(Example2.dat$SprayedRank,
%                           Example2.dat$YieldRank),
%                   Map=c(rep('Spray',length(Example2.dat$SprayedRank)),
%                         rep('Yield',length(Example2.dat$YieldRank))))
%ggplot(MapsSprayed, aes(Longitude,Latitude)) + 
%geom_point(aes(colour = Value),size=1) + 
%scale_colour_gradient2(low=vermillion, mid=yellow, high=blue, midpoint = 0.5) +
%labs(colour = "Relative Rank", x="Easting", y="Northing", title = "Strip Trial") +% facet_wrap(~ Map)
%@
%\caption{Spray and harvest maps for Example 2}\label{Example2SprayYieldMaps}
%\end{figure}

\begin{figure}\centering
<<Example2ActualYield, fig=TRUE,echo=FALSE, width=8, height=3>>=
ggplot(Example2.dat, aes(Northing, Yield)) + 
geom_point(aes(colour = Sprayed),size=.5) + 
#geom_line(aes(colour = Sprayed),linewidth=.2) + 
scale_colour_manual(values=cbPalette[pair.colors]) + 
labs(colour = "Sprayed", x="Northing", y="Yield", title = "Fungicide Trial, as Harvested") #+ ylim(0,60)
@
\caption{Instantaneous yield monitor harvest data estimates from Example 2. These data are much noisier than the data from Example 1 (see Figure \ref{Example1RawHarvestPasses}) and not all harvest passes are the same length.}
\label{Example2ActualYield}
\end{figure}

<<Example2SmoothHarvestStrips, echo=FALSE>>=
Yield2 = NULL
Easting2 = NULL
Strip2 = NULL
#Block2 = NULL
Treated2 = NULL
Northing2 = NULL
Pass2 <- NULL
Example2Passes <- unique(Example2.dat[,'Pass'])
for(pass in Example2Passes) {
  current.pass <- Example2.dat[Example2.dat[,'Pass']==pass,]
  MinN <- ceiling(min(current.pass[,'Northing']))
  MaxN <- floor(max(current.pass[,'Northing']))
  #may not be an even integer
  MinN <- 2*(ceiling(MinN/2))
  MaxN <- 2*(ceiling(MaxN/2))
  smooth2.basis <- seq(MinN, MaxN, by=2)
  current.gam <- gam(Yield ~ s(Northing),data=current.pass)
  #current.gam <- gam(Yield ~ s(Northing, bs='ad'),data=current.pass)
  current.smoothed <- predict(current.gam, data.frame(Northing = smooth2.basis))
  L <- length(current.smoothed)
  Yield2 <- c(Yield2, current.smoothed)
  Easting2 <- c(Easting2, rep(mean(current.pass[,'Easting']),L))
  Strip2 <- c(Strip2, rep(current.pass[1,'Strip'],L))
  #Block2 <- c(Block2, rep(current.pass[1,'Block'],L))
  Pass2 <- c(Pass2, rep(current.pass[1,'Pass'],L))
  Treated2 <- c(Treated2, rep(current.pass[1,'Sprayed'],L))
  Northing2 <- c(Northing2, smooth2.basis)
}
Example2SmoothHarvestPasses.dat <- data.frame(
  Yield = Yield2,
  Easting = Easting2,
  Strip = Strip2,
  #Block = Block2,
  Pass = Pass2,
  Treated = Treated2,
  Northing = Northing2
)
@

\begin{figure}\centering
<<Example2HarvestPasses, fig=TRUE,echo=FALSE, width=8, height=3>>=
ggplot(Example2SmoothHarvestPasses.dat, aes(Northing, Yield)) + 
geom_line(aes(colour = Treated, group=Pass),linewidth=.5) +
scale_colour_manual(values=cbPalette[pair.colors]) + 
labs(colour = "Sprayed", x="Northing", y="Yield", title = "Fungicide Trial, Smoothed Harvest Passes")
@
\caption{Smoothed harvest strips for Example 2. Note that these lines are more highly variable than the prior example, and that some line do not extend over the entire plotting range (0-700m \texttt{Northing}). There are more lines in this graph than for the previous data example because the treated strips covered four harvest passes instead of two.}
\label{Example2HarvestPasses}
\end{figure}

Figure \ref{Example2ActualYield} shows the yield monitor harvest estimates for Example 2. Note that there are many low or zero values; this emphasizes the harvest gaps found in these data. \texttt{gam} smoothing models were fit to yield values for each harvest pass (Figure \ref{Example2HarvestPasses}). The interpolated values for each harvest pass were averaged to provide an estimate of yield for each treated strip, with four harvest passes combined for each sprayed strip. (Figure \ref{Example2Strips}. The field was not a uniform rectangle, so some treated strips are longer than others. 

Since the strips are of unequal lengths, we do not provide strip level summary statistics as in Example 1. Functional forms of treatment differences, standard deviations, $t$-statistics and $p$-values were calculated as described in the previous section, with the exception that the number of strips was also computed at each distance. The distance basis from $42,44,\cdots, 744$, was measured from the south end of the field. The southern most end-rows were not included in the analysis.


<<Example2SmoothHarvestStrips, echo=FALSE>>=
Example2SmoothHarvestStrips.dat <- aggregate(Yield ~ Northing + Strip, FUN=mean, data=Example2SmoothHarvestPasses.dat)
Example2SmoothHarvestStrips.dat[,'Treated']<- Example2SmoothHarvestStrips.dat[,'Strip'] %in% c(2,4,6,8)
@

\begin{figure}\centering
<<Example2Strips, fig=TRUE,echo=FALSE, width=8, height=3>>=
ggplot(Example2SmoothHarvestStrips.dat, aes(Northing, Yield)) + 
#geom_point(aes(colour = Treated),size=.2) + 
geom_line(aes(colour = Treated, group=Strip),linewidth=.5) +
scale_colour_manual(values=cbPalette[pair.colors]) + 
labs(colour = "Sprayed", x="Northing", y="Yield", title = "Fungicide, Smoothed and Averaged Strips")
@
\caption{Smoothed harvest strips pooled by by strip for Example 2. It is much more clear from this plot, compared to Figure \ref{Example2ActualYield} that there were fewer experimental strips than in the previous example, and that there are portions of the field were the strips are unreplicated.}
\label{Example2Strips}
\end{figure}

<<TTestExample2, echo=FALSE>>=
MinN <- min(Example2SmoothHarvestStrips.dat[,'Northing'])
MaxN <- max(Example2SmoothHarvestStrips.dat[,'Northing'])
Example2.basis <- seq(MinN, MaxN, by=2)
TTestExample2 <- data.frame(
  Northing = Example2.basis
)
TTestExample2[,'SD'] <- NA
TTestExample2[,'Delta'] <- NA
TTestExample2[,'t'] <- NA
TTestExample2[,'P'] <- NA
TTestExample2[,'N'] <- NA
for(i in 1:length(Example2.basis)) {
  l <- Example2.basis[i]
  obs <- Example2SmoothHarvestStrips.dat[Example2SmoothHarvestStrips.dat[,'Northing']==l,]
  MeansSprayed <- obs[obs[,'Treated'],]
  MeansUnsprayed <- obs[!obs[,'Treated'],]
  if(length(MeansSprayed[,'Yield']) == length(MeansUnsprayed[,'Yield'])) {
    
    n <- length(MeansSprayed[,'Yield'])
    TTestExample2[i,'N']<-n
    if(n>1) {
      Differences <- MeansSprayed[,'Yield'] - MeansUnsprayed[,'Yield']
      TTestExample2[i,'Delta'] <- mean(Differences)
      TTestExample2[i,'SD'] <- sd(Differences)
      TTestExample2[i,'t'] <- TTestExample2[i,'Delta']/(TTestExample2[i,'SD']*sqrt(1/n))
      TTestExample2[i,'P'] <- 2*(1-pt(abs(TTestExample2[i,'t']),n-1))
    } else {
      TTestExample2[,'Delta'][i] <- NA
    } 
  } else {
    TTestExample2[,'Delta'][i] <- NA
  }
}
TTestExample2[,'Significant'] <- TTestExample2[,'P']<0.05
TTestExample2[is.na(TTestExample2[,'Significant']),'Significant'] <- FALSE
@

%\begin{figure}\centering
%<<ResultsExample2,fig=TRUE,echo=FALSE, width=8, height=6>>=
%CombinedT2 <- data.frame(
%  Northing=rep(TTestExample2[,'Northing'],4),
%  Measure = c(rep('Difference (Sprayed-Unsprayed)',length(TTestExample2[,'Delta'])),
%              rep('Standard Deviation',length(TTestExample2[,'SD'])),
%              rep('t Ratio',length(TTestExample2$t)),
%              rep('Probability>t',length(TTestExample2$P))),
%  Value = c(TTestExample2[,'Delta'],TTestExample2[,'SD'],TTestExample2$t,TTestExample2$P),
%  Significant = rep(TTestExample2$P<0.05,4)
%)

%CombinedT2 <- CombinedT2[!is.na(CombinedT2$Significant),]

%CombinedT2$Measure <- factor(CombinedT2$Measure,levels=c('Difference (Sprayed-Unsprayed)','Standard Deviation','t Ratio','Probability>t'))
%ggplot(CombinedT2, aes(Northing,Value)) + 
%geom_point(aes(color=Significant)) + #cbPalette[1]) + 
%scale_colour_manual(values=cbPalette) +
%labs(x="Northing", title = "Functional Statistics") + facet_wrap(~ Measure,nrow = 4,scales="free_y") #+ xlim(650,1400)
%@
%\caption{Pointwise $t$-tests for Example 2}\label{ResultsExample2}
%\end{figure}

\begin{figure}\centering
<<Example2Differences, fig=TRUE,echo=FALSE, width=8, height=3>>=
ggplot(TTestExample2, aes(Northing, Delta)) + 
geom_point(aes(colour = Significant),size=1) + 
scale_colour_manual(values=cbPalette[pair.colors]) + 
labs(colour = "Significant", x="Northing", y="Yield (bu/acre)", title = "Example 2 Yield Difference")
@
\caption{Pointwise yield differences $\bar{d}(x)$ for Example 2. Compare this with Figure \ref{Example1Differences}. Paired differences are computed for only the regions in the field were strips were replicated, so this plot is missing values from the southern end of the field. The number of pairs of strips is shown in Figure \ref{Example2N}. Yield differences tend to be negative in the northern portion of the field, suggesting that spraying in these local environments was detrimental. There were positive yield differences in the southern portion of the field. The southern portion of the field is dominated by wet spots.}
\label{Example2Differences}
\end{figure}

\begin{figure}\centering
<<Example2N, fig=TRUE,echo=FALSE, width=8, height=3>>=
ggplot(TTestExample2, aes(Northing, N)) + 
geom_point(aes(colour = Significant),size=1) + 
scale_colour_manual(values=cbPalette[pair.colors]) + 
labs(colour = "Significant", x="Easting", y="Number of Strips", title = "Example 2 number of strips (n)")
@
\caption{Pointwise number of strips $n(x)$ for Example 2. Unlike Example 2, where the number of experimental strips was constant along the length of the field, for these data the number of strips available for analysis varies with distance \texttt{Northing}.}
\label{Example2N}
\end{figure}

\begin{figure}\centering
<<Example2SD, fig=TRUE,echo=FALSE, width=8, height=3>>=
ggplot(TTestExample2, aes(Northing, SD)) + 
geom_point(aes(colour = Significant),size=1) + 
scale_colour_manual(values=cbPalette[pair.colors]) + 
labs(colour = "Significant", x="Easting", y="Standard Deviation", title = "Example 2 Standard Deviation")
@
\caption{Pointwise yield differences standard deviations $S_d(x)$ for Example 2. Standard deviation of the differences tends to be high across the length of the field, with discontinuities where the number of pairs of strips varies (see Figure \ref{Example2N})}
\label{Example2SD}
\end{figure}

\begin{figure}\centering
<<Example2T, fig=TRUE,echo=FALSE, width=8, height=3>>=
ggplot(TTestExample2, aes(Northing, t)) + 
geom_point(aes(colour = Significant),size=1) + 
scale_colour_manual(values=cbPalette[pair.colors]) + 
labs(colour = "Significant", x="Easting", y="t ratio", title = "Example 2 t-test")
@
\caption{Pointwise $t(x)$ ratio values for Example 2. In general the $t$ ratio is near 0, with the exception of a region near 100m \texttt{Northing}, where calculated standard deviation is low (Figure \ref{Example2SD})}
\label{Example2T}
\end{figure}

\begin{figure}\centering
<<Example2P, fig=TRUE,echo=FALSE, width=8, height=3>>=
ggplot(TTestExample2, aes(Northing, P)) + 
geom_point(aes(colour = Significant),size=1) + 
scale_colour_manual(values=cbPalette[pair.colors]) + 
labs(colour = "Significant", x="Easting", y="P(>t)", title = "Example 2 p-values")
@
\caption{Pointwise $P(>t)$ values for Example 2. $p$-values follow the pattern of $t$ ratios (Figure \ref{Example2T}) and reaches a value of significance at only one point near 100m \texttt{Northing}. Note the discontinuities present; these are due in part to the differences in the number of pairs of strips (Figure \ref{Example2N}), which affects the available degrees of freedom for significance testing.}
\label{Example2P}
\end{figure}

Figures \ref{Example2Differences}, \ref{Example2N}, \ref{Example2SD}, \ref{Example2T} and  \ref{Example2P} show the functional paired $t$-test as described for Example 1, but using the data from Example 2 (Figure \ref{Example2SprayYieldMaps}) In Figure \ref{Example2Differences}, that the average difference between fungicide-treated strips and the adjacent untreated strips regions varied considerably; from -10 to 10 bushels per acre. The curves for means and test statistics are discontinuous, reflecting the uneven nature of the treated strips. We find a single region of about four meters where we find statistically significant differences between the treated and untreated strips. The treated strips yielded roughly 5-10 bushels/acre less than untreated strips in this region. This difference occurred in a region of the field where there where only two treated strips that overlapped with unplantable regions of the field. However, this also occurs in region where there were only two pairs of strips to analyze, and show in Figure \ref{Example2N}, and significance may only be a result of the small number of observations contributing to the estimate of standard deviation.

In general, the fungicide trial as executed is not suitable for functional data analysis, due to the discontinuous nature of the strips and the naturally occurring unplantable spots in the field. However, these data may be amenable to spatial analysis using generalized linear models and the smoothing functionality \texttt{s} from the library \texttt{mgcv} \texttt{gam} function to fit interpolations in two dimensions. We explore this in a later chapter.

%\begin{figure}\centering
%<<CIPlotExample2,fig=TRUE,echo=FALSE, width=8, height=3>>=
%CIExample2 <- data.frame(
%  Northing = TTestExample2[,'Northing'],
%  Difference = TTestExample2[,'Delta'],
%  CI.Upper = TTestExample2[,'Delta']+qt(0.025,TTestExample2$N-1,lower.tail=FALSE)*TTestExample2[,'SD']/sqrt(TTestExample2$N),
%  CI.Lower = TTestExample2[,'Delta']-qt(0.025,TTestExample2$N-1,lower.tail=FALSE)*TTestExample2[,'SD']/sqrt(TTestExample2$N)
%)
%CIExample2$Significant <- sign(CIExample2$CI.Upper) == sign(CIExample2$CI.Lower)
%ggplot(CIExample2, aes(Northing,Difference)) + 
%geom_point(aes(col=Significant),size=2) +
%ylim(c(min(CIExample2$CI.Lower), max(CIExample2$CI.Upper)))+
%geom_line(aes(Northing,CI.Upper),col=cbPalette[3],data=CIExample2) +
%geom_line(aes(Northing,CI.Lower),col=cbPalette[3],data=CIExample2) +
%scale_colour_manual(values=cbPalette) +
%labs(x="Easting",y = "Difference (bu.acre)", title = "Fungicide Yield Differences")
%@
%\caption{Pointwise mean difference and confidence intervals among fungicide strips from Example 2}\label{CIPlotExample}
%\end{figure}

\section{Summarizing results}

Presentation of experimental results in functional form as an appeal. Consider how a farmer might experience crop yields - in a harvester, moving through time and space. Yield changes may be detectable by changes in engine speed or output; a farmer may remember patches of different soil types experienced during planting. 

This method is not suitable for all executions of on-farm strip trials. In this example, the length of each strip is large, compared to the distance across strips. Thus, we can capture a degree of information about spatial yield variability along the length of strips and minimize the variance between strips, relative to length. A field with many short strips may not be as amenable to this type of analysis, and if the strips are short enough, methods associated with small-plot analysis may be preferred.

This method may also be problematic when the strips are not all of the same length or there are gaps in the strips, as shown in Example 2. The number of replicates is not a constant function of distance, so there will be jumps in the functional form of the $t$-test; this is a less visually appealing presentation and may make interpretation difficult.

We've made a deliberate choice to use the default smoothing algorithm chosen by the software package \texttt{mgcv}. If our interest where only the analysis of a single strip trial, we might identify optimal smoothing parameters for each harvest strip by systematically varying the number of knots per strip (\texttt{k} as a argument to \texttt{s}, in \texttt{gam} syntax). However, for this method to be broadly useful, it would be a applied to a large number of strip trials, and fitting harvest strips individually may be prohibitive. In this case, we accept the default smoothing interpolating of \texttt{mgcv} \texttt{gam} function as good enough for general use.

It might be possible to code a single model that fits all harvest passes simultaneously, using a single smoothing parameter for each harvest pass. However, variability for these data is patchy in 2 dimensions. Consider, for example, the low yielding zone at the southern most portion of the field at about 100m easting. The optimal smoothing parameter for harvest passes passing through this region is likely to be different than harvest passes in the northern portion of the field, where yield across the field is higher yielding. 

Frequently, fields are partitioned into management zones\cite{clay.7-02-2018}. If the strips are oriented such that harvest passes cover different management zones, and management zone have be previously defined, this method of presenting analysis can include management zones in the model as a covariate, or visual inspection of the pointwise statistical analysis can give insight into the behavior of the treatments in different management zones.

Finally, the examples presented here were not truly randomized within blocks; each pair of strips has the same treatment order. It would be desirable to assign treatments at random among pairs of strips, but that presents extra effort in the execution of the strip trial, and may be impossible as in the case of a split-planter trial. The lack of randomization among units may introduce a bias in the analysis. Indeed, in the first example there is a definite yield trend from north to south that may have resulted in larger than expected yield differences among the two varieties. Spatial methods can be used to model variation in two dimensions and can provide a more accurate assessment 

\section{Summary of functional data analysis of strip trials}

In summary, we present a method of functional data analysis of strip trials based on pairwise $t$-tests. Pairwise $t$ tests where chosen because of the arrangement of strips, where each treated strip can be reasonably paired with an untreated strip. If treated strips where instead randomized independently of untreated strips, then a functional data analysis based on a independent sample $t$-test may be preferred. 

The analysis presented uses examples where only one treatment is applied. In some cases, more than one treatment may be applied to strips. If the treatments (including untreated) are appropriately randomized to strips, then a pointwise analysis of variance may be preferred, followed up by pointwise mean comparisons.

We note, however, that in our example data the treated and untreated strips were not independently randomized. Instead, treated strips were paired with untreated strips in a systematic pattern. Thus, the data as presented is not in a form suitable for inferences based on paired $t$-tests. We recognize this limitation and present the data in this chapter as illustrating a method only, and are not to be used for inferences. We do address this weakness in the following chapter and propose a method of pointwise analysis that will be more suitable for systematically applied strip trials.

\chapter{Pointwise Trend Analysis of Functional Strip Data}\label{TrendAnalysisChapter}

\section{Overview}

<<RelevelExample1Product, echo=FALSE, include=FALSE>>=
#re-level product to make the treatment coefficient positive
Example1.dat[,'Product'] <- factor(Example1.dat[,'Product'],levels=c("E","B"))
Example1HarvestPasses.dat[,'Product'] <- factor(Example1HarvestPasses.dat[,'Product'],levels=c("E","B"))
Example1SmoothHarvestPasses.dat[,'Product'] <- factor(Example1SmoothHarvestPasses.dat[,'Product'],levels=c("E","B"))
@

Yield means of experimental strips for Example 1 was shown as a bar plot (Figure \ref{Example1StripYieldsPlot}). But we see from this plot that the treatments were not randomized. Variety E was always planted in a strip to the south of a strip planted with product B. This is inherent in the method used to plant the strips - a split planter method was used, such that one half the planter units received one variety and the other strip received the other variety. Thus, varieties were not randomized independently. 

This can lead to variety effects being confounded with field effects. Indeed, in Figure \ref{Example1StripYieldsPlot} we see a clear trend of increasing yield with \texttt{Northing} position, and since Variety B in each pair always lies to the north of Variety E, we can expect a bias due to field effects confounded with varietal differences. We can account for this bias by using a regression analysis to smooth the spatial trend, then add to the trend a term for variety. 

\section{Trend analysis}

For pointwise trend analysis of functional harvest pass data, our model for strip yields takes the form \cite{hastie-1996, wood-2017}

\begin{equation}
y_i = a_i\theta(x) + f(z) + \epsilon_i(z)
\end{equation}\label{UnivariateGAM}

In this case, $a_i$ is an indicator variable for variety. Variety E is the baseline or intercept, $a_i=0$ when the variety planted in strip $i$ is labelled `E`, while $a_i=1$ incorporates the additive effect if the variety planted in strip $i$ is variety labelled "B". $f(z)$ is a smoothing function orthogonal to the smoothing functions described in prior sections. That is, where $x$ represents a position within a strip, $z$ represents the position of the strip relative to the other strips (for example, \texttt{Northing} of strips planted along a west-to-east axis. As before, we can computed a smooth function using the default \texttt{s} specification in \texttt{mgcv} \texttt{gam} function. 

A first pass at using \texttt{gam} to provide a spatial trend smoother, we use strip means for harvest strips. There are two harvest strips for each variety, so there are two means per variety. The split-planter layout was duplicated six times, so there are 24 total mean estimates, two harvest strips per variety and two varieties per planted strip. Additionally, for each strip, the average \texttt{Northing} value is taken as the position of the strip. These data were fit to a generalized additive model using syntax of the form \texttt{Yield \textasciitilde Product + s(Northing)}, where \texttt{Product} is the parametric portion of the model and \texttt{Northing} is a spatial covariate. 

<<TrendAnalysisMeans, echo=FALSE>>=
Yield.gam <- gam(Yield ~ Product + s(Northing), data= Example1HarvestPasses.dat)
#plot(Yield.gam)
@

\section{Trend analysis of strip means, Example 1}

\begin{figure}\centering
<<PointEstimatesWithPrediction, fig=TRUE,echo=FALSE, width=8, height=3>>=
#Now we add the predicted values to the trend
Example1HarvestPasses.dat[,'Predicted'] <- predict(Yield.gam)
ggplot(Example1HarvestPasses.dat, aes(x=Northing, y=Yield)) +
  geom_point(aes(color=Product, fill=Product)) + scale_colour_manual(values=cbPalette[pair.colors]) + scale_fill_manual(values=cbPalette[pair.colors]) +
  geom_line(aes(x=Northing, y = Predicted),color=cbPalette[1])
@
\caption{Point estimates of strip yield with predicted yield from a GAM model fit using \texttt{mgcv} with the default arguments to \texttt{s}. The point estimates should be compared to Figure \ref{Example1StripYieldsPlot}, where yield estimates are shown as bars instead of points. We use points in this plot to emphasis the functional nature of the observations with respect to position (\texttt{Northing} in the field. The predicted yield from the \texttt{mgcv} fit followed a 'toothed' pattern, as each pair of yield observations represents a different variety from the adjacent pair.}
\label{PointEstimatesWithPrediction}
\end{figure}

Figure \ref{PointEstimatesWithPrediction} shows the point estimates of yield, along with GAM interpolation. The GAM interpolation show a general increase in yield from the south end of the field to the north end, which matches visual inspection of the raw data. The GAM interpolation form as gear-tooth pattern, as the `Product` effect is additive. The coefficient associated with variety B is 12.16, which suggest that variety B out-yields variety E by 12 bushels/acre. This estimate is smaller than we found by computing a paired $t$-test from strip yields (15.48 bu/acre, $B-E$). This suggest that the paired $t$ test was biased by the spatial trend. The associated $t$-statistic (computed using `summary` of the  \texttt{gam} object) is 5.80, with an associated $p$-value $<0.0001$. 

There are 9 coefficients associated with the smoothing portion of the model, with an effective degrees of freedom of 4.845 (see \cite{wood-2017}, p 252 for a formal definition of effective degrees of freedom in a GAM model)

<<Example1UnivariateTrendModel, eval=FALSE, include=FALSE,echo=FALSE>>=
#anova(Yield.gam)
#summary(Yield.gam)
#Yield.sum <- summary(Yield.gam)
#str(summary(Yield.sum))
#Yield.sum[['p.coeff']][2]
#Yield.sum[['p.t']][2]
#Yield.sum[['p.pv']][2]
@

\section{Pointwise Trend Analysis of Harvest Strips from Example 1, Interpolated using \texttt{mgcv} Defaults}

<<ComputePointwiseTrendAnalysis, echo=FALSE>>=
smooth.basis <- rep(seq(2,398, by=2))
Trends <- data.frame(
  Easting = smooth.basis
)
for(i in 1:length(smooth.basis)) {
  l <- smooth.basis[i]
  obs <- Example1SmoothHarvestPasses.dat[Example1SmoothHarvestPasses.dat[,'Easting']==l,]
  current.gam <- gam(Yield ~ Product + s(Northing), data = obs)
  #current.gam <- gam(Yield ~ Product + s(Northing, bs='ps'), data = obs)
  #failed to converge after 400 iterations
  #current.gam <- gam(Yield ~ Product + s(Northing, bs='ad'), data = obs)
  current.sum <- summary(current.gam)
  Trends[i,'p.coeff'] <- current.sum[['p.coeff']][2]
  Trends[i,'p.t'] <- current.sum[['p.t']][2]
  Trends[i,'p.pv'] <- current.sum[['p.pv']][2]
  Trends[i,'edf'] <- current.sum[['edf']]
  Trends[i,'rank'] <- current.sum[['rank']]
}
Trends[,'Significant'] = Trends[,'p.pv']<0.05
@

\begin{figure}\centering
<<Example1PointwiseBCoefficients, fig=TRUE,echo=FALSE, width=8, height=3>>=
ggplot(Trends, aes(Easting, p.coeff)) + 
geom_point(aes(color=Significant)) + 
scale_colour_manual(values=cbPalette) +
labs(x="Easting", y="Yield (bu/acre)", title = "Product B Coefficient")
@
\caption{Pointwise variety B coefficients from \texttt{mgcv} trend analysis. The coefficient is the additive effect of variety B relative to variety E, after accounting for a function smoothing trend across the width of the field. This estimate varies by position in the field, with the largest affect attributed to B occurring in the eastern third of the field, which tends to be the higher yielding region of the field (Figure \ref{Example1SegmentYieldsPlot}). }
\label{Example1PointwiseBCoefficients}
\end{figure}

\begin{figure}\centering
<<Example1PointwiseBtTests, fig=TRUE,echo=FALSE, width=8, height=3>>=
ggplot(Trends, aes(Easting, p.t)) + 
geom_point(aes(color=Significant)) + 
scale_colour_manual(values=cbPalette) +
labs(x="Easting", y="t", title = "Product B t ratios")
@
\caption{Pointwise $t$ statistics for the coefficients in Figure \ref{Example1PointwiseBCoefficients}. The values are associated with pointwise model coefficients are extracted from \texttt{mgcv} models using the \texttt{summary} function. There are unexpected discontinuities is this curve; the discontinuities are also present in Figure \ref{Example1PointwiseBCoefficients} but are  more pronounced in this curve.}\label{Example1PointwiseBtTests}
\end{figure}

\begin{figure}\centering
<<Example1PointwiseBpValues, fig=TRUE,echo=FALSE, width=8, height=3>>=
ggplot(Trends, aes(Easting, p.pv)) + 
geom_point(aes(color=Significant)) + 
scale_colour_manual(values=cbPalette) +
labs(x="Easting", y="p", title = "Product B p values")
@
\caption{Pointwise $p$ values for the $t$ ratios in Figure \ref{Example1PointwiseBtTests}. For most of the length of the field, $p$ values are low, indicating significant differences between the varieties. The two regions with relatively large $p$-values correspond to the portion of the field dominated by low-yields zones, near 100m and again near 200m. As with the $t$ value curve, this curve shows discontinuities that cannot be easily explained by the data.}
\label{Example1PointwiseBpValues}
\end{figure}

Now, we extend this analysis to point-wise functional analysis, much as the paired $t$-test was extended to functional data. Interpolated harvest passes as shown in Figure \ref{Example1SmoothedHarvestPasses}. For each point in the set ${2,4, \cdots, 398}$m the GAM smoother \texttt{Yield \textasciitilde Product + s(Northing)} was computed, where \texttt{Northing} was computed as the average of \texttt{Northing} values associated with the current strip. Figure \ref{Example1PointwiseBCoefficients} shows resulting coefficient associated with product B, while Figure \ref{Example1PointwiseBtTests} shows the associated $t$ statistic and Figure \ref{Example1PointwiseBpValues} the associated $p$-values. 

The coefficient associated with variety B varies along the length of the strip, with the largest coefficients occurring on the eastern end of the field. Visual inspection of the yield map (Figure \ref{Example1Maps}) suggests that the eastern end is higher yielding, so the trend in Figure \ref{Example1PointwiseBCoefficients} suggests that variety B tends to outperforms variety E, but more-so when yield in general are higher. The coefficient associated with variety is statistically significantly different than 0 for most of the length of the field, which strengthens the assertion that variety B out yields variety E.

%```{r,CombinedTrends, fig.cap="Pointwise product E coefficient, t and p values", echo=FALSE,fig.width=8,fig.height=6}
%CombinedTrends <- data.frame(
%  Easting=rep(Trends[,'Easting'],4),
%  Measure = c(rep('Treatment Coefficient',length(Trends[,'p.coeff'])),
%              rep('t Ratio',length(Trends[,'p.t'])),
%              rep('Probability>t',length(Trends[,'p.pv'])),
%              rep('Effective d.f.',length(Trends[,'edf']))),
%  Value = c(Trends[,'p.coeff'],Trends[,'p.t'],Trends[,'p.pv'],Trends[,'edf']),
%  Significant = rep(Trends[,'p.pv']<0.05,4)
%)
%CombinedTrends[,'Measure'] <- factor(CombinedTrends[,'Measure'],levels=c('Treatment Coefficient','t Ratio','Probability>t','Effective d.f.'))
%ggplot(CombinedTrends, aes(Easting,Value)) + 
%geom_point(aes(color=Significant)) + #cbPalette[1]) + 
%scale_colour_manual(values=cbPalette) +
%labs(x="Easting", title = "Functional Statistics") + facet_wrap(~ Measure,nrow = 4,scales="free_y")
%```

\section{Discontinuities in the Example 1 pointwise trend analysis}


\begin{figure}\centering
<<Example1210PointwiseBCoefficients, fig=TRUE,echo=FALSE, width=8, height=3>>=
mask = Trends[,'Easting'] %in% 210:240
ggplot(Trends[mask,], aes(Easting, p.coeff)) + 
geom_point(aes(color=Significant)) + 
scale_colour_manual(values=cbPalette) +
labs(x="Easting", y="Yield (bu/acre)", title = "Product B Coefficient from 210-240m")
@
\caption{Pointwise variety B coefficients from \texttt{gam} trend analysis in the region of 210-240m \texttt{Easting}. This reproduces the portion of Figure \ref{Example1PointwiseBCoefficients} where there were discontinuities in model coefficients.}
\label{Example1210PointwiseBCoefficients}
\end{figure}

We should be cautious about the interpretation of Figures \ref{Example1PointwiseBCoefficients}, \ref{Example1PointwiseBtTests} and \ref{Example1PointwiseBpValues}. There are unexpected discontinuities in the pointwise functional data statistics. We focus in particular in the region from about 210-240m \texttt{Easting}. Figure \ref{Example1210PointwiseBCoefficients} shows the pointwise coefficients associated with Product B in the 210-240m region. Note that there is an abrupt increase in the yield (bu/acre) effect associated with product B between 214 and 216m. This is a similar abrupt change in coefficient value at 232m. 


\begin{figure}\centering
<<Example1HarvestPasses210, fig=TRUE,echo=FALSE, width=8, height=3>>=
Sub.dat <- Example1SmoothHarvestPasses.dat[Example1SmoothHarvestPasses.dat[,'Easting']>=210,]
Sub.dat <- Sub.dat[Sub.dat[,'Easting']<=240,]
ggplot(Sub.dat, aes(Easting,Yield)) + 
geom_line(aes(colour = Product, group=Pass),linewidth=.2) + 
scale_colour_manual(values=cbPalette[pair.colors]) + 
labs(colour = "Variety", x="Easting", y="Yield", title = "East Quarter, Pooled Strips") +
  ylim(100, 240)
@
\caption{Interpolated harvest passes in the region 210-240m \texttt{Easting}. This is a subset of the data in \ref{Example1SmoothedHarvestPasses}, corresponding to the region containing discontinuities in estimated treatment coefficients (see Figure \ref{Example1210PointwiseBCoefficients}) and associated $t$ statistics and $p$ values. As we see in this figure, there are no discontinuities in the smoothed yield data used to computed variety effects.}
\label{Example1HarvestPasses210}
\end{figure}

Figure \ref{Example1HarvestPasses210} show the interpolated harvest passes in the region from 210m to 240m \texttt{Easting}. Inspection of these curves reveals no obvious reason for discontinuities in the functional analysis. However, Figure \ref{Example1210PointwiseBCoefficients} suggests the discontinuities are introduced as unintended side-effect of the \texttt{mgcv} analysis. At 216m, effective degrees of freedom of the smooth (\texttt{s}) portion of the \texttt{mgcv} model is 1. This implies the best fit to the \texttt{Northing} trend is a straight line.

\begin{figure}\centering
<<Example1PointwiseEDF210, fig=TRUE,echo=FALSE, width=8, height=3>>=
ggplot(Trends[mask,], aes(Easting, edf)) + 
geom_point(aes(color=Significant)) + 
scale_colour_manual(values=cbPalette) +
labs(x="Easting", y="Effective d.f.", title = "Pointwise Degrees of Freedom")
@
\caption{Pointwise effective degrees of freedom for \texttt{mgcv} trend analysis in the region of 210-240m \texttt{Easting}. The effective degrees of freedom in this figure are those values computed by \texttt{mgcv} for the smooth portion of a generalized additive trend model described in the text (as illustrated in Figure \ref{PointEstimatesWithPrediction}). The effective degrees of freedom drops varies slightly at 210-214m \texttt{Easting}, but drops abruptly to 1 until 232m, where it again rises to values on the order of 8 d.f. This drop corresponds to the discontinuities in Figure \ref{Example1PointwiseBCoefficients} and related graphs. Note that an effective degrees of freedom of 1 corresponds to a straight line smoother.}
\label{Example1PointwiseEDF210}
\end{figure}

%```{r,CombinedTrendsSub, fig.cap="Pointwise product E coefficient, t and p values for points in the range of 210-240m Easting", echo=FALSE,fig.width=8,fig.height=6}
%mask = CombinedTrends[,'Easting'] %in% 210:240
%ggplot(CombinedTrends[mask,], aes(Easting,Value)) + 
%geom_point(aes(color=Significant)) + #cbPalette[1]) + 
%scale_colour_manual(values=cbPalette) +
%labs(x="Easting", title = "Functional Statistics") + facet_wrap(~ Measure,nrow = 4,scales="free_y")
%```


\begin{figure}\centering
<<Example1PointwiseEDF, fig=TRUE,echo=FALSE, width=8, height=3>>=
ggplot(Trends, aes(Easting, edf)) + 
geom_point(aes(color=Significant)) + 
scale_colour_manual(values=cbPalette) +
labs(x="Easting", y="Effective d.f.", title = "Pointwise Degrees of Freedom")
@
\caption{Pointwise effective degrees of freedom for \texttt{mgcv} trend analysis. Effective degrees of freedom of generalized additive trend models vary by position in the field. For the most part, these values vary smoothly, with adjacent values being similar. However, in three regions the effective degrees of freedom drop dramatically; these regions correspond to discontinuities in pointwise analysis (e.g. Figure \ref{Example1PointwiseBCoefficients})}
\label{Example1PointwiseEDF}
\end{figure}

This behavior of the fitted curves is troubling, so we examine the output of \texttt{mgcv}. Figure \ref{Example1PointwiseEDF} shows the effective degrees of freedom associated with each point-wise \texttt{mgcv} fit. Although effective degrees of freedom of the default \texttt{gam} smoothed model varies considerable along the length of the field, in regions corresponding to discontinuities in Figure \ref{Example1Differences}, effective degrees of freedom drops dramatically to less than 2, suggesting the models in these regions are not appropriately smoothed and may be under-fitting the field spatial trend. This is disconcerting, in that this suggests that for GAM models must be individually fit (by selecting smoothing parameters (\texttt{k})) for each point in the set defined by ${2,4,\cdots,398}$. This is a large number of models, which makes point-wise functional trend analysis prohibitive for anything other than a small number of trials.


<<EDFCompDataFrame, echo=FALSE>>=
EDFComp <- data.frame(
  Easting = rep(smooth.basis, 3),
  Basis = c(rep("default", length(smooth.basis)),
            rep("bs='ps'", length(smooth.basis)),
            rep("bs='ad'", length(smooth.basis)))
            
)
EDFComp[,'EDF'] <- NA
EDFComp[,'Rank'] <- NA
@

<<EDFCompCalculations, echo=FALSE>>=
for(i in 1:length(smooth.basis)) {
  l <- smooth.basis[i]
  obs <- Example1SmoothHarvestPasses.dat[Example1SmoothHarvestPasses.dat[,'Easting']==l,]
  mask = EDFComp[,'Easting'] == l & EDFComp[,'Basis'] == "default"
  current.default.gam <- gam(Yield ~ Product + s(Northing), data = obs)
  current.default.sum <- summary(current.default.gam)
  EDFComp[mask,'EDF'] <- current.default.sum[['edf']]
  EDFComp[mask,'Rank'] <- current.default.sum[['rank']]
  
  mask = EDFComp[,'Easting'] == l & EDFComp[,'Basis'] == "bs='ps'"
  current.ps.gam <- gam(Yield ~ Product + s(Northing, bs='ps'), data = obs)
  current.ps.sum <- summary(current.ps.gam)
  EDFComp[mask,'EDF'] <- current.ps.sum[['edf']]
  EDFComp[mask,'Rank'] <- current.ps.sum[['rank']]
  
  mask = EDFComp[,'Easting'] == l & EDFComp[,'Basis'] == "bs='ad'"
  current.ad.gam <- gam(Yield ~ Product + s(Northing, bs='ad',k=9), data = obs)
  current.ad.sum <- summary(current.ad.gam)
  EDFComp[mask,'EDF'] <- current.ad.sum[['edf']]
  EDFComp[mask,'Rank'] <- current.ad.sum[['rank']]
  
}
EDFComp[,'Basis'] <- factor(EDFComp[,'Basis'], levels = c("default","bs='ps'","bs='ad'"))
#EDFComp[,'Basis'] <- factor(EDFComp[,'Basis'])
@

\begin{figure}\centering
<<Example1EDFCompEDF, fig=TRUE,echo=FALSE, width=8, height=3>>=
ggplot(EDFComp, aes(Easting, EDF)) + 
geom_line(aes(col=Basis), linewidth=1) + 
scale_colour_manual(values=cbPalette) + 
labs(colour = "Method", x="Easting", y="EDF", title = "Pointwise GAM Effective DF")
@
\caption{Effective degrees of freedom associated with pointwise \texttt{mgcv} fit to different bases. The discontinuities in effective degrees of freedom is present regardless of the spline basis chosen for smoothing; indeed, it appears that both penalized splines and adaptive splines show less consistent estimates of effective degrees of freedom than do the trends fit with b-splines.}
\label{Example1EDFCompEDF}
\end{figure}

To further investigate these discontinuities, we repeat the trend analysis using penalized splines (option \texttt{bs='ps'}) and adaptive splines (option \texttt{bs='ad'}) Figure \ref{Example1EDFCompEDF} shows the effective degrees of freedom reported by \texttt{mgcv} \texttt{gam} for the smoothers. In \ref{Example1EDFCompEDF} we see that both p-spline and adaptive spline bases also abruptly drop to 1 effective degrees of freedom for many of the point-wise trend estimates. We note that when we fit using \texttt{bs='ad'} we also specify \texttt{k=9}, since the default `k` for adaptive splines is larger than the number of points to be fit. In general, the default smoother seems the best behaved for finding trends across \texttt{Northing}.



<<SubsetAndAnalyze21416, include=FALSE,echo=FALSE>>=
Example1SmoothHarvestStrip214.dat <- Example1SmoothHarvestPasses.dat[Example1SmoothHarvestPasses.dat[,'Easting']==214,]
Example1SmoothHarvestStrip216.dat <- Example1SmoothHarvestPasses.dat[Example1SmoothHarvestPasses.dat[,'Easting']==216,]
Smoothed214.gam <- gam(Yield ~ Product + s(Northing), data = Example1SmoothHarvestStrip214.dat)
Smoothed216.gam <- gam(Yield ~ Product + s(Northing), data = Example1SmoothHarvestStrip216.dat)
#summary(Smoothed214.gam)
#summary(Smoothed216.gam)
#plot(Smoothed214.gam)
#plot(Smoothed216.gam)
@


\begin{figure}\centering
<<SubsetAndAnalyze21416RawDataPlot, fig=TRUE,echo=FALSE, width=8, height=3>>=
Example1SmoothHarvestStrip214.dat[,'Easting'] <- 214
Example1SmoothHarvestStrip216.dat[,'Easting'] <- 216
Example1SmoothHarvestStrip214.dat[,'Predicted'] <- predict(Smoothed214.gam)
Example1SmoothHarvestStrip216.dat[,'Predicted'] <- predict(Smoothed216.gam)
Smoothed2146.dat = rbind(Example1SmoothHarvestStrip214.dat,Example1SmoothHarvestStrip216.dat)
Smoothed2146.dat[,'Easting'] <- as.factor(Smoothed2146.dat[,'Easting'])
ggplot(Smoothed2146.dat, aes(Northing, Yield)) + 
geom_point(aes(col=Easting)) + #cbPalette[1]) + 
geom_line(aes(Northing, Predicted, col=Easting),data=Smoothed2146.dat) +
scale_colour_manual(values=cbPalette) +
labs(x="Northing", title = "Yield Estimates at Easting 214 and 216")
@
\caption{Yield estimates by Northing for \texttt{Easting} points 214m and 216m. This graph compares two of the pointwise trend analysis that comprise Figure \ref{Example1PointwiseBCoefficients} and related graphs. Estimates of yield from Figure \ref{Example1SmoothedHarvestPasses} corresponding to 214m and 216m \texttt{Easting} are shown as points, while the predicted curves from a generalized additive trend analysis are shown as lines. The curves are not smooth because variety effect is included in the predicted values. The line for 216m, however, is clearly linear with respect to \texttt{Northing} position, while the line for 214m does show some curvature. There is , however, little difference between the points used to generate these curves.}
\label{SubsetAndAnalyze21416RawDataPlot}
\end{figure}

To further our investigation of the behavior of \texttt{mgcv}, we consider the results for trend analysis at \texttt{Easting} distances of 214m and 216m.

In Figure \ref{SubsetAndAnalyze21416RawDataPlot} we focus on trend analysis at 214 and 216 meters \texttt{Northing}. The points are interpolated yield estimates using the \texttt{mgcv} \texttt{gam} function on the 24 yield monitor data curves, while the lines are \texttt{mgcv} \texttt{gam} smoothing curves of \texttt{Yield} by \texttt{Northing}. We particularly note that the points are nearly superimposed over each other at similar \texttt{Northing} positions. This is to be expected since the point estimates are derived from smoothed functional forms of the original data. However, for \texttt{Northing} 214, the \texttt{gam} solver finds a curve with 7.7 effective degrees of freedom, while for \texttt{Northing} 216, with the same function arguments and nearly the same data, \texttt{gam} finds an interpolating curve with 1 degree of freedom - essentially a straight line for the smoothing part of the model. The curves themselves are not simple lines, since the \texttt{mgcv} model includes `Variety` in the fixed effects component of the model.


\section{Pointwise trend analysis of harvest strips from Example 1, interpolated using adaptive splines}

We now consider that the discontinuities follow from the method used to interpolate the harvest passes. We repeat the above analysis, but this time base our regression on point-wise yield interpolations based on adaptive splines (\texttt{bs='ad'} in \texttt{mgcv} notation).

<<ComputePassInterpolationsAD, echo=FALSE>>=
SmoothedAD.dat <- data.frame(
  Yield = rep(0, total.passes*length(smooth.basis)),
  Easting = rep(smooth.basis, total.passes),
  PassNo = rep(1:total.passes, each=length(smooth.basis)),
  Pair = rep(0, total.passes*length(smooth.basis)),
  Product = rep(levels(Example1.dat[,'Product']),total.passes*length(smooth.basis)/2),
  Northing = rep(0, total.passes*length(smooth.basis))
)
for(i in 1:total.passes) {
  current.pass <- Example1.dat[Example1.dat[,'PassNo']==i,]
  #current.gam <- gam(Yield ~ s(Easting),data=current.pass)
  current.gam <- gam(Yield ~ s(Easting, bs='ad'),data=current.pass)
  current.smoothed <- predict(current.gam, data.frame(Easting = smooth.basis))
  SmoothedAD.dat[SmoothedAD.dat[,'PassNo']==i,'Yield'] <- current.smoothed
  SmoothedAD.dat[SmoothedAD.dat[,'PassNo']==i,'Pair'] <- current.pass[,'Pair'][1]
  SmoothedAD.dat[SmoothedAD.dat[,'PassNo']==i,'Product'] <- as.character(current.pass[,'Product'][1])
  SmoothedAD.dat[SmoothedAD.dat[,'PassNo']==i,'Northing'] <- mean(current.pass[,'Northing'])
}
SmoothedAD.dat[,'Pass'] <- as.factor(SmoothedAD.dat[,'PassNo'])
#keep reference levels the same as prior analysis
SmoothedAD.dat[,'Product'] <- factor(SmoothedAD.dat[,'Product'],levels=c("E","B"))
@

\begin{figure}\centering
<<Example1SmoothedPassesAD, fig=TRUE,echo=FALSE, width=8, height=3>>=
ggplot(SmoothedAD.dat, aes(Easting,Yield)) + 
geom_line(aes(colour = Product, group=PassNo),linewidth=.2) + 
scale_colour_manual(values=cbPalette[pair.colors]) + 
labs(colour = "Variety", x="Easting", y="Yield", title = "Example 1 Harvest Strips") #+ylim(100, 240)
@
\caption{Interpolated yield values for harvest strips from Example 1 using adaptive splines. This reproduces the functional forms of harvest pass yield shown in Figure \ref{Example1SmoothedPassesAD}, but in this case \texttt{bs='ad'} was used as an argument to the \texttt{mgcv} \texttt{s} specifier instead of the default. Note that the resulting functional forms of harvest data is less smooth than the defaults.}
\label{Example1SmoothedPassesAD}
\end{figure}

<<ComputePointwiseTrendAnalysisAD, include=FALSE, echo=FALSE>>=
TrendsAD <- data.frame(
  Easting = smooth.basis
)
for(i in 1:length(smooth.basis)) {
  l <- smooth.basis[i]
  obs <- SmoothedAD.dat[SmoothedAD.dat[,'Easting']==l,]
  current.gam <- gam(Yield ~ Product + s(Northing), data = obs)
  #current.gam <- gam(Yield ~ Product + s(Northing, bs='ps'), data = obs)
  #failed to converge after 400 iterations
  #current.gam <- gam(Yield ~ Product + s(Northing, bs='ad'), data = obs)
  current.sum <- summary(current.gam)
  TrendsAD[i,'p.coeff'] <- current.sum[['p.coeff']][2]
  TrendsAD[i,'p.t'] <- current.sum[['p.t']][2]
  TrendsAD[i,'p.pv'] <- current.sum[['p.pv']][2]
  TrendsAD[i,'edf'] <- current.sum[['edf']]
  TrendsAD[i,'rank'] <- current.sum[['rank']]
}
TrendsAD[,'Significant'] <- TrendsAD[,'p.pv'] < 0.05
@

\begin{figure}\centering
<<Example1PointwiseBCoefficientsAD, fig=TRUE,echo=FALSE, width=8, height=3>>=
ggplot(TrendsAD, aes(Easting, p.coeff)) + 
geom_point(aes(color=Significant)) + 
scale_colour_manual(values=cbPalette) +
labs(x="Easting", y="Yield (bu/acre)", title = "Variety B Coefficient")
@
\caption{Pointwise variety B coefficients from \texttt{gam} trend analysis using harvest passes interpolated with adaptive splines. This reproduces the trend analysis shown in Figure \ref{Example1PointwiseBCoefficients}, but with yield estimates from Figure \ref{Example1SmoothedPassesAD}. Note that these estimates are less smooth, with respect to \texttt{Easting} position in the field, than the estimates from Figure \ref{Example1PointwiseBCoefficients}.}
\label{Example1PointwiseBCoefficientsAD}
\end{figure}

\begin{figure}\centering
<<Example1PointwiseEDFAD, fig=TRUE,echo=FALSE, width=8, height=3>>=
ggplot(TrendsAD, aes(Easting, edf)) + 
geom_point(aes(color=Significant)) + 
scale_colour_manual(values=cbPalette) +
labs(x="Easting", y="Effective d.f.", title = "Pointwise Degrees of Freedom")
@
\caption{Pointwise effective degrees of freedom for \texttt{mgcv} trend analysis. As with Figure \ref{Example1PointwiseEDF}, effective degrees of freedom of the smoothed portion of a generalized additive trend model varies by position in the field, but in this graph, based on adaptive splines instead of \texttt{mgcv} defaults, is more variable. The region where effective degrees of freedom drops to 1 is still present, but occurs at a different \texttt{Easting} position than the found in Figure \ref{Example1PointwiseEDF}.}
\label{Example1PointwiseEDFAD}
\end{figure}

%```{r,CombinedTrendsAD, fig.cap="Pointwise product E coefficient, t and p values", echo=FALSE,fig.width=8,fig.height=6}
%CombinedTrendsAD <- data.frame(
%  Easting=rep(Trends[,'Easting'],4),
%  Measure = c(rep('Treatment Coefficient',length(TrendsAD$p.coeff)),
%              rep('t Ratio',length(TrendsAD$p.t)),
%              rep('Probability>t',length(TrendsAD$p.pv)),
%              rep('Effective d.f.',length(TrendsAD$p.pv))),
%  Value = c(TrendsAD$p.coeff,TrendsAD$p.t,TrendsAD$p.pv,TrendsAD$edf),
%  Significant = rep(TrendsAD$p.pv<0.05,4)
%)
%CombinedTrendsAD$Measure <- factor(CombinedTrendsAD$Measure,levels=c('Treatment Coefficient','t Ratio','Probability>t','Effective d.f.'))
%ggplot(CombinedTrendsAD, aes(Easting,Value)) + 
%geom_point(aes(color=Significant)) + #cbPalette[1]) + 
%scale_colour_manual(values=cbPalette) +
%labs(x="Easting", title = "Functional Statistics") + facet_wrap(~ Measure,nrow = 4,scales="free_y")
%```

%\begin{figure}\centering
%<<,fig=TRUE,echo=FALSE, width=8, height=3>>=

%@
%\caption{}\label{}
%\end{figure}

As we see in \ref{Example1PointwiseBCoefficientsAD}, there are fewer discontinuities in the point-wise product B coefficient, but discontinuities are not eliminated; in particular, there is a discontinuity at 190-200m. If we consider the effective degrees of freedom associated with each point-wise model, there are clear departures from a smoothly continuous function and points where the effective d.f. are 1, indicating a straight line estimate. In general, the variety effect estimates are very wiggly, in some cases masking the discontinuities. We need to look at Figure \ref{Example1PointwiseEDFAD} to discern where there might be issues with adjacent trend analysis.

\section{Pointwise trend analysis using ordinary least squares}

<<ComputePointwiseTrendAnalysisLM, echo=FALSE>>=
for(i in 1:length(smooth.basis)) {
  l <- smooth.basis[i]
  obs <- Example1SmoothHarvestPasses.dat[Example1SmoothHarvestPasses.dat[,'Easting']==l,]
  current.lm <- lm(Yield ~ Product + bs(Northing, df=3), data = obs)
  Trends[i,'lm.coef3'] <- coef(current.lm)[2]
  current.lm <- lm(Yield ~ Product + bs(Northing, df=8), data = obs)
  Trends[i,'lm.coef8'] <- coef(current.lm)[2]
}
CoefficientsComp <- data.frame(
   Easting = rep(Trends[,'Easting'], 3),
   Coefficient = c(Trends[,'p.coeff'], Trends[,'lm.coef3'], Trends[,'lm.coef8']),
   Method = c(rep("GAM default", length(smooth.basis)),
              rep("LM BS(df=3)", length(smooth.basis)),
              rep("LM BS(df=8)", length(smooth.basis))
              )
)
@

\begin{figure}\centering
<<Example1EDFCompCoeffGAMLM, fig=TRUE,echo=FALSE, width=8, height=3>>=
ggplot(CoefficientsComp, aes(Easting, Coefficient)) + 
geom_line(aes(col=Method), linewidth=1) + 
scale_colour_manual(values=cbPalette) + 
labs(colour = "Method", x="Easting", y="Yield (bu/acre)", title = "Pointwise Coefficient Estimates")
@
\caption{Coefficients associated with variety B when pointwise trend analysis is fit using either \texttt{mgcv} or b-splines (\texttt{bs}) with \texttt{lm}. Note the discontinuities by \texttt{Easting} present in the \texttt{mgcv} model is not present in the \texttt{lm} models; however, the effective degrees of freedom for \texttt{lm} models is constant with respect to \texttt{Easting} position of the model, while for \texttt{mgcv} models the effective degrees of freedom can vary by position in the field.}
\label{Example1EDFCompCoeffGAMLM}
\end{figure}

Since the default \texttt{mgcv} fit produces discontinuities in the predicted coefficients due to errors in effective degrees of freedom, we might choose to use the \texttt{spline} package. However, we much chose an appropriate degrees of freedom. Since it is impractical to examine each trend fit individually, we fit pointwise trends using two degrees of freedom (\texttt{df=3} and \texttt{df=8}). Figure \ref{Example1EDFCompCoeffGAMLM} show the result of such a fit. We see that at about 300m \texttt{Easting} the three different methods agree on the effect of variety B, but otherwise are very different. B-splines with \texttt{df=3} tend to over-smooth the effect of variety B as we move eastward in the field, while splines with \texttt{df=8} tend to under-smooth variety B effect, relative to the response of variety B relative to variety E as estimated using \texttt{mgcv}. Splines fit using \texttt{lm} do not suffer from the discontinuities that we find in \texttt{mgcv} fit models, but it's not clear what the optimal choice of degrees of freedom is, based on the plot of coefficients. Instead, we should inspect the model fit at each \texttt{Easting} point; that is, we should inspect each of 199 possible models. That is obviously not ideal, and why we would hope that automatic smoothing providing by \texttt{mgcv} be useful in this context.

\section{Pointwise trend analysis of harvest strips from Example 1 using the \texttt{gam} package}

<<ComputePointwiseTrendAnalysisGAM, echo=FALSE>>=
library(gam)
TrendsGAM <- data.frame(
  Easting = smooth.basis
)
for(i in 1:length(smooth.basis)) {
  l <- smooth.basis[i]
  obs <- Example1SmoothHarvestPasses.dat[Example1SmoothHarvestPasses.dat[,'Easting']==l,]
  current.gam <- gam(Yield ~ Product + s(Northing), data = obs)
  current.sum <- summary(current.gam)
  TrendsGAM[i,'F'] <- current.sum[['parametric.anova']][1,'F value']
  TrendsGAM[i,'pf'] <- current.sum[['parametric.anova']][1,'Pr(>F)']
  TrendsGAM[i,'df'] <- current.sum[['anova']][3,"Npar Df"]
}
TrendsGAM[,'Significant'] <- TrendsGAM[,'pf']<0.05

detach("package:gam", unload=TRUE)
library(mgcv)
@

\begin{figure}\centering
<<Example1TrendAnalysisGAMF, fig=TRUE,echo=FALSE, width=8, height=3>>=
ggplot(TrendsGAM, aes(Easting, F)) + 
geom_point(aes(color=Significant)) + 
scale_colour_manual(values=cbPalette) +
labs(x="Easting", y="F", title = "Variety F value")
@
\caption{Pointwise $F$ values computed using the \texttt{gam} library for trend analysis. There is no simple method for extracting variety coefficients from a generalized additive trend model using the \texttt{gam} library, so instead of coefficients as in Figure \ref{Example1PointwiseBCoefficients} we show $F$ ratios and the associated $p$-values (Figure \ref{Example1TrendAnalysisGAMp}). Unlike Figure \ref{Example1PointwiseBCoefficients}, the curves in this graph do not suffer from discontinuities; however, for \texttt{gam} models the default degrees of freedom is held constant at 3 (Figure \ref{Example1TrendAnalysisGAMdf})}
\label{Example1TrendAnalysisGAMF}
\end{figure}

\begin{figure}\centering
<<Example1TrendAnalysisGAMp, fig=TRUE,echo=FALSE, width=8, height=3>>=
ggplot(TrendsGAM, aes(Easting, pf)) + 
geom_point(aes(color=Significant)) + 
scale_colour_manual(values=cbPalette) +
labs(x="Easting", y="p-value", title = "Variety p value")
@
\caption{Pointwise $p$ values associated with the $F$ values in Figure \ref{Example1TrendAnalysisGAMF}. See Figure \ref{Example1TrendAnalysisGAMF} for discussion.}
\label{Example1TrendAnalysisGAMp}
\end{figure}

\begin{figure}\centering
<<Example1TrendAnalysisGAMdf, fig=TRUE,echo=FALSE, width=8, height=3>>=
ggplot(TrendsGAM, aes(Easting, df)) + 
geom_point(aes(color=Significant)) + 
scale_colour_manual(values=cbPalette) +
labs(x="Easting", y="d.f.", title = "Smooth Term degrees of freedom")
@
\caption{Pointwise non-parametric (i.e. \texttt{s}) degrees of freedom for the \texttt{gam} models generating $F$ values in Figure \ref{Example1TrendAnalysisGAMF}. Unlike \texttt{mgcv} generalized additive models, the degrees of freedom of the smoothing term is not adapted to the data.}\label{Example1TrendAnalysisGAMdf}
\end{figure}


We consider using the \texttt{gam} function from the \texttt{gam} library. This function supports the \texttt{s} smoother syntax, but does not automatically adjust smoothing parameters for each fit, thus each point has the same smoother degrees of freedom. Further, the summary of the \texttt{gam} object does not include an estimate of fixed effect coefficients, only $F$ statistics are available. This does not allow us to easily determine the magnitude of varietal differences (as in Figure \ref{Example1PointwiseBCoefficients}), so the \texttt{gam} library is not an acceptable substitute for \texttt{mgcv} in this context. Further, the \texttt{gam} library does not automatically adjust smooth term degrees of freedom, as shown in Figure \ref{Example1TrendAnalysisGAMdf}.

%```{r,CombinedTrendsGAM, fig.cap="Pointwise product E coefficient, t and p values estimated using the \texttt{gam} library", echo=FALSE,fig.width=8,fig.height=6}
%CombinedTrendsGAM <- data.frame(
%  Easting=rep(TrendsGAM[,'Easting'],3),
%  Measure = c(rep('Treatment F',length(TrendsGAM$F)),
%              rep('Probability>F',length(TrendsGAM$pf)),
%              rep('Smoother d.f.',length(TrendsGAM$df))),
%  Value = c(TrendsGAM$F,TrendsGAM$pf,TrendsGAM$df),
%  Significant = rep(TrendsGAM$pf<0.05,3)
%)
%CombinedTrendsGAM$Measure <- factor(CombinedTrendsGAM$Measure,levels=c('Treatment F','Probability>F','Smoother d.f.'))
%ggplot(CombinedTrendsGAM, aes(Easting,Value)) + 
%geom_point(aes(color=Significant)) + #cbPalette[1]) + 
%scale_colour_manual(values=cbPalette) +
%labs(x="Easting", title = "Functional Statistics") + facet_wrap(~ Measure,nrow = 4,scales="free_y")
%```

\section{Pointwise trend analysis of harvest strips from Example 1 using the \texttt{brms} package}

We can also compute a trend analysis using \texttt{brm}, but this is computationally slower than other methods. Each call to \texttt{brm} requires an associated C++ compilation call to compile the \texttt{stan} model generated by \texttt{brm}, and this takes the greater part of the analysis. To fit 199 trend models (one for each \texttt{Easting} value in $2, 4,\cdots, 398$ ) took 1.4 hours using a 2022 MacBook Pro with an Apple M2 chip and 8 GB memory.

\texttt{brm} by default does not provide null hypothesis tests; Consider the univariate case, where we regress the harvest pass means against position from south to north. We see in this case that \texttt{brm} produces confidence intervals for the fixed parameter estimates. This changes the nature of the presentation of a functional analysis of the data. Where in Figures \ref{Example1Differences},\ref{Example1SD}, \ref{Example1T} and \ref{Example1P} we show the statistics associated with a null hypothesis test based on the $t$ statistic, in Figure \ref{CITrendCoefficientBRM} we simply show the pointwise effect estimate for variety B along with the upper and lower confidence bands. The information content of \texttt{brm} confidence bounds in Figure \ref{CITrendCoefficientBRM} is comparable to the difference plot with confidence bounds from the pointwise paired differences in Figure \ref{CIPlotExample1}. Confidence intervals can also be used to create a dummy variable that controls coloring of the curves in Figure \ref{CITrendCoefficientBRM} as significant or non-significant.

<<AverageTrendBRM, echo=FALSE, include=FALSE>>=
Yield.brm <- brm(Yield ~ Product + s(Northing), silent=2, refresh = 0, data= Example1HarvestPasses.dat)
#summary(Yield.brm)
#hypothesis(Yield.brm, "ProductB=0")
#plot(Yield.brm)
#Example1SmoothHarvestStrips.dat[,'Product'] <- factor(Example1SmoothHarvestStrips.dat[,'Product'], levels=c("E","B"))
@

<<ComputePointwiseTrendAnalysisBRM, echo=FALSE, include=FALSE>>=
if(!file.exists("Trend.brm")) {
  start <- Sys.time()
  TrendsBRM <- data.frame(
    Easting = smooth.basis
  )
  for(i in 1:length(smooth.basis)) {
    l <- smooth.basis[i]
    obs <- Example1SmoothHarvestPasses.dat[Example1SmoothHarvestPasses.dat[,'Easting']==l,]
    current.brm <- brm(Yield ~ Product + s(Northing),silent=2, refresh = 0, data = obs)
    current.sum <- summary(current.brm)
    TrendsBRM[i,'Coef'] <- current.sum[['fixed']][['Estimate']][2]
    TrendsBRM[1,'Error'] <- current.sum[['fixed']][['Est.Error']][2]
    TrendsBRM[i,'Sigma'] <- current.sum[['spec_pars']][['Estimate']][1] #sigma
    hypo.brm <- hypothesis(current.brm, "ProductB=0")
    TrendsBRM[i,'CI.Upper'] <- hypo.brm[['hypothesis']][['CI.Upper']]
    TrendsBRM[i,'CI.Lower'] <- hypo.brm[['hypothesis']][['CI.Lower']]
  
    #to test for significance, we check that the CI are of the same sign
    TrendsBRM[i,'Significant'] <- sign(hypo.brm[['hypothesis']][['CI.Lower']]) == sign(hypo.brm[['hypothesis']][['CI.Upper']])
    #TrendsBRM[i,'df'] <- current.sum[['anova']][3,"Npar Df"]
  }
  end <- Sys.time()
  BRMTime <- end-start
  save(TrendsBRM, BRMTime, file="Trend.brm")
} else {
  load(file="Trend.brm")
}
@

<<BRMTime, echo=FALSE, include=FALSE, eval=FALSE>>=
BRMTime
@

\begin{figure}\centering
<<CITrendCoefficientBRM, fig=TRUE,echo=FALSE, width=8, height=3>>=
ggplot(TrendsBRM, aes(Easting, Coef)) + 
geom_point(aes(col=Significant),size=2) +
ylim(c(min(TrendsBRM$CI.Lower), max(TrendsBRM$CI.Upper)))+
geom_line(aes(Easting, CI.Upper),col=cbPalette[3],data=TrendsBRM) +
geom_line(aes(Easting, CI.Lower),col=cbPalette[3],data=TrendsBRM) +
scale_colour_manual(values=cbPalette) +
labs(x="Easting",y = "Variety B Effect", title = "Variety Yield Difference")
@
\caption{Pointwise product B coefficient and confidence intervals estimated using the \texttt{brm} library. These curves are not smooth, but that is to be expected; \texttt{brm} fits models stochastically and so there is no expectation of continuity in coefficients estimates for adjacent trend analysis relative to position \texttt{Easting}. However, this curve does not suffer from the major discontinuities we find in Figure \ref{Example1PointwiseBCoefficients}}
\label{CITrendCoefficientBRM}
\end{figure}

We compare Figures \ref{Example1PointwiseBCoefficients} and \ref{CITrendCoefficientBRM}. Discounting the discontinuities in Figure \ref{Example1PointwiseBCoefficients}, the general shape of the estimated variety differences are similar; we see that the coefficient associated with variety E is smallest in the regions near 120 and 240 meters, and the difference greatest at 300 meters. From the general shape of the harvest passes in Figure \ref{Example1SmoothedHarvestPasses}, we can surmise by inspection that there are low yielding zones for about half the harvest passes, centered at about 120 and 240 meters. In the region of 300 meters, the harvest pass yield is higher and generally more stable across with width of the field. 

\section{Pointwise trend analysis summary}

If we consider the regression methods used here, we see that \texttt{mgcv} use with the default \texttt{s} arguments and \texttt{brm} produce similar estimates for the effect of variety E, and we can use the results to consider how spatial variability along the harvest passes affects these estimates. \texttt{mgcv} with the defaults is much faster, but produces discontinuities in the summary graphs (Figure \@ref(fig:CombinedTrends)). These discontinuities were not resolved by changing basis for smoothing individual harvest passes (Figure \ref{Example1PointwiseBCoefficientsAD}). Instead, the discontinuities appear to be caused by situations where \texttt{mgcv} defaults to an under-smoothed (i.e. straight) trend response, and it's not clear from the mathematic discussion of the methods for choosing smoothing penalties \cite{wood-2017} why two nearly identical trend data sets (Figure \ref{SubsetAndAnalyze21416RawDataPlot}) produce meaningfully different smoothing terms. However, in general \texttt{mgcv} is a stable software library, so we will continue to consider \texttt{mgcv} solutions in the following chapter.

It would appear the for this type of regression, \texttt{brm} is a more stable solution, however, there is a penalty for computational time. Unlike \texttt{mgcv}, \texttt{brm} does not support $t$-tests of effects, instead the default summary of a \texttt{brm} analysis includes confidence intervals of parameter estimates. $p$-values are not provided as part of the \texttt{brm} analysis, so are not displayed in Figure \ref{CITrendCoefficientBRM}, while we provide pointwise $p$-value graphs with other analysis. The \texttt{brm} plots are also less smooth, this is likely a side affect of the nature of \texttt{brm} computations. \texttt{brm} provides a front-end to the \texttt{stan} library, which implements MCMC sampling and so there is a degree of random variation for two adjacent point wise trend estimates.

Additionally, the \texttt{brm} fitting process frequently produces warnings of the type 

\begin{verbatim}
Warning: Bulk Effective Samples Size (ESS) is too low, 
indicating posterior means and medians may be unreliable.
Running the chains for more iterations may help. See
https://mc-stan.org/misc/warnings.html#bulk-ess
\end{verbatim}

\begin{verbatim}
Warning: Tail Effective Samples Size (ESS) is too low, 
indicating posterior variances and tail quantiles may be unreliable.
Running the chains for more iterations may help. See
https://mc-stan.org/misc/warnings.html#tail-ess
\end{verbatim}

\begin{verbatim}
Warning: Parts of the model have not converged (some Rhats are > 1.05). 
Be careful when analysing the results! We recommend running more iterations
and/or setting stronger priors.
\end{verbatim}

These messages suggests that the MCMC methods implemented in \texttt{stan} via \texttt{brm} require detailed analysis of each functional data unit, something we are trying to avoid in developing methods for analyzing strip trials.

The \texttt{gam} library offers a \texttt{gam} function that provides a default smoother, but this smoother is not adaptive and the default smoothness penalty is too large for the regressions at hand. It does not appear the the \texttt{gam} library will be of much benefit in resolving the problem of how to compute multiple trend analysis for functional data.

\chapter{Spatial analysis of Strip Trials}\label{SpatialAnalysisChapter}

\section{Overview}

To this point, we've fit harvest passes in one dimension, either \texttt{Easting} or \texttt{Northing}, depending on the data set. We've further fit a pointwise trend in the orthogonal dimensions, either \texttt{Northing} or \texttt{Easting}, again, depending on the data set. This begs the question - can we fit both dimensions simultaneously, generate a smooth response surface in two dimensions (i.e. $f(x, y)$) while also including in the model the variety or product associated with point $x, y$ (or, more specifically \texttt{Northing},\texttt{Easting}). Instead of pointwise estimates of the effect of agronomic treatment, we end up with a model that provides a single estimate for treatment effect.

\section{The spatial (response surface) model}

Consider the model for spatial analysis of design experiments, from \cite{schabenberger-2002}.
\begin{equation}
   Z(\mathbf{s}) = \mathbf{X}(\mathbf{s}) \mathbf{\beta} + \delta(\mathbf{s})
\end{equation}
where $Z$ is a single realization of a stochastic process in two dimensions and ${Z(\mathbf{s}) : \mathbf{s} \in D \subset \mathcal{R}^2}$ are values of $Z$ at location $\mathbf{s}$. $Z$ in this model has two components. The first, $\mathbf{X}(\mathbf{s}) \mathbf{\beta}$ specifies the design matrix of the experiment with associated fixed effect parameters $\mathbf{\beta}$ That is, $X$ specifies the treatment effect associated with point $\mathbf{s}$. For our data, $X$ will be composed of $0$ or $1$ indicator variables, dependent on whether point $\mathbf{s}$ was planted with variety $B$ or was sprayed with fungicide.

$Z(\mathbf{s})$ is a random variable, and has such as properties mean at location $\mathbf{s}$, $\mathbb{E}[Z(\mathbf{s}] = \mu(\mathbf{s})$ and variance at $\mathbf{s}$, $\mathbb{V}[Z(\mathbf{s})]$. When a spatial random variable $Z$ is said to be second order stationary, then $\mathbb{E}[Z(\mathbf{s}] = \mu$ (that is, the mean is independent of location $\mathbf{s}$, and that $Cov[Z(\mathbf{s}),Z(\mathbf{s}+\mathbf{h}))] = C(\mathbf{h})$ where $C$ is called covariance function and $\mathbf{h}$ is a translation applied to $\mathbf{s}$. 

$\delta(\mathbf{s})$ is assumed to be a second order stationary random field with semivariogram $\gamma(\mathbf{h})$ and covariogram $C(\mathbf{h})$. Stationarity of a random field implies that any part of the random field looks similar to any other part of the random field. More precisely, suppose we have two observations, one at point $\mathbf{s}$ and another at a displaced point $\mathbf{s} + \mathbf{h}$. The properties of the field $\delta$ should be similar a both points. This allows us to estimate a degree of covariance between possible pairs of points $\delta(\mathbf{s_i})$, $\delta(\mathbf{s_i}+\mathbf{h})$ such that the covariance depends only on $\mathbf{h}$ and not on the specific $s_i$. 

Now, we've consider $\delta$ as a random field, but Schabenberger \cite{schabenberger-2002} reminds of the adage \textit{one modeler's random effect is another modeler's fixed effect}. Specifically, in the case of $\delta$, we capture the spatial correlation structure as a smooth function over points $\mathbf{s}$, that is, $f(\mathbf{s}) = f(r, c)$, where $r$ is a row indicator and $c$ is a column indicator. This lead us to model the spatial structure of yield monitor data as a smoothed function of a generalized additive model (Equation \ref{GAMFormula}),

\begin{equation}
   g(y_{i}) = \mathbf{A_i} \mathbf{\theta} + f(x_2, x_3) + e_{i}
\end{equation}\label{SpatialGAM}

where $f$ is a smooth response (or trend) surface, $\mathbf{A_i}$ is a treatment structure associated with a point specified by row $x_2$ and column $x_3$, $\mathbf{\theta}$ are parameters associated with treatments and estimated from the data, and $f(x_2,x_3)$ is a smoothing function approximating the random field $\delta$. The $e_{ij}$ are an uncorrelated white noise process. Again, for these data, $g$ is the identity function, and there is no $f(x_1)$. We can use generalized additive models (i.e. \texttt{mgcv}) to fit this surface, using syntax of the form \texttt{s(Easting,Northing, k = *)}  We can also use \texttt{lm} with splines, specifically b-splines, using syntax of the form \texttt{bs(Easting, df=*)*bs(Northing, df=*)}

How should we determine the appropriate degree of smoothing? For this, we consider the properties of the semivariogram.

%$$
%Z(\mathbf{s_i}) = \theta(\mathbf{s}_i) + Y(\mathbf{s}_i) + e(\mathbf{s}_i)
%$$

%The spatial model consists of three components. The first is the fixed effect component and includes the experimental treatments applied to point $\mathbf{s_i}$. The inherent response or fertility, in the absence of treatment, is represented as a random spatial field $Y$  that can be index by points $\mathbf{s}_i$. Finally, there is a white-noise error process $e$ associated with measurement of the realized random field, also indexed by $\mathbf{s}_i$.

%\subsection{Information Criteria}

%Information criteria, such as AIC or BIC, are commonly used to select the 'best' number of parameters in a statistical model. 

%These are calculated as adding, or penalizing, some value based on the number of parameters from the log-likelihood. The formula for AIC can be simplified to

%$$
%AIC = - 2 \ln(\widehat{L}) + 2k
%$$
%where $\widehat{L}$ is the maximum value of the likelihood function for model, ($L=P(X|\theta,M)$, with $X$ being the data and $M$ the model with parameters $\theta$) and $k$ is the number of parameters in the model. Similarly, BIC is given by

%$$
%BIC =  - 2 \ln(\widehat{L}) + k \ln(n)
%$$
%with $k$ and $\widehat{L}$ as before and $n$ is the number of observations in $X$. These criteria are well defined for methods that depend on maximum likelihood estimation, but are less well suited when other optimization methods are employed, such as the penalized maximum likelihood used in GAM (generalized additive models). In this paper, we consider alternative methods for selection an optimal number of parameters for smoothing a spatial random field, specifically the semivariogram.

\section{The semivariogram}
 
%Consider a simple random field, absent any fixed effects:

%\begin{equation}
%Z(\mathbf{s_i}) = Y(\mathbf{s_i}) + e(\mathbf{s_i})
%\end{equation} \label{SimpleSpatialModel}

%One of the properties of a random field is the correlation structure among the various points $\mathbf{s_i}$. Specifically, the covariance function of two points $s_i$ and $s_i +h$ separated by a distance $h$ is a function strictly of the distance $h$, when the spatial field is isotropic and stationary. We write this as

%\begin{equation}
%var(Y(\mathbf{s} + \mathbf{h}) - Y(\mathbf{s} + \mathbf{h})) = 2\gamma_Y(\mathbf{h})
%\end{equation}\label{SemivariogramEquation}
Again from Schabenberger (\cite{schabenberger-2002}) we define the semivariogram of $Z(\mathbf{s})$ as

\begin{equation}
\gamma(\mathbf{h}) = \frac{1}{2} \mathbb{V}[Z(\mathbf{s})-Z(\mathbf{s}+\mathbf{h})]
\end{equation}\label{SemivariogramEquation}

When variance between two points $\mathbf{s}_i + \mathbf{h}$  and $\mathbf{s}_i$ is strictly a function of the distance between the points $\lVert\mathbf{h} \rVert $, and is not influenced by absolute position or direction, the process is said to be isotropic.

\section{Empirical semivariogram estimators}
 Following Schabenberger \cite{schabenberger-2002}, we write the estimator of the semivariogram as

\begin{equation}
   \widehat{\gamma}(\mathbf{h}) = \frac{1}{2|N(\mathbf{h})|} \sum_{N(\mathbf{h})} \left( Z(\mathbf{s_i}) - Z(\mathbf{s_j}) \right)^2
\end{equation}\label{EmpiricalSemivariogram}

where $N(\mathbf{h})$ is the set of points of points $s_1, s_2, \cdots$ that are separated by lag $\mathbf{h}$ and $|N(\mathbf{h})|$ is the number of such points. The pairs in $N(\mathbf{h})$ are unique, so, for example $\left( Z(\mathbf{s_1}) - Z(\mathbf{s_2}) \right)^2 = \left( Z(\mathbf{s_2}) - Z(\mathbf{s_1}) \right)^2$ is not counted twice. If the field is assumed to be isotropic, the $\mathbf{h}$ is replaced by $\lVert\mathbf{h} \rVert $; in our estimates of empirical semivariograms we will assume isotropy.

Since our data is not generated on a uniform grid, there will be some lag distances that have only one observation. To account for this, we compute variances based on lag class, that is, we group observations by $\lVert \mathbf{h} \rVert + \epsilon$. This does introduce a bias in our estimates of the semivariogram, and so interpretation of the semivariograms may be somewhat subjective.

Strictly speaking, because of the factor $1/2$ $\gamma(\mathbf{h})$ is called the semivariogram, while $2 \gamma(\mathbf{h})$ is called the variogram. This term is not always used consistently \cite{bachmaier-02-2008}; we will tend to use the term semivariogram in this context.

The semivariogram usually starts at some non-zero point at the $x$ origin (where $x$ represents $\mathbf{h}$, or the distance between two points). This non-zero value represents the nugget. The semivariogram usually rises along the $x$-axis, as the variance between points increases with distance. At some point the semivariogram become asymptotic; this upper limit is called the sill. 

There are a number of parametric models for semivariograms, including those known as the exponential model, the Gaussian model and the spherical model. As representative of most parametric functions for the semivariogram, the spherical model takes the form

\begin{equation}
   \gamma(\mathbf{h};\mathbf{\theta})=
   \begin{cases} 
       0 &: \lVert h \rVert =0 \\
       \theta_0 + \theta_s \left\{  \frac{3}{2} \frac{\lVert \mathbf{h} \rVert}{\alpha} - \frac{1}{2} \left( \frac{\lVert \mathbf{h} \rVert}{\alpha}\right)^3 \right\} &: 0 < \lVert \mathbf{h} \rVert \le \alpha \\
       \theta_0 + \theta_s &: \lVert \mathbf{h} \rVert > \alpha
    \end{cases}
\end{equation}

where $\theta_0$ is the nugget and $\theta_s$ is the sill. For our purposes, we aren't particularly interested in the sill. But the nugget $\theta_0$ is of interest, as we'll discuss later.

In general, empirical semivariograms are used for kriging to interpolate yield values at points where yield is not measured \cite{souza-03-2016}. However, in this analysis, we use empirical semivariograms to evaluate the quality of fit of response surfaces to noisy yield monitor data.

Ideally, the semivariogram will have a nugget of 0, as points not separated in space should be equal, and as such have no variance. That is, $\gamma(\mathbf{h}) \to 0$ as $\lVert \mathbf{h} \rVert \to 0$. In most data, there is a nugget $\theta_0 \ne 0$ such that $\gamma(\mathbf{h}) \to \theta_0$ as $\lVert \mathbf{h} \rVert \to 0$. Schabenberger (\cite{schabenberger-2002}) argues that the nugget is composes of two processes, a measurement error $\sigma^2_e$ that results from errors that might arise from repeated measurements at a single point in space, and a variance component derived from processes that operate on a smaller spatial scale than the observed lags ($\mathbf{h}$) in the data set; this process results in a variance component $\sigma^2_{\eta}$; such that $\theta_0 = \sigma^2_e + \sigma^2_{\eta}$. For our purposes, we will ignore $\sigma^2_{\eta}$, and assume that

\begin{equation}
\theta_0 = \sigma^2_e
\end{equation}

That is, the nugget in our semivariograms is a measure of the effects of $e_i$ from Equation \ref{SpatialGAM}. We seek to partition semivarigrams into two components; the part described by $f(\mathbf{s} = f(x_2, x_3)$ and one that is solely comprised of $e_i$.

So our process, then, is to fit spatial models and extract both the predicted values and residual values, us. We then find an empirical semivariogram using the \texttt{variogram} function from the \texttt{gstat} library \cite{bivand.r-2013}. The \texttt{variogram} returns a pair of vectors, \texttt{distance} ($\mathbf{h}$) and \texttt{gamma} ($\gamma(\mathbf{h})$). We then fit a b-spline using \texttt{lm} of the form \texttt{gamma \textasciitilde bs(distance)}. We then look for the model with a nugget not significantly different from 0, but produces the largest sill. This we take as evidence the the associated model has explained the majority of the spatial variation. Thus, we choose as the best most the most detailed model producing nugget not significantly different from 0. We call this the nugget selection.

Alternative, we consider that if a proposed spatial model adequately fits the data, then the residuals will be a white-noise process and have no spatial correlation. In this case the semivariogram will be a flat line. We test each proposed spatial most and choose the simplest model that produces a residual semivariogram that is flat; that is, the slope of a simple regression of variance versus distance (e.g. \texttt{lm(gamma \textasciitilde distance)}) is statistically not different from 0. We call this the residual method.

We compare these methods to more traditional methods of model selection, adjusted $R^2$ and cross-validation. When fitting a model using \texttt{lm}, we use the $k$-fold cross validation method, as the preferred leave-one-out cross validation is too computationally expensive for these data; some strip trials have several thousand data points. For the GAM fit models, we use the generalized cross validation score provided as part of the model fit.

\section{Preparation of example data for response surface analysis}

<<ReloadExample2, echo=FALSE>>=
#copied from LoadandProcessExample2, except we use the full field except for the small outlying portion.
Example2Full.dat <- read.csv(file='../ManagementZoneML/data/B 2019 Soybean Treated.csv')
left = 1000
Example2Full.dat <- Example2Full.dat[Example2Full.dat[,'Longitude'] >left,]
#the original data was anonymized to the southwest corner of 'Home'. After trimming the odd oblong shape, we standardize back to zero
Example2Full.dat[,'Longitude'] <- Example2Full.dat[,'Longitude'] - min(Example2Full.dat[,'Longitude'])
Example2Full.dat[,'Latitude'] <- Example2Full.dat[,'Latitude'] - min(Example2Full.dat[,'Latitude'])
#remove outliers
Example2Full.dat <- Example2Full.dat[Example2Full.dat[,'Yield']<80,]
Example2Full.dat <- Example2Full.dat[Example2Full.dat[,'Yield']>0,]

#data were read into QGIS and points labelled with Block
#Blocks 1-8 should be analyzed as strips, with even numbered blocks being sprayed
#Example2Full.dat <- Example2Full.dat[Example2Full.dat],'Block'] %in% 1:8,]
#keep only those harvest strips that are marked as sample
#Example2Full.dat <- Example2Full.dat[Example2Full.dat[,'Sample']>0,]
Example2Full.dat[,'Strip'] <- Example2Full.dat[,'Block']
Example2Full.dat[,'Sprayed'] <- Example2Full.dat[,'Block'] %in% c(2,4,6,8)
Example2Full.dat[,'Pass'] <- factor(Example2Full.dat[,'Block']):factor(Example2Full.dat[,'Sample'])
Example2Full.dat[,'Northing'] <- Example2Full.dat[,'Latitude']-min(Example2Full.dat[,'Latitude'])
Example2Full.dat[,'Easting'] <- Example2Full.dat[,'Longitude']-min(Example2Full.dat[,'Longitude'])
@

\begin{figure}\centering
<<Example2FullMaps, fig=TRUE, echo=FALSE, width=8, height=6>>=
grid.arrange(
  arrangeGrob(
       ggplot(Example2Full.dat, aes(Easting, Northing)) + 
            geom_point(aes(colour = Sprayed),size=1) + 
            scale_colour_manual(values=cbPalette[pair.colors]) +
            labs(colour = "Sprayed", x="Easting", y="Northing", title = "Spray Map"),
    ggplot(Example2Full.dat, aes(Easting, Northing)) + 
           geom_point(aes(colour = Yield),size=1) + 
           scale_colour_gradient2(low=vermillion, mid=yellow, high=blue, midpoint = median(Example2Full.dat[,'Yield'])) +
           labs(colour = "Yield", x="Easting", y="Northing",title="Harvest Map"),
    nrow=1
  )
)
@
\caption{Spraying and yield monitor harvest maps for response surface analysis of Example 2. We include a larger portion of the cropland than is included in the prior analysis to get better estimates of the model and residual semivariograms.}
\label{Example2FullMaps}
\end{figure}


We can use the yield monitor data from Example 1 directly, specifying a response surface GAM model with variety effects as
\texttt{Yield \textasciitilde Product + s(Easting,Northing, k=*)}, where \texttt{Product} specifies a dummy variable for variety, and the resulting model will provide a coefficient that in our simple two-treatment structure indicates the additive effect of variety "B". The \texttt{s} specifier with two arguments generates a response surface fit in the two dimensions \texttt{Northing} and \texttt{Easting}. This will expand out to a uniformly spaced grid, thus is well suited to fit a response surface to the data from Example 1.

Example 2, in contrast, is not a rectangular shape (Figure \ref{Example2FullMaps}). We will first fit response surface models to the larger data set, including portions of the field the did not receive treatment. However, \texttt{s(Easting,Northing)} will expand out to cover areas of the field not planted. We'll still attempt to fit a response surface to these data, keeping in mind that the fit may be misleading. However, data of the form of Example 2 - not nicely rectangular - are common in collaborative strip trials, so we wish to understand if fitting trend surfaces can be performed on non-ideal data.

\section{Algorithm to determine optimal smoother for response surface analysis}
It is common practice to optimize the degrees of freedom or number of parameters in statistical models using methods such as adjusted $R^2$ or cross-validation methods. While in the proceeding analysis we include adjusted $R^2$ and either $k$-fold cross-validation (from \texttt{lm} models) or the generalized cross validation score provided by \texttt{mgcv}, we also consider a model selection algorithm based on the characteristics of semivariograms obtained from model predictions - the response surface model - or from the residuals of the models. 
The algorithm is as follows:

\begin{enumerate}
  \item Select a range of \texttt{k} values to consider. This is largely done by trial an error. We select a sufficiently large range (i.e. 30-300 for \texttt{mgcv} models and 3-30 for \texttt{lm} models), then examine the semivariograms and adjust \texttt{k} or \texttt{df} values to provide tighter coverage of the critical values. 
  \item Fit response surface models using, for example, \texttt{gam(Yield \textasciitilde Product + c(Easting, Northing,k=*), data=Example1.dat)} and \texttt{lm(Yield \textasciitilde Product + bs(Easting, df=*)*bs(Northing, df=*),data=Example1.dat)}
  \item Generate a predicted response surface, removing \texttt{Product} or \texttt{Sprayed} effects, by predicting with a new data set that includes the original \texttt{Northing} and \texttt{Easting} values, but only one level of \texttt{Product} or \texttt{Sprayed}.
  \item Compute semivariograms for the predict data and for the residuals, for each each level of smoothing (\texttt{k} or \texttt{df}).
  \item For each smoothed data semivariogram, using a linear model basis spline approximation to determine a coefficient for the nugget of the semivariogram. We report in the resulting graphs the $p$-value associated with the nugget. If some of the white noise process (i.e. $e_{ij}$) is retained in the semivariogram of the fitted data, this suggests the fitted model is too wiggly. We wish to choose the largest smoother (\texttt{k} or \texttt{df}) value with an intercept not significantly different from 0.
  \item For each residual semivariogram, compute a simple linear model to determine the slope of the residual semivariogram.  When the residuals contain only white noise information, the slope of the linear model will be near 0. If some spatial correlation information is retained in the residuals, there will be a non-zero slope
\end{enumerate}

\section{Fitting response surface models for Example 1}
<<CreateModelVectors, echo=FALSE>>=
#for gam models, I find that k in the range of 30-300 are reasonable, 
#for lm models, each call to bs in the range of 3-30
SpatialGamKs = (1:10)*30
SpatialLMBs = (1:10)*3

# create lists to hold the results of fitting using gam and lm
Example1GAMModels <- vector(mode='list',length=length(SpatialGamKs))
Example1ModelsLM <- vector(mode='list',length=length(SpatialGamKs))

#We save predicted values from the various smoothers for plotting.
Example1PredictedGAM <- NULL
Example1PredictedLM <- NULL

#We compute variograms independently for both predicted values from the smoothers and for the residuals.
Example1VariogramsGAMPredictions <- vector(length(SpatialGamKs)+1, mode='list')
Example1VariogramsResiduals <- vector(length(SpatialGamKs)+1, mode='list')
Example1VariogramsLMPredictions <- vector(length(SpatialGamKs)+1, mode='list')
Example1VariogramsLMResiduals <- vector(length(SpatialGamKs)+1, mode='list')
@

<<Example1Variogram, include=FALSE, echo=FALSE>>=
#For reference, we compute a variogram for the raw \texttt{Yield} values, and save variograms values for each level of $k$.
#Raw yield will correspond to k=0
tmp.var <- variogram(Yield~1, 
                     locations=~Easting+Northing,
                      data=Example1.dat)
  
Example1PredictionGAMVariogramPars <-  data.frame(Distance=tmp.var[['dist']],
                               Gamma=tmp.var[['gamma']],
                               K=0)
Example1ResidualsGAMVariogramPars <-  Example1PredictionGAMVariogramPars
Example1PredictionLMVariogramPars <-  data.frame(Distance=tmp.var[['dist']],
                               Gamma=tmp.var[['gamma']],
                               K=0)
Example1ResidualsLMVariogramPars <-  Example1PredictionLMVariogramPars

#predict a spatial response surface as if only "E" were planted. This reduces noise between rows of different varieties
NewData <- Example1.dat[, c('Yield','Northing','Easting')]
NewData[,'Product'] <- "E"
@ 

<<ComputeExample1GAMSpatial, include=FALSE, echo=FALSE>>=
if(!file.exists("ComputeExample1GAMSpatial.Rda")) {
  Example1GAMAdjR2 <- 1:(length(SpatialGamKs)+1)
  Example1GAMGCV <- 1:(length(SpatialGamKs)+1)

  Example1NullGAM <- gam(Yield ~ Product, data=Example1.dat[,c('Yield','Product','Northing','Easting')])
  Example1DefaultGAM <- gam(Yield ~ Product+s(Easting, Northing), data=Example1.dat[,c('Yield','Product','Northing','Easting')])

  Example1NullSummary<- summary(Example1NullGAM)
  Example1GAMAdjR2[1] <- Example1NullSummary[['r.sq']]
  Example1GAMGCV[1] <- Example1NullSummary[['sp.criterion']]


  for(i in 1:length(SpatialGamKs)) {
    tmp.dat <- Example1.dat[, c('Yield','Product','Northing','Easting')]
    fmla <- paste('Yield ~ Product + s(Easting, Northing, k=',SpatialGamKs[i],')')
    Example1GAMModels[[i]] <- gam(as.formula(fmla),data=Example1.dat)
  
    gam.summary <- summary(Example1GAMModels[[i]])
    Example1GAMAdjR2[i+1] <- gam.summary[['r.sq']]
    Example1GAMGCV[i+1] <- gam.summary[['sp.criterion']]

    tmp.dat[,'Yield'] <- predict(Example1GAMModels[[i]],newdata=NewData)
    tmp.dat[,'K'] = SpatialGamKs[i]
    tmp.dat[,'Residuals'] <- residuals(Example1GAMModels[[i]])
    
    Example1PredictedGAM <- rbind(Example1PredictedGAM, tmp.dat)
    
    tmp.var <- variogram(Yield~1,  locations=~Easting+Northing,  data=tmp.dat)
    Example1VariogramsGAMPredictions[[i+1]] <- tmp.var

    Example1PredictionGAMVariogramPars <- rbind(Example1PredictionGAMVariogramPars,
                         data.frame(Distance=tmp.var[['dist']],
                                    Gamma=tmp.var[['gamma']],
                                     K=SpatialGamKs[i]))
    tmp.var <- variogram(Residuals~1, locations=~Easting+Northing, data=tmp.dat)
    Example1VariogramsResiduals[[i+1]] <- tmp.var
    Example1ResidualsGAMVariogramPars <- rbind(Example1ResidualsGAMVariogramPars,
                             data.frame(Distance=tmp.var[['dist']],
                                        Gamma=tmp.var[['gamma']],
                                        K=SpatialGamKs[i]))
  }

  save(Example1PredictedGAM,
       Example1PredictionGAMVariogramPars,
       Example1ResidualsGAMVariogramPars,
       Example1GAMAdjR2,
       Example1GAMGCV,
       Example1NullGAM,
       Example1DefaultGAM,
       Example1GAMModels,
       file="ComputeExample1GAMSpatial.Rda")
} else {
  load(file="ComputeExample1GAMSpatial.Rda")
}
@

<<ComputeExample1LMSpatial, include=FALSE, echo=FALSE>>=
if(!file.exists("ComputeExample1LMSpatial.Rda")) {
  Example1LMAdjR2 <- 1:(length(SpatialGamKs)+1)
  Example1KFoldCVLM <- 1:(length(SpatialGamKs)+1)

  #null model has no spatial information
  Example1ColumnSubset = Example1.dat[,c('Yield','Product','Northing','Easting')]
  Example1NullLM <- lm(Yield ~ Product, data=Example1ColumnSubset)
  current.summary <- summary(Example1NullLM)
  Example1LMAdjR2[1] <- current.summary[['adj.r.squared']]

  #predict a spatial response surface as if only "E" were planted. This reduces noise between rows of different varieties
  NewData <- Example1.dat[, c('Yield','Northing','Easting')]
  NewData[,'Product'] <- "E"
  
  #compute k-fold cross validation
  #ordinary LOOCV takes several hours to compute for just one set of models.
  #we'll have k folds
  k=10
  #assign folds to data in groups of 10, at random. We do this in the order the data were collected to
  #be sure that that test data set is uniformly space throughout the field
  foldgroups <- ceiling(dim(Example1ColumnSubset)[1]/k)
  maxidx = dim(Example1ColumnSubset)[1]
  Example1ColumnSubset[,'Fold'] <- 0
  for(j in 1:foldgroups) {
    range = 1:10 + (j-1)*10
    if(j*10>maxidx) {
      range = ((j-1)*10):maxidx
      Example1ColumnSubset[range,'Fold'] <- sample(1:length(range))
    } else {
      Example1ColumnSubset[range,'Fold'] <- sample(1:10)
    }
  }
  #now, hold out fold and fit model to training data 
  sum <- 0
  FoldMSE <- 1:k
  for(j in 1:k) {
    current.train <- Example1ColumnSubset[Example1ColumnSubset[,'Fold'] != j,]
    current.test <- Example1ColumnSubset[Example1ColumnSubset[,'Fold'] == j,]
    current.model <- lm(Yield ~ Product, data=current.train)
    predicted <- predict(current.model, newdata = current.test)
    FoldMSE[j] <- sum(predicted - current.test[,'Yield'])^2/dim(current.test)[1]
  }
  Example1KFoldCVLM[1] <- mean(FoldMSE)

  for(i in 1:length(SpatialGamKs)) {
  
    tmp.dat <- Example1.dat[, c('Yield','Product','Northing','Easting')]
  
    fmla <- paste('Yield ~ Product + bs(Easting, df=',SpatialLMBs[i],')*bs(Northing, df=',SpatialLMBs[i],')')
    Example1ModelsLM[[i]] <- lm(as.formula(fmla), data=tmp.dat)
  
    #compute adjusted R2
    current.summary <- summary(Example1ModelsLM[[i]])
    Example1LMAdjR2 [i+1] <- current.summary[['adj.r.squared']]
    #use the same folds as the null model
    tmp.dat[,'Fold'] <- Example1ColumnSubset[,'Fold']
    sum <- 0
    FoldMSE <- 1:k
    for(j in 1:k) {
      current.train <- tmp.dat[tmp.dat[,'Fold'] != j,]
      current.test <- tmp.dat[tmp.dat[,'Fold'] == j,]
      current.model <- lm(as.formula(fmla), data=current.train)
      predicted <- predict(current.model, newdata = current.test)
      FoldMSE[j] <- sum(predicted - current.test[,'Yield'])^2/dim(current.test)[1]
    }
    Example1KFoldCVLM[i+1] <- mean(FoldMSE)

    tmp.dat[,'Yield'] <- predict(Example1ModelsLM[[i]],newdata=NewData)
    tmp.dat[,'K'] = SpatialLMBs[i]
    tmp.dat[,'Residuals'] <- residuals(Example1ModelsLM[[i]])
    
    Example1PredictedLM <- rbind(Example1PredictedLM, tmp.dat)
    
    tmp.var <- variogram(Yield~1,  locations=~Easting+Northing,  data=tmp.dat)
    Example1VariogramsLMPredictions[[i+1]] <- tmp.var

    Example1PredictionLMVariogramPars <- rbind(Example1PredictionLMVariogramPars,
                           data.frame(Distance=tmp.var[['dist']],
                                      Gamma=tmp.var[['gamma']],
                                       K=SpatialLMBs[i]))
    tmp.var <- variogram(Residuals~1, locations=~Easting+Northing, data=tmp.dat)
    Example1VariogramsLMResiduals[[i+1]] <- tmp.var
    Example1ResidualsLMVariogramPars <- rbind(Example1ResidualsLMVariogramPars,
                               data.frame(Distance=tmp.var[['dist']],
                                          Gamma=tmp.var[['gamma']],
                                          K=SpatialLMBs[i]))
  }
  save(Example1PredictedLM,
       Example1PredictionLMVariogramPars,
       Example1ResidualsLMVariogramPars,
       Example1LMAdjR2,
       Example1KFoldCVLM,
       Example1NullLM,
       Example1ModelsLM,
       file="ComputeExample1LMSpatial.Rda")
} else {
  load(file="ComputeExample1LMSpatial.Rda")
}
@

\begin{figure}\centering
<<Example1LMPredictions, fig=TRUE, width=9, height=6, echo=FALSE>>=
ggplot(Example1PredictedLM, aes(Easting, Northing)) + 
geom_point(aes(colour = Yield),size=1) + 
scale_colour_gradient2(low=vermillion, mid=yellow, high=blue, midpoint = mean(Example1PredictedLM[,'Yield'])) +
labs(colour = "Yield", x="Easting", y="Northing") + facet_wrap(~ K)
@
\caption{Predicted yield response surfaces from linear models with b-splines for Example 1. Predictions are made from the associated \texttt{lm} model at each yield point in the original data, but with the new data including only "E" as variety. This was done to remove effects of variety and provide a visualization of the field response surface ony. Subgraph headers are the \texttt{df} values as arguments to the \texttt{bs} function model fit using \texttt{lm}. The response surfaces represented by these graphs tend to become more coarse as \texttt{df} increases. There is, however, an unexpected surface at \texttt{df=27}, where the passes with variety B are shown with values very close to zero, in contrast with the passes planted with variety E. At \texttt{df=30} we also see that the passes with variety B are distinguishable from the passes planted with variety E, but in this case strips with variety B are estimated to have higher yields than E. }\label{Example1LMPredictions}
\end{figure}

\begin{figure}\centering
<<Example1GAMPredictions, fig=TRUE, width=9, height=6, echo=FALSE>>=
ggplot(Example1PredictedGAM, aes(Easting, Northing)) + 
geom_point(aes(colour = Yield),size=1) + 
scale_colour_gradient2(low=vermillion, mid=yellow, high=blue, midpoint = mean(Example1PredictedGAM[,'Yield'])) +
labs(colour = "Yield", x="Easting", y="Northing") + facet_wrap(~ K)
@
\caption{Predicted yield response surfaces from \texttt{mgcv} model with default bases for Example 1. As with Figure \ref{Example1LMPredictions}, the predictions were made as if only variety E was planted in this field. Subgraph headers are the \texttt{k}  values passed as arguments to the \texttt{s} specifier. As with Figure \ref{Example1LMPredictions} we see that the predicted response surface becomes more coarse as \texttt{k} increases. However, the response surfaces are most consistent. By inspection, we might choose a \texttt{k} around 150-180 to capture most of the detail of the yield response surface for Example 1.}
\label{Example1GAMPredictions}
\end{figure}

Figure \ref{Example1LMPredictions} show the expected response surface of a field planted with only variety "E" based on linear models with b-splines fit to Example 1, with varying degrees of freedom associated with the spline model. At small degrees of freedom, we see little of the field structure observable in Figure \ref{Example1Maps}. As degrees of freedom increases, we see more of the yield structure, with low yield zones apparent at \texttt{df = 9} and  \texttt{df=12}. As \texttt{df} increases we see more detail, included banded structures where harvest passes may be misaligned. However, at \text{df=25} we see an abrupt change in the contours of the response surface, with prominent bands where one variety is planted; these bands grossly underestimate yield. At \texttt{df=30} these bands disappear, but are replaced with less prominent bands of higher than expected predicted yields. This series of response surfaces suggests against using \texttt{lm} with b-splines to generate response surfaces from yield monitor data.

In contrast to Figure \ref{Example1LMPredictions}, Figure \ref{Example1GAMPredictions} shows a smoothly increasing level of detail of the yield structure of the predicted response surfaces. Upon visual inspection, \texttt{k} values of 150-180 provide a optimal degree of smoothing, while still retaining much of the yield structure visible in the raw data (Figure \ref{Example1Maps}). So we now consider the best smoothing models as determined by model selection criteria.

<<Example1SpatialModelSelectionGAM, include=FALSE, echo=FALSE>>=
Example1VariogramSelectionGAM <- data.frame(K=unique(Example1ResidualsGAMVariogramPars[,'K']))
tmp.lm <- lm(Gamma ~ Distance, data=Example1ResidualsGAMVariogramPars[Example1ResidualsGAMVariogramPars[,'K']==0,])
Example1VariogramSelectionGAM[,'Intercept'] <- coef(tmp.lm)[1]
Example1VariogramSelectionGAM[,'Slope'] <- coef(tmp.lm)[2]
sum <- summary(tmp.lm)
Example1VariogramSelectionGAM[,'T'] <- sum[['coefficients']]['Distance','t value']
Example1VariogramSelectionGAM[,'P'] <- sum[['coefficients']]['Distance','Pr(>|t|)']
  
tmp.lm <- lm(Gamma ~ bs(Distance),data=Example1PredictionGAMVariogramPars[Example1PredictionGAMVariogramPars[,'K']==0,])
Example1VariogramSelectionGAM[1,'Nugget'] <- coef(tmp.lm)[1]
sum <- summary(tmp.lm)
Example1VariogramSelectionGAM[1,'NuggetT'] <- sum[['coefficients']][1,'t value']
Example1VariogramSelectionGAM[1,'NuggetP'] <- sum[['coefficients']][1,'Pr(>|t|)']
      
  for (k in 1:length(Example1VariogramSelectionGAM[,'K'])) {
    K <- Example1VariogramSelectionGAM[,'K'][k]
    tmp.lm <- lm(Gamma ~ Distance, data=Example1ResidualsGAMVariogramPars[Example1ResidualsGAMVariogramPars[,'K']==K,])
    Example1VariogramSelectionGAM[k,'Intercept'] <- coef(tmp.lm)[1]
    Example1VariogramSelectionGAM[k,'Slope'] <- coef(tmp.lm)[2]
    sum <- summary(tmp.lm)
    Example1VariogramSelectionGAM[k,'SlopeT'] <- sum[['coefficients']]['Distance','t value']
    Example1VariogramSelectionGAM[k,'SlopeP'] <- sum[['coefficients']]['Distance','Pr(>|t|)']
    tmp.lm <- lm(Gamma ~ bs(Distance),data=Example1PredictionGAMVariogramPars[Example1PredictionGAMVariogramPars[,'K']==K,])
    Example1VariogramSelectionGAM[k,'Nugget'] <- coef(tmp.lm)[1]
    sum <- summary(tmp.lm)
    Example1VariogramSelectionGAM[k,'NuggetT'] <- sum[['coefficients']][1,'t value']
    Example1VariogramSelectionGAM[k,'NuggetP'] <- sum[['coefficients']][1,'Pr(>|t|)']
  }
  
  Example1ModelSelectionGAM <- data.frame(K = rep(c(0,SpatialGamKs),4),
                           #Criteria = as.factor(c(rep('AIC',length(SpatialGamKs)+1),
                           #                        rep('BIC',length(SpatialGamKs)+1),
                            Criteria = as.factor(c(rep('Adjusted R Squared',length(SpatialGamKs)+1),
                                                   rep('Generalized Cross Validation',length(SpatialGamKs)+1),
                                                   rep('Nugget',length(SpatialGamKs)+1),
                                                   rep('Slope',length(SpatialGamKs)+1))),
                            #Score = c(c(AIC(Example1NullGAM),unlist(lapply(Example1GAMModels,AIC))),
                            #          c(BIC(Example1NullGAM),unlist(lapply(Example1GAMModels,BIC))),
                            Score = c(Example1GAMAdjR2,
                                      Example1GAMGCV,
                                      Example1VariogramSelectionGAM[,'NuggetP'],
                                      Example1VariogramSelectionGAM[,'SlopeP'])
                            )
#SelectionGAM  
@

<<Example1SpatialModelSelectionLM, include=FALSE, echo=FALSE>>=
Example1VariogramSelectionLM <- data.frame(K=unique(Example1ResidualsLMVariogramPars[,'K']))
tmp.lm <- lm(Gamma ~ Distance, data=Example1ResidualsLMVariogramPars[Example1ResidualsGAMVariogramPars[,'K']==0,])
Example1VariogramSelectionLM[,'Intercept'] <- coef(tmp.lm)[1]
Example1VariogramSelectionLM[,'Slope'] <- coef(tmp.lm)[2]
sum <- summary(tmp.lm)
Example1VariogramSelectionLM[,'SlopeT'] <- sum[['coefficients']]['Distance','t value']
Example1VariogramSelectionLM[,'SlopeP'] <- sum[['coefficients']]['Distance','Pr(>|t|)']
  
tmp.lm <- lm(Gamma ~ bs(Distance),data=Example1PredictionLMVariogramPars[Example1PredictionLMVariogramPars[,'K']==0,])
Example1VariogramSelectionLM[,'Nugget'] <- coef(tmp.lm)[1]
sum <- summary(tmp.lm)
Example1VariogramSelectionLM[1,'NuggetT'] <- sum[['coefficients']][1,'t value']
Example1VariogramSelectionLM[1,'NuggetP'] <- sum[['coefficients']][1,'Pr(>|t|)']
  
  for (k in 1:length(Example1VariogramSelectionLM[,'K'])) {
    K <- Example1VariogramSelectionLM[,'K'][k]
    tmp.lm <- lm(Gamma ~ Distance, data=Example1ResidualsLMVariogramPars[Example1ResidualsLMVariogramPars[,'K']==K,])
    Example1VariogramSelectionLM[k,'Intercept'] <- coef(tmp.lm)[1]
    Example1VariogramSelectionLM[k,'Slope'] <- coef(tmp.lm)[2]
    sum <- summary(tmp.lm)
    Example1VariogramSelectionLM[k,'SlopeT'] <- sum[['coefficients']]['Distance','t value']
    Example1VariogramSelectionLM[k,'SlopeP'] <- sum[['coefficients']]['Distance','Pr(>|t|)']
    tmp.lm <- lm(Gamma ~ bs(Distance),data=Example1PredictionLMVariogramPars[Example1PredictionLMVariogramPars[,'K']==K,])
    Example1VariogramSelectionLM[k,'Nugget'] <- coef(tmp.lm)[1]
    sum <- summary(tmp.lm)
    Example1VariogramSelectionLM[k,'NuggetT'] <- sum[['coefficients']][1,'t value']
    Example1VariogramSelectionLM[k,'NuggetP'] <- sum[['coefficients']][1,'Pr(>|t|)']
  }
  
  Example1ModelSelectionLM <- data.frame(K = rep(c(0,SpatialLMBs),4),
                            #Criteria = as.factor(c(rep('AIC',length(SpatialGamKs)+1),
                            #                       rep('BIC',length(SpatialGamKs)+1),
                            Criteria = as.factor(c(rep('Adjusted R Squared',length(SpatialGamKs)+1),
                                                   rep('K-Fold Cross Validation',length(SpatialGamKs)+1),
                                                   rep('Nugget p-value',length(SpatialGamKs)+1),
                                                   rep('Slope p-value',length(SpatialGamKs)+1))),
                            #Score = c(c(AIC(nulllm.model),unlist(lapply(Example1ModelsLM,AIC))),
                             #         c(BIC(nulllm.model),unlist(lapply(Example1ModelsLM,BIC))),
                            Score = c(Example1LMAdjR2 ,
                                      Example1KFoldCVLM,
                                      Example1VariogramSelectionLM[,'NuggetP'],
                                      Example1VariogramSelectionLM[,'SlopeP'])
                            )
@

\subsection{Response surface model selection, Example 1}

<<Example1SpatialGAMSummaries. include=FALSE, echo=FALSE, eval=FALSE>>=
summary(Example1NullGAM)
summary(Example1DefaultGAM)
summary(Example1GAMModels[[8]])
summary(Example1GAMModels[[9]])

gam.check(Example1DefaultGAM)
gam.check(Example1GAMModels[[8]])
gam.check(Example1GAMModels[[9]])
@

<<Example1SpatialLMSummaries, eval=FALSE,echo=FALSE, include=FALSE>>=
summary(Example1NullLM)
summary(Example1LMModels[[6]])
summary(Example1LMModels[[7]])
@

\begin{figure}\centering
<<Example1VariogramsLMPlot, fig=TRUE,echo=FALSE, width=8, height=6>>=
Example1PredictionLMVariogramPars$Source <- "Response Surface"
Example1ResidualsLMVariogramPars$Source <- "Residuals"
side.dat <- rbind(Example1PredictionLMVariogramPars, Example1ResidualsLMVariogramPars)
side.dat[,'K'] <- factor(side.dat[,'K'])
side.dat[,'DF'] <- side.dat[,'K']
ggplot(side.dat, aes(Distance,Gamma)) + 
    geom_point(aes(colour = DF),size=2) + 
    geom_smooth(aes(group = DF, color=DF), se=FALSE,alpha=0.5) +
    scale_colour_manual(values=c(cbPalette, cbPalette)) + 
    labs(title=paste('Variograms, LM, Example 1'), x ="Distance (m)", y = "Covariance") +
    facet_wrap(~ Source, scales = "free")
@
\caption{Semivariograms for linear regression basis spline response surface models fit to data from Example 1. First, consider the variogram associated with residuals, on the left side of this figure. The gray line represents the semivariogram of the original data. This is not an ideal semivariogram in that there is no clear sill, but this is common for semivariograms fit to yield data. At \texttt{k=3} we see a more ideal semivariogram, with a clear sill. However, the left graph is a variogram of the residuals, and if we have fit enough enough detail in the model, then residuals will not have a classical variogram shape. By inspection, we might choose \texttt{df=15} as a model that captures most of the response surface and results in residuals with no clear spatial correlation. Turning our attention to the right side, we see that the semivariogram at \texttt{k=3} increases, but does not form an asymptote as we might expect. The sill of the semivariogram of the model increases with \texttt{k}, and a nugget effect appears at about \texttt{k=15}. By inspection of this graph, then, we might choose a \texttt{bs} model with \texttt{k=15}. Finally, we should note the unusual pattern of the model semivariogram at \texttt{k=27}. Remember from Figure \ref{Example1LMPredictions} that the model fit with \texttt{k=27} predicted very low values for variety B, in conflict with other models. }
\label{Example1VariogramsLMPlot}
\end{figure}

Consider Figure \ref{Example1VariogramsLMPlot}. We see with \texttt{df=3} much of the semivariogram structure is retained in the residuals; the curve follows a clear upward trend with distance. It's not until we reach \texttt{df=27} were we see a residual semivariogram that retains none of the idealized semivariogram structure and instead covariance is independent of distance. However, in the predicted response surface variagram we see an unrealistic semivariogram model for \texttt{df=27}. This is evidence that \texttt{lm} models are not suitable for fitting response surfaces for yield data.

\begin{figure}\centering
<<Example1VariogramsGAMPlot, fig=TRUE,echo=FALSE, width=8, height=6>>=
Example1PredictionGAMVariogramPars$Source <- "Response Surface"
Example1ResidualsGAMVariogramPars$Source <- "Residuals"
side.dat <- rbind(Example1PredictionGAMVariogramPars, Example1ResidualsGAMVariogramPars)
side.dat[,'K'] <- factor(side.dat[,'K'])

ggplot(side.dat, aes(Distance,Gamma)) + 
    geom_point(aes(colour = K),size=2) + 
    geom_smooth(aes(group = K, color=K), se=FALSE,alpha=0.5) +
    scale_colour_manual(values=c(cbPalette, cbPalette)) + 
    labs(title=paste('Variograms, GAM, Model 1'), x ="Distance (m)", y = "Covariance") +
    facet_wrap(~ Source, scales = "free")
@
\caption{Variograms for GAM basis spline models fit to data from Example 1. As with Figure \ref{Example1VariogramsLMPlot}, the left subgraph shows the variograms associated with residuals, while the right subgraph shows the semivariograms associated with the response surface. In both, the upper gray curve is the semivariogram from the raw data. As we consider the residuals semivariograms, we see that at larger values of \texttt{k} the semivariogram is a straight line, suggesting no spatial correlations among the residuals for these models. As \texttt{k} increases we see the residuals semivariogram approaches the raw data semivariogram at small values of \texttt{k}. By inspection of this subgraph, we might choose a model with \texttt{k=150} as ideal. In the right subgraph, we see the response surface semivariogram approaches that of the raw data. By inspection, we might choose \texttt{k=90} as a model that captures most of the response surface pattern without overfitting and creating nugget effect in the model.}\label{Example1VariogramsGAMPlot}
\end{figure}


Figure \ref{Example1VariogramsGAMPlot} plots the semivariograms computed from \texttt{mgcv} fitted response surfaces that their residuals. The curve associated with \texttt{k}=0 is the semivariogram computed from the raw data. This curve follows a typical pattern for semivariograms - there is a large nugget effect at a distance of 0, and the variance between points increases steadily as distance between points increases. This is not, however, an ideal semivariogram as there is no clear sill; the semivariogram does not show an asymptote. If we consider the residual semivariograms, we see that the residuals from small \texttt{k} fitted models retain some of the semivariogram structure. This suggests that these models are under-smoothed, and that some of the variance structure of the random field fit using \texttt{s} is not being fit by the model. It is only a large values of \texttt{k} do we see a straight line of variances, suggesting the residuals of these models represent a white-noise process, such that there is no correlation between the distance between points and the variances of the residuals of those points. We might prefer a large \texttt{k} (say, 210) by inspection of the residual semivariograms. In the response surface semivariograms, at small \texttt{k} we see a more idealized semivariogram which shows no nugget and a clear sill approaching a variance of about 500. The lack of a nugget suggests that the response surface contains little white noise, but the sill at \texttt{k=30} is small than the sills of models fit with larger \texttt{k}, suggesting that 30 is too small to capture the entire spatial structure of the data. \texttt{k} values greater than 180 add little to the sill, but start to show a nugget effect, suggesting that the response surface associated with these values contains some of the white noise process we would like to attribute to residuals. By inspection of this graph, we might choose a \texttt{k=180}.

\begin{figure}\centering
<<Example1ModelSelectionLM, fig=TRUE,echo=FALSE, width=8, height=6>>=
ggplot(Example1ModelSelectionLM, aes(K,Score)) + 
    geom_point(aes(colour = Criteria),size=2) + 
    geom_line(aes(colour = Criteria),size=1) + 
    scale_colour_manual(values=cbPalette) + facet_wrap(~Criteria, nrow=3, scales="free_y") +
    labs(title="Scores for LM smoothers", x ="Smoothing Parameter", y = "Score")
@
\caption{Selection criteria for response surfaces fit as in Figure \ref{Example1VariogramsLMPlot}. We see model adjusted $R^2$ in the upper left subgraph. Adjusted $R^2$ increases with increasing model complexity, but the increase lessens as complexity increases. We would have a hard time choosing an optimal model d.f. from adjusted $R^2$. In the upper right graph, we see that $k$-fold cross validation of the models decreases with model d.f., and from this subgraph we might choose \texttt{k=18} as an optimum. From the lower left subgraph, we might choose \texttt{k=9} as the most detailed model which has no significant nugget effect, while from the lower right subgraph we might choose \texttt{k=18} as a model where the residuals have no spatial correlation.}\label{Example1ModelSelectionLM}
\end{figure}

\begin{figure}\centering
<<Example1ModelSelectionGAM, fig=TRUE,echo=FALSE, width=8, height=6>>=
ggplot(Example1ModelSelectionGAM, aes(K,Score)) + 
    geom_point(aes(colour = Criteria),size=2) + 
    geom_line(aes(colour = Criteria),size=1) + 
    scale_colour_manual(values=cbPalette) + facet_wrap(~Criteria, nrow=3, scales="free_y") +
    labs(title="Scores for GAM smoothers", x ="Smoothing Parameter", y = "Score")
    
@
\caption{Selection criteria for response surfaces fit as in Figure \ref{Example1VariogramsGAMPlot}. In the upper left subgraph, we see that adjusted $R^2$ increases dramatically with small \texttt{k}, then increases steadily with increasing model complexity. Using the "elbow" method, we might prefer a model with \texttt{k=90}. The upper right subgraph is nearly a mirror image of the upper left subgraph; here we see that the generalized cross validation score decreases with model complexity, and application of the "elbow" method would similarly lead us to choose a model with \texttt{k=90}. The lower left sub graph suggests a model of \texttt{k=120} as the most complex model without a significant nugget effect, while the lower right subgraph suggests that models with \texttt{k>30} retain some spatial correlation in the residuals.}\label{Example1ModelSelectionGAM}
\end{figure}

Figures \ref{Example1ModelSelectionLM} and \ref{Example1ModelSelectionGAM} show model selection criteria for response surfaces using \texttt{mgcv} and \texttt{lm} fitting b-splines. With \texttt{mcgv} models, adjusted $R^2$ increases asymptotically as the degree of smoothing \texttt{k}) increases, while the generalized cross-validation score decreases smoothly. The "elbow method" applied to adjusted $R^2$ suggests an optimal \texttt{k=60}, while the same method applied to GCV scores suggest a similar optimum \texttt{k}. 
The lower left corner plot of Figure \ref{Example1ModelSelectionGAM} shows the $p$ values associated with the nugget estimate of the response surface semivariogram. The $p$-value at 0 is significant, since this is the semivariogram of the raw data and this semivariogram includes the white noise process as part of the nugget. This $p$-value increases to non-significance at \texttt{k}=30,60,90 and 120, and becomes significant only at \texttt{k}=120. This suggests that the nugget effect is effectively zero for \texttt{k} up to 120. Based on the semivariogram, we might prefer a more wiggly model (\texttt{k=120}) that we would choose based on adjusted $R^2$ or generalized cross-validation. In contrasts, the slope of the residuals semivariogram is non-significantly difference from zero only at \texttt{k=30}. This suggests an optimal \texttt{k} based on fitting the residual semivariogram suggests a smaller smoothing parameter than we might chose based on inspection of the semivariograms.

\subsection{Variety coefficients, Example 1}

<<Example1ExtractCoefficents, include=FALSE, echo=FALSE>>=
Coef <- c()
Source <- c()
K <- c()
UB <- c()
LB <- c()

totaldf <- dim(Example1.dat)[1]
for(i in 1:length(SpatialGamKs)) {
  coefstbl <- summary(Example1GAMModels[[i]])[['p.table']]
  dftbl <- summary(Example1GAMModels[[i]])[['s.table']]
  crit.t <- qt(1-0.025, totaldf - dftbl[1,2])
  UB <- c(UB, coefstbl[2,1]+crit.t*coefstbl[2,2])
  LB <- c(LB, coefstbl[2,1]-crit.t*coefstbl[2,2])
  Coef <- c(Coef, coefstbl[2,1])
  K <- c(K,SpatialGamKs[i])
  Source <- c(Source, "MGCV")
  
  #coefs <- coef(Example1ModelsLM[[i]])
  coefstbl <- summary(Example1ModelsLM[[i]])[['coefficients']]
  dftbl <- summary(Example1ModelsLM[[i]])[['df']]
  crit.t <- qt(1-0.025, dftbl[2] - dftbl[1]+1)
  Coef <- c(Coef, coefstbl[1,1])
  UB <- c(UB, coefstbl[1,1]+crit.t*coefstbl[1,2])
  LB <- c(LB, coefstbl[1,1]-crit.t*coefstbl[1,2])
  Source <- c(Source, "LM")
  K <- c(K,SpatialLMBs[i])
}
Plot1 <- data.frame(Coef=Coef, Source=Source,K=K,UB=UB,LB=LB)
@

\begin{figure}\centering
<<Example1Coefficents, fig=TRUE,echo=FALSE, width=8, height=6>>=
grid.arrange(
  arrangeGrob(
  ggplot(Plot1[Plot1['Source'] == "LM",], aes(K,Coef)) + 
  geom_ribbon(aes(ymin = LB, ymax = UB),col=cbPalette[1],fill=cbPalette[2]) +
      geom_point(size=2) + 
      geom_line(linewidth=1) + 
      scale_colour_manual(values=cbPalette) +
      labs(title="LM Coefficients, Example 1", x ="Smoothing Parameter", y = "Product Effect Estimate"),
 ggplot(Plot1[Plot1['Source'] == "MGCV",], aes(K,Coef)) + 
     geom_ribbon(aes(ymin = LB, ymax = UB),col=cbPalette[1],fill=cbPalette[2]) +
         geom_point(size=2) + 
         geom_line(linewidth=1) + 
         scale_colour_manual(values=cbPalette) + 
         labs(title="MGCV Coefficients, Example 1", x ="Smoothing Parameter", y = "Product Effect Estimate"),
    nrow=2
  )
)
@
\caption{Coefficients from \texttt{mcgv} and \texttt{lm} fitting b-splines for spatial models fit to Example 1. The curve on the upper subgraph is unreadable, because of the unstable behavior of \texttt{lm} fits at large d.f. However, even at smaller \texttt{df} values the coefficients associated with variety B are unrealistic, on the order of 200 bu/acre. In contrast, in the lower subgraph, the variety B effect estimated using \texttt{mgcv} trend surface models is relatively stable, on the order of 12-13 bu/acre.}
\label{Example1Coefficents}
\end{figure}

The purpose of fitting response surfaces is to provide an estimate of treatment effect in the presence of a varying spatial field. Figure \ref{Example1Coefficents} show the coefficients associated with variety B (that is, the additive effect of variety B relative to variety E). 
For \texttt{mgcv} fit models, estimates range from just under 12 bu/acre to nearly 14 bu/acre.  These values compare well with the value obtained from trend analysis of combined strip yield means (Chapter \ref{TrendAnalysisChapter} - 12.16 bu/acre) and the with pointwise trend coefficient estimates (Figure \ref{Example1PointwiseBCoefficients}). Based on adjusted $R^2$ and GCV, we might prefer the model with \texttt{k=60}, which corresponds to an estimate of just under 12 bu/acre, while model selection by inspection of Figure \ref{Example1VariogramsGAMPlot} might lead us to prefer a model that produces a coefficient of almost exactly 12.5 bu/acre. while basing model selection on the $p$-value of the nugget would lead us to a slightly smaller coefficient. In any of these cases, the coefficient associated with variety B is statistically significant.

In general, the coefficients from the b-spline fits using \texttt{lm} are similar at small values of smoothing parameters, but jump dramatically with larger degrees of freedom, with a maximum of 120 bu/acre corresponding to a \texttt{df=27}, while the coefficient associated with \texttt{df=30} is negative. This is further evidence that b-splines fit with \texttt{lm} is not suitable for these data. As we move to analyze Example 2, we will only use \texttt{mgcv} to fit response surfaces.

\section{Fitting response surface models for Example 2}

<<Example2FullCreateModelVectors, echo=FALSE>>=
# create lists to hold the results of fitting using gam and lm
Example2FullGAMModels <- vector(mode='list',length=length(SpatialGamKs))
#Example2FullModelsLM <- vector(mode='list',length=length(SpatialGamKs))

#We save predicted values from the various smoothers for plotting.
Example2FullPredictedGAM <- NULL
#Example2FullPredictedLM <- NULL

#We compute variograms independently for both predicted values from the smoothers and for the residuals.
Example2FullVariogramsGAMPredictions <- vector(length(SpatialGamKs)+1, mode='list')
Example2FullVariogramsResiduals <- vector(length(SpatialGamKs)+1, mode='list')
#Example2FullVariogramsLMPredictions <- vector(length(SpatialGamKs)+1, mode='list')
#Example2FullVariogramsLMResiduals <- vector(length(SpatialGamKs)+1, mode='list')
@

<<Example2FullVariogram, include=FALSE, echo=FALSE>>=
#For reference, we compute a variogram for the raw \texttt{Yield} values, and save variograms values for each level of $k$.
#Raw yield will correspond to k=0
tmp.var <- variogram(Yield~1, 
                     locations=~Easting+Northing,
                      data=Example2Full.dat)
  
Example2FullPredictionGAMVariogramPars <-  data.frame(Distance=tmp.var[['dist']],
                               Gamma=tmp.var[['gamma']],
                               K=0)
Example2FullResidualsGAMVariogramPars <-  Example2FullPredictionGAMVariogramPars
Example2FullPredictionLMVariogramPars <-  data.frame(Distance=tmp.var[['dist']],
                               Gamma=tmp.var[['gamma']],
                               K=0)
Example2FullResidualsLMVariogramPars <-  Example2FullPredictionLMVariogramPars

NewData <- Example2Full.dat[, c('Yield','Northing','Easting')]
NewData[,'Sprayed'] <- FALSE
@

<<ComputeExample2FullGAMSpatial, include=FALSE, echo=FALSE>>=
if(!file.exists("ComputeExample2FullGAMSpatial.Rda")) {
  Example2FullGAMAdjR2 <- 1:(length(SpatialGamKs)+1)
  Example2FullGAMGCV <- 1:(length(SpatialGamKs)+1)

  Example2FullNullGAM <- gam(Yield ~ Sprayed, data=Example2Full.dat[,c('Yield','Sprayed','Northing','Easting')])
  Example2FullDefaultGAM <- gam(Yield ~ Sprayed+s(Easting, Northing), data=Example2Full.dat[,c('Yield','Sprayed','Northing','Easting')])

  Example2FullNullSummary<- summary(Example2FullNullGAM)
  Example2FullGAMAdjR2[1] <- Example2FullNullSummary[['r.sq']]
  Example2FullGAMGCV[1] <- Example2FullNullSummary[['sp.criterion']]

  for(i in 1:length(SpatialGamKs)) {
    tmp.dat <- Example2Full.dat[, c('Yield','Sprayed','Northing','Easting')]
    fmla <- paste('Yield ~ Sprayed + s(Easting, Northing, k=',SpatialGamKs[i],')')
    Example2FullGAMModels[[i]] <- gam(as.formula(fmla),data=tmp.dat)
  
    gam.summary <- summary(Example2FullGAMModels[[i]])
    Example2FullGAMAdjR2[i+1] <- gam.summary[['r.sq']]
    Example2FullGAMGCV[i+1] <- gam.summary[['sp.criterion']]

    tmp.dat[,'Yield'] <- predict(Example2FullGAMModels[[i]],newdata=NewData)
    tmp.dat[,'K'] = SpatialGamKs[i]
    tmp.dat[,'Residuals'] <- residuals(Example2FullGAMModels[[i]])
    
    Example2FullPredictedGAM <- rbind(Example2FullPredictedGAM, tmp.dat)
    
    tmp.var <- variogram(Yield~1,  locations=~Easting+Northing,  data=tmp.dat)
    Example2FullVariogramsGAMPredictions[[i+1]] <- tmp.var

    Example2FullPredictionGAMVariogramPars <- rbind(Example2FullPredictionGAMVariogramPars,
                           data.frame(Distance=tmp.var[['dist']],
                                      Gamma=tmp.var[['gamma']],
                                       K=SpatialGamKs[i]))
    tmp.var <- variogram(Residuals~1, locations=~Easting+Northing, data=tmp.dat)
    Example2FullVariogramsResiduals[[i+1]] <- tmp.var
    Example2FullResidualsGAMVariogramPars <- rbind(Example2FullResidualsGAMVariogramPars,
                               data.frame(Distance=tmp.var[['dist']],
                                          Gamma=tmp.var[['gamma']],
                                          K=SpatialGamKs[i]))
  }

  save(Example2FullPredictedGAM,
       Example2FullPredictionGAMVariogramPars,
       Example2FullResidualsGAMVariogramPars,
       Example2FullGAMAdjR2,
       Example2FullGAMGCV,
       Example2FullNullGAM,
       Example2FullDefaultGAM,
       Example2FullGAMModels,
       file="ComputeExample2FullGAMSpatial.Rda")
} else {
  load(file="ComputeExample2FullGAMSpatial.Rda")
}
@

%<<ComputeExample2FullLMSpatial,include=FALSE, echo=FALSE>>=
%if(!file.exists("ComputeExample2FullLMSpatial.Rda")) {
%  Example2FullLMAdjR2 <- 1:(length(SpatialGamKs)+1)
%  Example2FullKFoldCVLM <- 1:(length(SpatialGamKs)+1)

%  #null model has no spatial information
%  Example2FullColumnSubset = Example2Full.dat[,c('Yield','Sprayed','Northing','Easting')]
%  Example2FullNullLM <- lm(Yield ~ Sprayed,data=Example2FullColumnSubset)
%  current.summary <- summary(Example2FullNullLM)
%  Example2FullLMAdjR2[1] <- current.summary[['adj.r.squared']]

%  #compute k-fold cross validation
%  #ordinary LOOCV takes several hours to compute for just one set of models.
%  #we'll have k folds
%  k=10
%  #assign folds to data in groups of 10, at random. We do this in the order the data were collected to
%  #be sure that that test data set is uniformly space throughout the field
%  foldgroups <- ceiling(dim(Example2FullColumnSubset)[1]/k)
%  maxidx = dim(Example2FullColumnSubset)[1]
%  Example2FullColumnSubset[,'Fold'] <- 0
%  for(j in 1:foldgroups) {
%    range = 1:10 + (j-1)*10
%    if(j*10>maxidx) {
%      range = ((j-1)*10):maxidx
%      Example2FullColumnSubset[,'Fold'][range] <- sample(1:length(range))
%    } else {
%      Example2FullColumnSubset[,'Fold'][range] <- sample(1:10)
%    }
%  }
%  #now, hold out fold and fit model to training data
%  sum <- 0
%  FoldMSE <- 1:k
%  for(j in 1:k) {
%    current.train <- Example2FullColumnSubset[Example2FullColumnSubset[,'Fold'] != j,]
%    current.test <- Example2FullColumnSubset[Example2FullColumnSubset[,'Fold'] == j,]
%    current.model <- lm(Yield ~ Sprayed, data=current.train)
%    predicted <- predict(current.model, newdata = current.test)
%    FoldMSE[j] <- sum(predicted - current.test[,'Yield'])^2/dim(current.test)[1]
%  }
%  Example2FullKFoldCVLM[1] <- mean(FoldMSE)

%  for(i in 1:length(SpatialGamKs)) {
  
%    tmp.dat <- Example2Full.dat[, c('Yield','Sprayed','Northing','Easting')]
  
%    fmla <- paste('Yield ~ Sprayed + bs(Easting,df=',SpatialLMBs[i],')*bs(Northing,df=',SpatialLMBs[i],')')
%    Example2FullModelsLM[[i]] <- lm(as.formula(fmla), data=tmp.dat)
  
%    #compute adjusted R2
%    current.summary <- summary(Example2FullModelsLM[[i]],newdata=NewData)
%    Example2FullLMAdjR2 [i+1] <- current.summary[['adj.r.squared']]
%    #use the same folds as the null model
%    tmp.dat[,'Fold'] <- Example2FullColumnSubset[,'Fold']
%    sum <- 0
%    FoldMSE <- 1:k
%    for(j in 1:k) {
%      current.train <- tmp.dat[tmp.dat[,'Fold'] != j,]
%      current.test <- tmp.dat[tmp.dat[,'Fold'] == j,]
%      current.model <- lm(as.formula(fmla), data=current.train)
%      predicted <- predict(current.model, newdata = current.test)
%      FoldMSE[j] <- sum(predicted - current.test[,'Yield'])^2/dim(current.test)[1]
%    }
%    Example2FullKFoldCVLM[i+1] <- mean(FoldMSE)

%    tmp.dat[,'Yield'] <- predict(Example2FullModelsLM[[i]])
%    tmp.dat[,'K'] = SpatialLMBs[i]
%    tmp.dat[,'Residuals'] <- residuals(Example2FullModelsLM[[i]])
    
%    Example2FullPredictedLM <- rbind(Example2FullPredictedLM, tmp.dat)
    
%    tmp.var <- variogram(Yield~1,  locations=~Easting+Northing,  data=tmp.dat)
%    Example2FullVariogramsLMPredictions[[i+1]] <- tmp.var

%    Example2FullPredictionLMVariogramPars <- rbind(Example2FullPredictionLMVariogramPars,
%                           data.frame(Distance=tmp.var[['dist']],
%                                      Gamma=tmp.var[['gamma']],
%                                       K=SpatialLMBs[i]))
%    tmp.var <- variogram(Residuals~1, locations=~Easting+Northing, data=tmp.dat)
%    Example2FullVariogramsLMResiduals[[i+1]] <- tmp.var
%    Example2FullResidualsLMVariogramPars <- rbind(Example2FullResidualsLMVariogramPars,
%                               data.frame(Distance=tmp.var[['dist']],
%                                          Gamma=tmp.var[['gamma']],
%                                          K=SpatialLMBs[i]))
%  }
%  save(Example2FullPredictedLM,
%       Example2FullPredictionLMVariogramPars,
%       Example2FullResidualsLMVariogramPars,
%       Example2FullLMAdjR2,
%       Example2FullKFoldCVLM,
%       Example2FullNullLM,
%       Example2FullModelsLM,
%       file="ComputeExample2FullLMSpatial.Rda")
%} else {
%  load(file="ComputeExample2FullLMSpatial.Rda")
%}
%@

%\begin{figure}\centering
%<<Example2FullLMPredictions,fig=TRUE, width=9, height=6,include=FALSE, echo=FALSE>>=
%ggplot(Example2FullPredictedLM, aes(Easting,Northing)) + 
%geom_point(aes(colour = Yield),size=1) + 
%scale_colour_gradient2(low=vermillion, mid=yellow, high=blue,midpoint = mean(Example2FullPredictedLM[,'Yield'])) +
%labs(colour = "Yield", x="Easting", y="Northing") + facet_wrap(~ K)
%@
%\caption{Predicted yield response surfaces from linear model with b-splines for Example 2}\label{Example2FullLMPredictions}
%\end{figure}

\begin{figure}\centering
<<Example2FullGAMPredictions, fig=TRUE, width=9, height=6, include=FALSE, echo=FALSE>>=
ggplot(Example2FullPredictedGAM, aes(Easting, Northing)) + 
geom_point(aes(colour = Yield),size=1) + 
scale_colour_gradient2(low=vermillion, mid=yellow, high=blue, midpoint = mean(Example2FullPredictedGAM[,'Yield'])) +
labs(colour = "Yield", x="Easting", y="Northing") + facet_wrap(~ K)
@
\caption{Predicted yield response surfaces from \texttt{mgcv} model with default bases for Example 2. As with the response surface graphs for Example 1, we see increasing coarseness with model complexity. By inspection, we might choose a model with \texttt{k=150} as reasonably complex but not overfitting the data.}
\label{Example2FullGAMPredictions}
\end{figure}

Response surface estimates for the full data set for Example 2 is shown in Figure \ref{Example2FullGAMPredictions}. As with Figure \ref{Example1GAMPredictions}, we see increasing detail in the predicted response surface yield structure. By inspection, models with \texttt{k=90} or \texttt{k=180} appears optimal, as described in the figure caption.


<<Example2FullSpatialModelSelectionGAM, include=FALSE, echo=FALSE>>=
Example2FullVariogramSelectionGAM <- data.frame(K=unique(Example2FullResidualsGAMVariogramPars[,'K']))
tmp.lm <- lm(Gamma ~ Distance, data=Example2FullResidualsGAMVariogramPars[Example2FullResidualsGAMVariogramPars[,'K']==0,])
Example2FullVariogramSelectionGAM[,'Intercept'] <- coef(tmp.lm)[1]
Example2FullVariogramSelectionGAM[,'Slope'] <- coef(tmp.lm)[2]
sum <- summary(tmp.lm)
Example2FullVariogramSelectionGAM[,'T'] <- sum[['coefficients']]['Distance','t value']
Example2FullVariogramSelectionGAM[,'P'] <- sum[['coefficients']]['Distance','Pr(>|t|)']
  
tmp.lm <- lm(Gamma ~ bs(Distance),data=Example2FullPredictionGAMVariogramPars[Example2FullPredictionGAMVariogramPars[,'K']==0,])
Example2FullVariogramSelectionGAM[1,'Nugget'] <- coef(tmp.lm)[1]
sum <- summary(tmp.lm)
Example2FullVariogramSelectionGAM[1,'NuggetT'] <- sum[['coefficients']][1,'t value']
Example2FullVariogramSelectionGAM[1,'NuggetP'] <- sum[['coefficients']][1,'Pr(>|t|)']
      
  for (k in 1:length(Example2FullVariogramSelectionGAM[,'K'])) {
    K <- Example2FullVariogramSelectionGAM[,'K'][k]
    tmp.lm <- lm(Gamma ~ Distance, data=Example2FullResidualsGAMVariogramPars[Example2FullResidualsGAMVariogramPars[,'K']==K,])
    Example2FullVariogramSelectionGAM[k,'Intercept'] <- coef(tmp.lm)[1]
    Example2FullVariogramSelectionGAM[k,'Slope'] <- coef(tmp.lm)[2]
    sum <- summary(tmp.lm)
    Example2FullVariogramSelectionGAM[k,'SlopeT'] <- sum[['coefficients']]['Distance','t value']
    Example2FullVariogramSelectionGAM[k,'SlopeP'] <- sum[['coefficients']]['Distance','Pr(>|t|)']
    tmp.lm <- lm(Gamma ~ bs(Distance),data=Example2FullPredictionGAMVariogramPars[Example2FullPredictionGAMVariogramPars[,'K']==K,])
    Example2FullVariogramSelectionGAM[k,'Nugget'] <- coef(tmp.lm)[1]
    sum <- summary(tmp.lm)
    Example2FullVariogramSelectionGAM[k,'NuggetT'] <- sum[['coefficients']][1,'t value']
    Example2FullVariogramSelectionGAM[k,'NuggetP'] <- sum[['coefficients']][1,'Pr(>|t|)']
  }
  
  Example2FullModelSelectionGAM <- data.frame(K = rep(c(0,SpatialGamKs),4),
                           #Criteria = as.factor(c(rep('AIC',length(SpatialGamKs)+1),
                           #                        rep('BIC',length(SpatialGamKs)+1),
                            Criteria = as.factor(c(rep('Adjusted R Squared',length(SpatialGamKs)+1),
                                                   rep('Generalized Cross Validation',length(SpatialGamKs)+1),
                                                   rep('Nugget',length(SpatialGamKs)+1),
                                                   rep('Slope',length(SpatialGamKs)+1))),
                            #Score = c(c(AIC(Example2FullNullGAM),unlist(lapply(Example2FullGAMModels,AIC))),
                            #          c(BIC(Example2FullNullGAM),unlist(lapply(Example2FullGAMModels,BIC))),
                            Score = c(Example2FullGAMAdjR2,
                                      Example2FullGAMGCV,
                                      Example2FullVariogramSelectionGAM[,'NuggetP'],
                                      Example2FullVariogramSelectionGAM[,'SlopeP'])
                            )
#SelectionGAM  
@

%<<Example2FullSpatialModelSelectionLM, include=FALSE, echo=FALSE>>=
%Example2FullVariogramSelectionLM <- data.frame(K=unique(Example2FullResidualsLMVariogramPars[,'K']))
%tmp.lm <- lm(Gamma ~ Distance,data=Example2FullResidualsLMVariogramPars[Example2FullResidualsGAMVariogramPars[,'K']==0,])
%Example2FullVariogramSelectionLM[,'Intercept'] <- coef(tmp.lm)[1]
%Example2FullVariogramSelectionLM[,'Slope'] <- coef(tmp.lm)[2]
%sum <- summary(tmp.lm)
%Example2FullVariogramSelectionLM[,'SlopeT'] <- sum[['coefficients']]['Distance','t value']
%Example2FullVariogramSelectionLM[,'SlopeP'] <- sum[['coefficients']]['Distance','Pr(>|t|)']
  
%tmp.lm <- lm(Gamma ~ bs(Distance),data=Example2FullPredictionLMVariogramPars[Example2FullPredictionLMVariogramPars[,'K']==0,])
%Example2FullVariogramSelectionLM[,'Nugget'] <- coef(tmp.lm)[1]
%sum <- summary(tmp.lm)
%Example2FullVariogramSelectionLM[,'NuggetT'][1] <- sum[['coefficients']][1,'t value']
%Example2FullVariogramSelectionLM[,'NuggetP'][1] <- sum[['coefficients']][1,'Pr(>|t|)']
  
%  for (k in 1:length(Example2FullVariogramSelectionLM[,'K'])) {
%    K <- Example2FullVariogramSelectionLM[,'K'][k]
%    tmp.lm <- lm(Gamma ~ Distance,data=Example2FullResidualsLMVariogramPars[Example2FullResidualsLMVariogramPars[,'K']==K,])
%    Example2FullVariogramSelectionLM[k,'Intercept'] <- coef(tmp.lm)[1]
%    Example2FullVariogramSelectionLM[k,'Slope'] <- coef(tmp.lm)[2]
%    sum <- summary(tmp.lm)
%    Example2FullVariogramSelectionLM[k,'SlopeT'] <- sum[['coefficients']]['Distance','t value']
%    Example2FullVariogramSelectionLM[k,'SlopeP'] <- sum[['coefficients']]['Distance','Pr(>|t|)']
%    tmp.lm <- lm(Gamma ~ bs(Distance),data=Example2FullPredictionLMVariogramPars[Example2FullPredictionLMVariogramPars[,'K']==K,])
%    Example2FullVariogramSelectionLM[k,'Nugget'] <- coef(tmp.lm)[1]
%    sum <- summary(tmp.lm)
%    Example2FullVariogramSelectionLM[k,'NuggetT'] <- sum[['coefficients']][1,'t value']
%    Example2FullVariogramSelectionLM[k,'NuggetP'] <- sum[['coefficients']][1,'Pr(>|t|)']
%  }
  
%  Example2FullModelSelectionLM <- data.frame(K = rep(c(0,SpatialLMBs),4),
%                            #Criteria = as.factor(c(rep('AIC',length(SpatialGamKs)+1),
%                            #                       rep('BIC',length(SpatialGamKs)+1),
%                            Criteria = as.factor(c(rep('Adjusted R Squared',length(SpatialGamKs)+1),
%                                                   rep('K-Fold Cross Validation',length(SpatialGamKs)+1),
%                                                   rep('Nugget p-value',length(SpatialGamKs)+1),
%                                                   rep('Slope p-value',length(SpatialGamKs)+1))),
%                            #Score = c(c(AIC(nulllm.model),unlist(lapply(Example2FullModelsLM,AIC))),
%                             #         c(BIC(nulllm.model),unlist(lapply(Example2FullModelsLM,BIC))),
%                            Score = c(Example2FullLMAdjR2 ,
%                                      Example2FullKFoldCVLM,
%                                      Example2FullVariogramSelectionLM[,'NuggetP'],
%                                      Example2FullVariogramSelectionLM[,'SlopeP'])
%                            )
%@

\begin{figure}\centering
<<Example2FullVariogramsGAMPlot, fig=TRUE,echo=FALSE, width=8, height=6>>=
Example2FullPredictionGAMVariogramPars$Source <- "Response Surface"
Example2FullResidualsGAMVariogramPars$Source <- "Residuals"
side.dat <- rbind(Example2FullPredictionGAMVariogramPars,Example2FullResidualsGAMVariogramPars)
side.dat[,'K'] <- factor(side.dat[,'K'])

ggplot(side.dat, aes(Distance,Gamma)) + 
    geom_point(aes(colour = K),size=2) + 
    geom_smooth(aes(group = K, color=K), se=FALSE,alpha=0.5) +
    scale_colour_manual(values=c(cbPalette, cbPalette)) + 
    labs(title=paste('Variograms, GAM, Model 2'), x ="Distance (m)", y = "Covariance") +
    facet_wrap(~ Source, scales = "free")
@
\caption{Variograms for \texttt{mgcv} response surface models fit to data from Example 2, Figure \ref{Example2FullMaps}. As with Figure \ref{Example1VariogramsLMPlot}, we see the variograms for the raw data as the upper curve in both subgraphs. In the left subgraph, we see that models with \texttt{k<210}, the residuals retain a considerable amount of spatial correlation. In contrast, by inspection of the right subgraph we might choose a model with \texttt{k=90} as the model that captures the maximum amount of spatial correlation without introducing a meaningful nugget effect.}
\label{Example2FullVariogramsGAMPlot}
\end{figure}

Semivariograms for \texttt{mgcv} models for Example 2 are shown in Figure \ref{Example2FullVariogramsGAMPlot}. Inspection of the residual semivariograms does not show a clear case where the residuals form a straight-line white noise process, but it appears that most of the structure of the semivariogram is removed from residuals at about \texttt{k=150}. There is a large nugget effect in the response surface variogram, and less than half the semivariance is explained by the response surface models. By inspection, most of the the spatial covariance of the response surface is explained at \texttt{k=150}, although there may be a significant nugget effect associated with this \texttt{k}.

\subsection{Response surface model selection, Example 2}

%\begin{figure}\centering
%<<Example2FullModelSelectionLM,fig=TRUE,echo=FALSE, width=8, height=6>>=
%ggplot(Example2FullModelSelectionLM, aes(K,Score)) + 
%    geom_point(aes(colour = Criteria),size=2) + 
%    geom_line(aes(colour = Criteria),size=1) + 
%    scale_colour_manual(values=cbPalette) + facet_wrap(~Criteria,nrow=3,scales="free_y") +
%    labs(title="Scores for LM smoothers", x ="Smoothing Parameter", y = "Score")
%@
%\caption{Selection criteria for linear regression fit basis splines models}\label{Example2FullModelSelectionLM}
%\end{figure}

\begin{figure}\centering
<<Example2FullModelSelectionGAM, fig=TRUE,echo=FALSE, width=8, height=6>>=
ggplot(Example2FullModelSelectionGAM, aes(K,Score)) + 
    geom_point(aes(colour = Criteria),size=2) + 
    geom_line(aes(colour = Criteria),size=1) + 
    scale_colour_manual(values=cbPalette) + facet_wrap(~Criteria, nrow=3, scales="free_y") +
    labs(title="Scores for GAM smoothers", x ="Smoothing Parameter", y = "Score")
@
\caption{Selection criteria for \texttt{mgcv} fit response surface models show in Figure \ref{Example2FullVariogramsGAMPlot}. As with Figure \ref{Example1ModelSelectionGAM}, we see that the adjusted $R^2$ and GCV score subgraphs are mirror images and do not readily suggest an optimal model complexity. In the lower left corner, we see that models with \texttt{k<120} do not have significant nugget effects, while the slope of residual variograms are significant even at small values of \texttt{k}.}
\label{Example2FullModelSelectionGAM}
\end{figure}

Unlike Figure \ref{Example1ModelSelectionGAM}, the "elbow method" is not easily applied to Figure \ref{Example2FullModelSelectionGAM}. Both adjusted $R^2$ and generalized cross validation scores show curves that are steadily increasing (or decreasing) and don't appear to become asymptotic in the region of \texttt{k} values tested. In contrast, the nugget $p$-value method clearly suggests an optimal \texttt{k} of between 90 and 120, while the residual slope method suggests a small \texttt{k} value of about 30.

<<Example2FullSpatialGAMSummaries. include=FALSE, echo=FALSE, eval=FALSE>>=
summary(Example2FullNullGAM)
summary(Example2FullDefaultGAM)
summary(Example2FullGAMModels[[8]])
summary(Example2FullGAMModels[[9]])

gam.check(Example2FullDefaultGAM)
gam.check(Example2FullGAMModels[[8]])
gam.check(Example2FullGAMModels[[9]])
@

%<Example2FullSpatialLMSummaries, eval=FALSE,echo=FALSE, include=FALSE>>=
%summary(Example2FullNullLM)
%summary(Example2FullLMModels[[6]])
%summary(Example2FullLMModels[[7]])
%@

%\begin{figure}\centering
%<<Example2FullVariogramsLMPlot,fig=TRUE,echo=FALSE, width=8, height=6>>=
%Example2FullPredictionLMVariogramPars$Source <- "Smoothed Yield"
%Example2FullResidualsLMVariogramPars$Source <- "Residuals"
%side.dat <- rbind(Example2FullPredictionLMVariogramPars,Example2FullResidualsLMVariogramPars)
%side.dat[,'K'] <- factor(side.dat[,'K'])
%ggplot(side.dat, aes(Distance,Gamma)) + 
%    geom_point(aes(colour = K),size=2) + 
%    geom_smooth(aes(group = K,color=K), se=FALSE,alpha=0.5) +
%    scale_colour_manual(values=c(cbPalette,cbPalette)) + 
%    labs(title=paste('Variograms, LM, Example 1'), x ="Distance (m)", y = "Covariance") +
%    facet_wrap(~ Source, scales = "free")
%@
%\caption{Variograms for linear regression basis spline models fit to data from Example 1}\label{Example2FullVariogramsLMPlot}
%\end{figure}

\subsection{Fungicide treatment coefficients, Example 2}

<<Example2FullExtractCoefficents, include=FALSE, echo=FALSE>>=
Coef <- c()
Source <- c()
K = c()
UB <- c()
LB <- c()

totaldf <- dim(Example2Full.dat)[1]
for(i in 1:length(SpatialGamKs)) {
  coefstbl <- summary(Example2FullGAMModels[[i]])$p.table
  dftbl <- summary(Example2FullGAMModels[[i]])$s.table
  crit.t <- qt(1-0.025, totaldf - dftbl[1,2])
  UB <- c(UB, coefstbl[2,1]+crit.t*coefstbl[2,2])
  LB <- c(LB, coefstbl[2,1]-crit.t*coefstbl[2,2])
  Coef <- c(Coef, coefstbl[2,1])
  K <- c(K,SpatialGamKs[i])
}
Plot2 <- data.frame(Coef=Coef, K=K,UB=UB,LB=LB)
@

\begin{figure}\centering
<<Example2FullCoefficents, fig=TRUE,echo=FALSE, width=8, height=3>>=
ggplot(Plot2, aes(K,Coef)) + 
      geom_ribbon(aes(ymin = LB, ymax = UB),col=cbPalette[1],fill=cbPalette[2]) +
      geom_point(size=2) + 
      geom_line(linewidth=1) + 
      scale_colour_manual(values=cbPalette) + 
      labs(title="GAM Coefficients, Example 2", x ="Smoothing Parameter", y = "Product Effect Estimate")
@
\caption{Coefficients from \texttt{mcgv} response surface models fit to Example 2 as in Figure \ref{Example2FullGAMPredictions}. At smaller values of the predicted effect of fungicide is a small positive value that is not statistically significant. At higher values, we see the effect due to fungicide become negative and approach statistical significance at about \texttt{k=150}, and become very much more negative (nearly -5 bu/acre) at \texttt{k=180}. This figure illustrates the importance of additive model complexity on inferences about fixed effects in the model.}\label{Example2FullCoefficents}
\end{figure}

In Figure \ref{Example2FullCoefficents} we see that the effect associated with fungicide spray is generally small for small \texttt{k}, and changes from a positive effect to a negative effect at \texttt{k=90}. This makes the choice of the smoothing parameter \texttt{k} more important - we see either a slight benefit to an increasingly negative effect due to fungicide application as \texttt{k} increases. This may be influenced in part by the large areas of missing data - as \texttt{k} becomes larger the fit over missing data points becomes unstable. Still, in the best case, fungicide application had little to no positive effect on yield and may have actually been detrimental.

%\section{MCMC/Bayesian Methods}

\section{Fitting response surface models using \texttt{brm}}

In Chapter \ref{TrendAnalysisChapter} we found that fitting trend analysis using \texttt{mgcv} resulted in some inconsistent behavior that resulted in discontinuities in functional data analysis. We might not wish to rely on the results of \texttt{mgcv} for response surface analysis. The \texttt{brms} package provides a simple interface using the same \texttt{s} smoother specifier as \texttt{mgcv}, but computes fitted values using the \texttt{stan} computational engine. 

In this section, we reproduce the models fit using \texttt{mgcv} as shown in Figure \ref{Example1GAMPredictions}. Note the similarity between Figures \ref{Example1GAMPredictions} and \ref{Example1BRMPredictions}. 

<<CreateModelVectorsBRM, echo=FALSE>>=
# create lists to hold the results of fitting using gam and lm
Example1BRMModels <- vector(mode='list',length=length(SpatialGamKs))

#We save predicted values from the various smoothers for plotting.
Example1PredictedBRM <- NULL

#We compute variograms independently for both predicted values from the smoothers and for the residuals.
Example1VariogramsBRMPredictions <- vector(length(SpatialGamKs)+1, mode='list')
Example1VariogramsBRMResiduals <- vector(length(SpatialGamKs)+1, mode='list')
@

<<Example1VariogramBRM, include=FALSE, echo=FALSE>>=
#For reference, we compute a variogram for the raw \texttt{Yield} values, and save variograms values for each level of $k$.
#Raw yield will correspond to k=0
tmp.var <- variogram(Yield~1, 
                     locations=~Easting+Northing,
                      data=Example1.dat)
  
Example1PredictionBRMVariogramPars <-  data.frame(Distance=tmp.var[['dist']],
                               Gamma=tmp.var[['gamma']],
                               K=0)
Example1ResidualsBRMVariogramPars <-  Example1PredictionBRMVariogramPars

NewData <- Example1.dat[, c('Yield','Northing','Easting')]
NewData[,'Product'] <- "E"
@ 

<<ComputeExample1BRMSpatial, include=FALSE, echo=FALSE>>=
if(!file.exists("ComputeExample1BRMSpatial.Rda")) {
  Example1BRMAdjR2 <- 1:(length(SpatialGamKs)+1)
  Example1BRMGCV <- 1:(length(SpatialGamKs)+1)

  Example1NullBRM <- brm(Yield ~ Product + s(Easting, Northing),
                            silent=2, refresh = 0, data=Example1.dat[,c('Yield','Product','Northing','Easting')])
  Example1DefaultBRM <- brm(Yield ~ Product+s(Easting, Northing),
                            silent=2, refresh = 0, data=Example1.dat[,c('Yield','Product','Northing','Easting')])

  Example1NullBRMSummary<- summary(Example1NullBRM)
  #Example1BRMAdjR2[1] <- Example1NullSummary[['r.sq']]
  #Example1BRMGCV[1] <- Example1NullSummary[['sp.criterion']]


  for(i in 1:length(SpatialGamKs)) {
    tmp.dat <- Example1.dat[, c('Yield','Product','Northing','Easting')]
    fmla <- paste('Yield ~ Product + s(Easting, Northing, k=',SpatialGamKs[i],')')
    Example1BRMModels[[i]] <- brm(as.formula(fmla),silent=2, refresh = 0, data=Example1.dat)
  
    #gam.summary <- summary(Example1BRMModels[[i]])
    #Example1GAMAdjR2[i+1] <- gam.summary[['r.sq']]
    #Example1GAMGCV[i+1] <- gam.summary[['sp.criterion']]

#NOTE: we have estimate, error and CI from predict and residuals
    tmp.dat[,'Yield'] <- predict(Example1BRMModels[[i]],newdata=NewData)[,1]
    tmp.dat[,'K'] = SpatialGamKs[i]
    tmp.dat[,'Residuals'] <- residuals(Example1BRMModels[[i]])[,1]
    
    Example1PredictedBRM <- rbind(Example1PredictedBRM, tmp.dat)
    
    tmp.var <- variogram(Yield~1,  locations=~Easting+Northing,  data=tmp.dat)
    Example1VariogramsBRMPredictions[[i+1]] <- tmp.var

    Example1PredictionBRMVariogramPars <- rbind(Example1PredictionBRMVariogramPars,
                         data.frame(Distance=tmp.var[['dist']],
                                    Gamma=tmp.var[['gamma']],
                                     K=SpatialGamKs[i]))
    tmp.var <- variogram(Residuals~1, locations=~Easting+Northing, data=tmp.dat)
    Example1VariogramsBRMResiduals[[i+1]] <- tmp.var
    Example1ResidualsBRMVariogramPars <- rbind(Example1ResidualsBRMVariogramPars,
                             data.frame(Distance=tmp.var[['dist']],
                                        Gamma=tmp.var[['gamma']],
                                        K=SpatialGamKs[i]))
  }

  save(Example1PredictedBRM,
       Example1PredictionBRMVariogramPars,
       Example1ResidualsBRMVariogramPars,
       Example1BRMAdjR2,
       Example1BRMGCV,
       Example1NullBRM,
       Example1DefaultBRM,
       Example1BRMModels,
       file="ComputeExample1BRMSpatial.Rda")
} else {
  load(file="ComputeExample1BRMSpatial.Rda")
}
@

\begin{figure}\centering
<<Example1BRMPredictions, fig=TRUE, width=9, height=6, include=FALSE, echo=FALSE>>=
ggplot(Example1PredictedBRM, aes(Easting, Northing)) + 
geom_point(aes(colour = Yield),size=1) + 
scale_colour_gradient2(low=vermillion, mid=yellow, high=blue, midpoint = mean(Example1PredictedBRM[,'Yield'])) +
labs(colour = "Yield", x="Easting", y="Northing") + facet_wrap(~ K)
@
\caption{Predicted yield response surfaces from \texttt{brms} model with default bases for Example 1. Compare these response surfaces with Figure ref{Example1GAMPredictions}.}
\label{Example1BRMPredictions}
\end{figure}

<<Example1SpatialModelSelectionBRM, include=FALSE, echo=FALSE>>=
Example1VariogramSelectionBRM <- data.frame(K=unique(Example1ResidualsBRMVariogramPars[,'K']))
tmp.lm <- lm(Gamma ~ Distance, data=Example1ResidualsBRMVariogramPars[Example1ResidualsBRMVariogramPars[,'K']==0,])
Example1VariogramSelectionBRM[,'Intercept'] <- coef(tmp.lm)[1]
Example1VariogramSelectionBRM[,'Slope'] <- coef(tmp.lm)[2]
sum <- summary(tmp.lm)
Example1VariogramSelectionBRM[,'T'] <- sum[['coefficients']]['Distance','t value']
Example1VariogramSelectionBRM[,'P'] <- sum[['coefficients']]['Distance','Pr(>|t|)']
  
tmp.lm <- lm(Gamma ~ bs(Distance),data=Example1PredictionBRMVariogramPars[Example1PredictionBRMVariogramPars[,'K']==0,])
Example1VariogramSelectionBRM[1,'Nugget'] <- coef(tmp.lm)[1]
sum <- summary(tmp.lm)
Example1VariogramSelectionBRM[1,'NuggetT'] <- sum[['coefficients']][1,'t value']
Example1VariogramSelectionBRM[1,'NuggetP'] <- sum[['coefficients']][1,'Pr(>|t|)']
      
  for (k in 1:length(Example1VariogramSelectionBRM[,'K'])) {
    K <- Example1VariogramSelectionBRM[,'K'][k]
    tmp.lm <- lm(Gamma ~ Distance, data=Example1ResidualsBRMVariogramPars[Example1ResidualsBRMVariogramPars[,'K']==K,])
    Example1VariogramSelectionBRM[k,'Intercept'] <- coef(tmp.lm)[1]
    Example1VariogramSelectionBRM[k,'Slope'] <- coef(tmp.lm)[2]
    sum <- summary(tmp.lm)
    Example1VariogramSelectionBRM[k,'SlopeT'] <- sum[['coefficients']]['Distance','t value']
    Example1VariogramSelectionBRM[k,'SlopeP'] <- sum[['coefficients']]['Distance','Pr(>|t|)']
    tmp.lm <- lm(Gamma ~ bs(Distance),data=Example1PredictionBRMVariogramPars[Example1PredictionBRMVariogramPars[,'K']==K,])
    Example1VariogramSelectionBRM[k,'Nugget'] <- coef(tmp.lm)[1]
    sum <- summary(tmp.lm)
    Example1VariogramSelectionBRM[k,'NuggetT'] <- sum[['coefficients']][1,'t value']
    Example1VariogramSelectionBRM[k,'NuggetP'] <- sum[['coefficients']][1,'Pr(>|t|)']
  }
  
  Example1ModelSelectionBRM <- data.frame(K = rep(c(0,SpatialGamKs),2),
                           #Criteria = as.factor(c(rep('AIC',length(SpatialGamKs)+1),
                           #                        rep('BIC',length(SpatialGamKs)+1),
                            Criteria = as.factor(c(#rep('Adjusted R Squared',length(SpatialGamKs)+1),
                                                   #rep('Generalized Cross Validation',length(SpatialGamKs)+1),
                                                   rep('Nugget',length(SpatialGamKs)+1),
                                                   rep('Slope',length(SpatialGamKs)+1))),
                            #Score = c(c(AIC(Example2FullNullGAM),unlist(lapply(Example1BRMModels,AIC))),
                            #          c(BIC(Example2FullNullGAM),unlist(lapply(Example1BRMModels,BIC))),
                            Score = c(#Example1BRMAdjR2,
                                      #Example1BRMGCV,
                                      Example1VariogramSelectionBRM[,'NuggetP'],
                                      Example1VariogramSelectionBRM[,'SlopeP'])
                            )
#SelectionGAM  
@

\begin{figure}\centering
<<Example1ModelSelectionBRM, fig=TRUE,echo=FALSE, width=8, height=6>>=
ggplot(Example1ModelSelectionBRM, aes(K,Score)) + 
    geom_point(aes(colour = Criteria),size=2) + 
    geom_line(aes(colour = Criteria),size=1) + 
    scale_colour_manual(values=cbPalette) + facet_wrap(~Criteria, nrow=3, scales="free_y") +
    labs(title="Scores for BRM smoothers", x ="Smoothing Parameter", y = "Score")
@
\caption{Selection criteria for \texttt{brm} fit response surface models. Unlike Figure \ref{Example1ModelSelectionGAM}, this graph only shows the $p$-values associated with the semivariograms (not shown). Note the similarity between this figure and the bottom subgraphs of Figure \ref{Example1ModelSelectionGAM}.}
\label{Example1ModelSelectionBRM}
\end{figure}

<<Example1ExtractCoefficentsBRM, include=FALSE, echo=FALSE>>=
Coef <- c()
LB <- c()
UB <- c()
K <- c()

for(i in 1:length(SpatialGamKs)) {
  coefs <- fixef(Example1BRMModels[[i]])
  Coef <- c(Coef, coefs[2,1])
  LB <- c(LB, coefs[2,3])
  UB <- c(UB, coefs[2,4])
  K <- c(K,SpatialGamKs[i])
}
PlotBRM <- data.frame(Coef=Coef, K=K,LB=LB, UB=UB)
@

\begin{figure}\centering
<<Example1BRMCoefficents, fig=TRUE,echo=FALSE, width=8, height=3>>=
ggplot(PlotBRM, aes(K,Coef)) + 
      geom_ribbon(aes(ymin = LB, ymax = UB),col=cbPalette[1],fill=cbPalette[2]) +
      geom_point(size=2) + 
      geom_line(linewidth=1) + 
      scale_colour_manual(values=cbPalette) + 
      labs(title="BRM Coefficients, Example 1", x ="Smoothing Parameter", y = "Product Effect Estimate")
@
\caption{Coefficients from \texttt{brms} response surface models fit to Example 1. This method identifies similar variety effects as in the lower subgraph of Figure \ref{Example1Coefficents}}
\label{Example1BRMCoefficents}
\end{figure}

<<FitBRMSpatialModels, eval=FALSE, echo=FALSE, include=FALSE>>=
if(!file.exists('FitBRMSpatialModels.Rda')) {
  fit.brms <- brm(Yield ~ Sprayed + s(Longitude,Latitude),data=Example2Full.dat)
  save(fit.brms, file='FitBRMSpatialModels.Rda')
  brms65 <- brm(Yield ~ Sprayed + s(Longitude, Latitude, k=65),data=Example2Full.dat)
  save(fit.brms, brms65, file='FitBRMSpatialModels.Rda')
  brms116 <- brm(Yield ~ Sprayed + s(Longitude, Latitude, k=116),data=Example2Full.dat)
  save(fit.brms, brms65, brms116,file='FitBRMSpatialModels.Rda')
  brms320 <- brm(Yield ~ Sprayed + s(Longitude, Latitude, k=320),data=Example2Full.dat)
  save(fit.brms, brms65, brms116, brms320, file='FitBRMSpatialModels.Rda')
} else {
  load(file='FitBRMSpatialModels.Rda')
}

summary(fit.brms)
#plot(fit.brms)

summary(brms65)
#plot(brms100)

summary(brms116)

summary(brms320)
#plot(brms320)
@

We see considerable agreement in the response surface models fit using either \texttt{mgcv} (Figure \ref{Example1Coefficents}, subgraph on the right) or \texttt{brms} (Figure \ref{Example1BRMCoefficents}). In general, fitting surfaces using  \texttt{mgcv} is faster, since it does not rely on Markov chain methods, as with the \texttt{stan} engine underlying \texttt{brms}.


\section{Response surface models using a subset of Example 2}



<<SubsetExample2, echo=FALSE, include=FALSE>>=
Example2Sub.dat <- Example2Full.dat[Example2Full.dat[,'Northing']>200,]
Example2Sub.dat <- Example2Sub.dat[Example2Sub.dat[,'Northing']<500,]
Example2Sub.dat <- Example2Sub.dat[Example2Sub.dat[,'Easting']>200,]
@

\begin{figure}\centering
<<Example2SubMaps, fig=TRUE, echo=FALSE, width=8, height=4>>=
grid.arrange(
  arrangeGrob(
       ggplot(Example2Sub.dat, aes(Easting, Northing)) + 
            geom_point(aes(colour = Sprayed),size=1) + 
            scale_colour_manual(values=cbPalette[pair.colors]) +
            labs(colour = "Sprayed", x="Easting", y="Northing", title = "Spray Map"),
    ggplot(Example2Sub.dat, aes(Easting, Northing)) + 
           geom_point(aes(colour = Yield),size=1) + 
           scale_colour_gradient2(low=vermillion, mid=yellow, high=blue, midpoint = median(Example2Sub.dat[,'Yield'])) +
           labs(colour = "Yield", x="Easting", y="Northing",title="Harvest Map"),
    nrow=1
  )
)
@
\caption{Spraying and yield monitor harvest maps for a subset of Example 2. The observed yield data has been trimmed to provide a more rectangular shape, although this does not remove the non-yields regions from the map.}\label{Example2SubMaps}
\end{figure}

In the preceding section, we fit a response surface model to the data from Example 2. However, a response surface is most commonly a rectangle, while the data from Example 2 do not form a rectangle. Thus, we are fitting a model to locations that do not have observations. This may give us unrealistic coefficient estimates. So, we will exclude a portion of the data from Example 2, restricting our analysis to a rectangular region of the field. Specifically will only analyze the region from 200m to 500m \texttt{Northing} and from 100m \texttt{Easting} as shown in Figure \ref{Example2SubMaps}. There is still missing data, but we don't force our model to extrapolate beyond the bounds of the experiment.

<<Example2SubCreateModelVectors, echo=FALSE>>=
# create lists to hold the results of fitting using gam and lm
Example2SubGAMModels <- vector(mode='list',length=length(SpatialGamKs))
#Example2SubModelsLM <- vector(mode='list',length=length(SpatialGamKs))

#We save predicted values from the various smoothers for plotting.
Example2SubPredictedGAM <- NULL
#Example2SubPredictedLM <- NULL

#We compute variograms independently for both predicted values from the smoothers and for the residuals.
Example2SubVariogramsGAMPredictions <- vector(length(SpatialGamKs)+1, mode='list')
Example2SubVariogramsResiduals <- vector(length(SpatialGamKs)+1, mode='list')
#Example2SubVariogramsLMPredictions <- vector(length(SpatialGamKs)+1, mode='list')
#Example2SubVariogramsLMResiduals <- vector(length(SpatialGamKs)+1, mode='list')
@

<<Example2SubVariogram, include=FALSE, echo=FALSE>>=
#For reference, we compute a variogram for the raw \texttt{Yield} values, and save variograms values for each level of $k$.
#Raw yield will correspond to k=0
tmp.var <- variogram(Yield~1, 
                     locations=~Easting+Northing,
                      data=Example2Sub.dat)
  
Example2SubPredictionGAMVariogramPars <-  data.frame(Distance=tmp.var[['dist']],
                               Gamma=tmp.var[['gamma']],
                               K=0)
Example2SubResidualsGAMVariogramPars <-  Example2SubPredictionGAMVariogramPars
#Example2SubPredictionLMVariogramPars <-  data.frame(Distance=tmp.var[['dist']],
#                               Gamma=tmp.var[['gamma']],
#                               K=0)
#Example2SubResidualsLMVariogramPars <-  Example2SubPredictionLMVariogramPars

NewData <- Example2Sub.dat[, c('Yield','Northing','Easting')]
NewData[,'Sprayed'] <- FALSE
@

<<ComputeExample2SubGAMSpatial, include=FALSE, echo=FALSE>>=
if(!file.exists("ComputeExample2SubGAMSpatial.Rda")) {
  Example2SubGAMAdjR2 <- 1:(length(SpatialGamKs)+1)
  Example2SubGAMGCV <- 1:(length(SpatialGamKs)+1)

  Example2SubNullGAM <- gam(Yield ~ Sprayed, data=Example2Sub.dat[,c('Yield','Sprayed','Northing','Easting')])
  Example2SubDefaultGAM <- gam(Yield ~ Sprayed+s(Easting, Northing), data=Example2Sub.dat[,c('Yield','Sprayed','Northing','Easting')])

  Example2SubNullSummary<- summary(Example2SubNullGAM)
  Example2SubGAMAdjR2[1] <- Example2SubNullSummary[['r.sq']]
  Example2SubGAMGCV[1] <- Example2SubNullSummary[['sp.criterion']]

  for(i in 1:length(SpatialGamKs)) {
    tmp.dat <- Example2Sub.dat[, c('Yield','Sprayed','Northing','Easting')]
    fmla <- paste('Yield ~ Sprayed + s(Easting, Northing, k=',SpatialGamKs[i],')')
    Example2SubGAMModels[[i]] <- gam(as.formula(fmla),data=tmp.dat)
  
    gam.summary <- summary(Example2SubGAMModels[[i]],newdata=NewData)
    Example2SubGAMAdjR2[i+1] <- gam.summary[['r.sq']]
    Example2SubGAMGCV[i+1] <- gam.summary[['sp.criterion']]

    tmp.dat[,'Yield'] <- predict(Example2SubGAMModels[[i]])
    tmp.dat[,'K'] = SpatialGamKs[i]
    tmp.dat[,'Residuals'] <- residuals(Example2SubGAMModels[[i]])
    
    Example2SubPredictedGAM <- rbind(Example2SubPredictedGAM, tmp.dat)
    
    tmp.var <- variogram(Yield~1,  locations=~Easting+Northing,  data=tmp.dat)
    Example2SubVariogramsGAMPredictions[[i+1]] <- tmp.var

    Example2SubPredictionGAMVariogramPars <- rbind(Example2SubPredictionGAMVariogramPars,
                           data.frame(Distance=tmp.var[['dist']],
                                      Gamma=tmp.var[['gamma']],
                                       K=SpatialGamKs[i]))
    tmp.var <- variogram(Residuals~1, locations=~Easting+Northing, data=tmp.dat)
    Example2SubVariogramsResiduals[[i+1]] <- tmp.var
    Example2SubResidualsGAMVariogramPars <- rbind(Example2SubResidualsGAMVariogramPars,
                               data.frame(Distance=tmp.var[['dist']],
                                          Gamma=tmp.var[['gamma']],
                                          K=SpatialGamKs[i]))
  }
  save(Example2SubPredictedGAM,
       Example2SubPredictionGAMVariogramPars,
       Example2SubResidualsGAMVariogramPars,
       Example2SubGAMAdjR2,
       Example2SubGAMGCV,
       Example2SubNullGAM,
       Example2SubDefaultGAM,
       Example2SubGAMModels,
       file="ComputeExample2SubGAMSpatial.Rda")
} else {
  load(file="ComputeExample2SubGAMSpatial.Rda")
}
@

%<<ComputeExample2SubLMSpatial,include=FALSE, echo=FALSE>>=
%if(!file.exists("ComputeExample2SubLMSpatial.Rda")) {
%  Example2SubLMAdjR2 <- 1:(length(SpatialGamKs)+1)
%  Example2SubKFoldCVLM <- 1:(length(SpatialGamKs)+1)

%  #null model has no spatial information
%  Example2SubColumnSubset = Example2Sub.dat[,c('Yield','Sprayed','Northing','Easting')]
%  Example2SubNullLM <- lm(Yield ~ Sprayed,data=Example2SubColumnSubset)
%  current.summary <- summary(Example2SubNullLM)
%  Example2SubLMAdjR2[1] <- current.summary[['adj.r.squared']]

%  #compute k-fold cross validation
%  #ordinary LOOCV takes several hours to compute for just one set of models.
%  #we'll have k folds
%  k=10
%  #assign folds to data in groups of 10, at random. We do this in the order the data were collected to
%  #be sure that that test data set is uniformly space throughout the field
%  foldgroups <- ceiling(dim(Example2SubColumnSubset)[1]/k)
%  maxidx = dim(Example2SubColumnSubset)[1]
%  Example2SubColumnSubset[,'Fold'] <- 0
%  for(j in 1:foldgroups) {
%    range = 1:10 + (j-1)*10
%    if(j*10>maxidx) {
%      range = ((j-1)*10):maxidx
%      Example2SubColumnSubset[,'Fold'][range] <- sample(1:length(range))
%    } else {
%      Example2SubColumnSubset[,'Fold'][range] <- sample(1:10)
%    }
%  }
%  #now, hold out fold and fit model to training data
%  sum <- 0
%  FoldMSE <- 1:k
%  for(j in 1:k) {
%    current.train <- Example2SubColumnSubset[Example2SubColumnSubset[,'Fold'] != j,]
%    current.test <- Example2SubColumnSubset[Example2SubColumnSubset[,'Fold'] == j,]
%    current.model <- lm(Yield ~ Sprayed, data=current.train)
%    predicted <- predict(current.model, newdata = current.test)
%   FoldMSE[j] <- sum(predicted - current.test[,'Yield'])^2/dim(current.test)[1]
%  }
%  Example2SubKFoldCVLM[1] <- mean(FoldMSE)

%  for(i in 1:length(SpatialGamKs)) {
  
%    tmp.dat <- Example2Sub.dat[, c('Yield','Sprayed','Northing','Easting')]
  
%    fmla <- paste('Yield ~ Sprayed + bs(Easting,df=',SpatialLMBs[i],')*bs(Northing,df=',SpatialLMBs[i],')')
%    Example2SubModelsLM[[i]] <- lm(as.formula(fmla), data=tmp.dat)
  
%    #compute adjusted R2
%    current.summary <- summary(Example2SubModelsLM[[i]],newdata=NewData)
%    Example2SubLMAdjR2 [i+1] <- current.summary[['adj.r.squared']]
%    #use the same folds as the null model
%    tmp.dat[,'Fold'] <- Example2SubColumnSubset[,'Fold']
%    sum <- 0
%    FoldMSE <- 1:k
%    for(j in 1:k) {
%      current.train <- tmp.dat[tmp.dat[,'Fold'] != j,]
%      current.test <- tmp.dat[tmp.dat[,'Fold'] == j,]
%      current.model <- lm(as.formula(fmla), data=current.train)
%      predicted <- predict(current.model, newdata = current.test)
%      FoldMSE[j] <- sum(predicted - current.test[,'Yield'])^2/dim(current.test)[1]
%    }
%    Example2SubKFoldCVLM[i+1] <- mean(FoldMSE)

%    tmp.dat[,'Yield'] <- predict(Example2SubModelsLM[[i]])
%    tmp.dat[,'K'] = SpatialLMBs[i]
%    tmp.dat[,'Residuals'] <- residuals(Example2SubModelsLM[[i]])
%    
%    Example2SubPredictedLM <- rbind(Example2SubPredictedLM, tmp.dat)
%    
%    tmp.var <- variogram(Yield~1,  locations=~Easting+Northing,  data=tmp.dat)
%    Example2SubVariogramsLMPredictions[[i+1]] <- tmp.var

%    Example2SubPredictionLMVariogramPars <- rbind(Example2SubPredictionLMVariogramPars,
%                           data.frame(Distance=tmp.var[['dist']],
%                                      Gamma=tmp.var[['gamma']],
%                                       K=SpatialLMBs[i]))
%    tmp.var <- variogram(Residuals~1, locations=~Easting+Northing, data=tmp.dat)
%    Example2SubVariogramsLMResiduals[[i+1]] <- tmp.var
%    Example2SubResidualsLMVariogramPars <- rbind(Example2SubResidualsLMVariogramPars,
%                               data.frame(Distance=tmp.var[['dist']],
%                                          Gamma=tmp.var[['gamma']],
%                                          K=SpatialLMBs[i]))
%  }
%  save(Example2SubPredictedLM,
%       Example2SubPredictionLMVariogramPars,
%      Example2SubResidualsLMVariogramPars,
%       Example2SubLMAdjR2,
%       Example2SubKFoldCVLM,
%       Example2SubNullLM,
%       Example2SubModelsLM,
%       file="ComputeExample2SubLMSpatial.Rda")
%} else {
%  load(file="ComputeExample2SubLMSpatial.Rda")
%}
%@


%\begin{figure}\centering
%<<Example2SubLMPredictions,fig=TRUE, width=9, height=6, echo=FALSE>>=
%ggplot(Example2SubPredictedLM, aes(Easting,Northing)) + 
%geom_point(aes(colour = Yield),size=1) + 
%scale_colour_gradient2(low=vermillion, mid=yellow, high=blue,midpoint = mean(Example2SubPredictedLM[,'Yield'])) +
%labs(colour = "Yield", x="Easting", y="Northing") + facet_wrap(~ K)
%@
%\caption{Predicted yield response surfaces from linear model with b-splines for Example 2}\label{Example2SubLMPredictions}
%\end{figure}

\begin{figure}\centering
<<Example2SubGAMPredictions, fig=TRUE, width=9, height=6, echo=FALSE>>=
ggplot(Example2SubPredictedGAM, aes(Easting, Northing)) + 
geom_point(aes(colour = Yield),size=1) + 
scale_colour_gradient2(low=vermillion, mid=yellow, high=blue, midpoint = mean(Example2SubPredictedGAM[,'Yield'])) +
labs(colour = "Yield", x="Easting", y="Northing") + facet_wrap(~ K)
@
\caption{Predicted yield response surfaces from \texttt{mgcv} model with default bases for a subset of Example 2. Compare this with Figure \ref{Example2FullGAMPredictions}.}
\label{Example2SubGAMPredictions}
\end{figure}

<<Example2SubSpatialModelSelectionGAM, include=FALSE, echo=FALSE>>=
Example2SubVariogramSelectionGAM <- data.frame(K=unique(Example2SubResidualsGAMVariogramPars[,'K']))
tmp.lm <- lm(Gamma ~ Distance, data=Example2SubResidualsGAMVariogramPars[Example2SubResidualsGAMVariogramPars[,'K']==0,])
Example2SubVariogramSelectionGAM[,'Intercept'] <- coef(tmp.lm)[1]
Example2SubVariogramSelectionGAM[,'Slope'] <- coef(tmp.lm)[2]
sum <- summary(tmp.lm)
Example2SubVariogramSelectionGAM[,'T'] <- sum[['coefficients']]['Distance','t value']
Example2SubVariogramSelectionGAM[,'P'] <- sum[['coefficients']]['Distance','Pr(>|t|)']
  
tmp.lm <- lm(Gamma ~ bs(Distance),data=Example2SubPredictionGAMVariogramPars[Example2SubPredictionGAMVariogramPars[,'K']==0,])
Example2SubVariogramSelectionGAM[1,'Nugget'] <- coef(tmp.lm)[1]
sum <- summary(tmp.lm)
Example2SubVariogramSelectionGAM[1,'NuggetT'] <- sum[['coefficients']][1,'t value']
Example2SubVariogramSelectionGAM[1,'NuggetP'] <- sum[['coefficients']][1,'Pr(>|t|)']
      
  for (k in 1:length(Example2SubVariogramSelectionGAM[,'K'])) {
    K <- Example2SubVariogramSelectionGAM[,'K'][k]
    tmp.lm <- lm(Gamma ~ Distance, data=Example2SubResidualsGAMVariogramPars[Example2SubResidualsGAMVariogramPars[,'K']==K,])
    Example2SubVariogramSelectionGAM[k,'Intercept'] <- coef(tmp.lm)[1]
    Example2SubVariogramSelectionGAM[k,'Slope'] <- coef(tmp.lm)[2]
    sum <- summary(tmp.lm)
    Example2SubVariogramSelectionGAM[k,'SlopeT'] <- sum[['coefficients']]['Distance','t value']
    Example2SubVariogramSelectionGAM[k,'SlopeP'] <- sum[['coefficients']]['Distance','Pr(>|t|)']
    tmp.lm <- lm(Gamma ~ bs(Distance),data=Example2SubPredictionGAMVariogramPars[Example2SubPredictionGAMVariogramPars[,'K']==K,])
    Example2SubVariogramSelectionGAM[k,'Nugget'] <- coef(tmp.lm)[1]
    sum <- summary(tmp.lm)
    Example2SubVariogramSelectionGAM[k,'NuggetT'] <- sum[['coefficients']][1,'t value']
    Example2SubVariogramSelectionGAM[k,'NuggetP'] <- sum[['coefficients']][1,'Pr(>|t|)']
  }
  
  Example2SubModelSelectionGAM <- data.frame(K = rep(c(0,SpatialGamKs),4),
                           #Criteria = as.factor(c(rep('AIC',length(SpatialGamKs)+1),
                           #                        rep('BIC',length(SpatialGamKs)+1),
                            Criteria = as.factor(c(rep('Adjusted R Squared',length(SpatialGamKs)+1),
                                                   rep('Generalized Cross Validation',length(SpatialGamKs)+1),
                                                   rep('Nugget',length(SpatialGamKs)+1),
                                                   rep('Slope',length(SpatialGamKs)+1))),
                            #Score = c(c(AIC(Example2SubNullGAM),unlist(lapply(Example2SubGAMModels,AIC))),
                            #          c(BIC(Example2SubNullGAM),unlist(lapply(Example2SubGAMModels,BIC))),
                            Score = c(Example2SubGAMAdjR2,
                                      Example2SubGAMGCV,
                                      Example2SubVariogramSelectionGAM[,'NuggetP'],
                                      Example2SubVariogramSelectionGAM[,'SlopeP'])
                            )
#SelectionGAM  
@

%<<Example2SubSpatialModelSelectionLM, include=FALSE, echo=FALSE>>=
%Example2SubVariogramSelectionLM <- data.frame(K=unique(Example2SubResidualsLMVariogramPars[,'K']))
%tmp.lm <- lm(Gamma ~ Distance,data=Example2SubResidualsLMVariogramPars[Example2SubResidualsGAMVariogramPars[,'K']==0,])
%Example2SubVariogramSelectionLM[,'Intercept'] <- coef(tmp.lm)[1]
%Example2SubVariogramSelectionLM[,'Slope'] <- coef(tmp.lm)[2]
%sum <- summary(tmp.lm)
%Example2SubVariogramSelectionLM[,'SlopeT'] <- sum[['coefficients']]['Distance','t value']
%Example2SubVariogramSelectionLM[,'SlopeP'] <- sum[['coefficients']]['Distance','Pr(>|t|)']
  
%tmp.lm <- lm(Gamma ~ bs(Distance),data=Example2SubPredictionLMVariogramPars[Example2SubPredictionLMVariogramPars[,'K']==0,])
%Example2SubVariogramSelectionLM[,'Nugget'] <- coef(tmp.lm)[1]
%sum <- summary(tmp.lm)
%Example2SubVariogramSelectionLM[,'NuggetT'][1] <- sum[['coefficients']][1,'t value']
%Example2SubVariogramSelectionLM[,'NuggetP'][1] <- sum[['coefficients']][1,'Pr(>|t|)']
  
%  for (k in 1:length(Example2SubVariogramSelectionLM[,'K'])) {
%    K <- Example2SubVariogramSelectionLM[,'K'][k]
%    tmp.lm <- lm(Gamma ~ Distance,data=Example2SubResidualsLMVariogramPars[Example2SubResidualsLMVariogramPars[,'K']==K,])
%    Example2SubVariogramSelectionLM[k,'Intercept'] <- coef(tmp.lm)[1]
%    Example2SubVariogramSelectionLM[k,'Slope'] <- coef(tmp.lm)[2]
%    sum <- summary(tmp.lm)
%    Example2SubVariogramSelectionLM[k,'SlopeT'] <- sum[['coefficients']]['Distance','t value']
%    Example2SubVariogramSelectionLM[k,'SlopeP'] <- sum[['coefficients']]['Distance','Pr(>|t|)']
%    tmp.lm <- lm(Gamma ~ bs(Distance),data=Example2SubPredictionLMVariogramPars[Example2SubPredictionLMVariogramPars[,'K']==K,])
%    Example2SubVariogramSelectionLM[k,'Nugget'] <- coef(tmp.lm)[1]
%    sum <- summary(tmp.lm)
%    Example2SubVariogramSelectionLM[k,'NuggetT'] <- sum[['coefficients']][1,'t value']
%    Example2SubVariogramSelectionLM[k,'NuggetP'] <- sum[['coefficients']][1,'Pr(>|t|)']
%  }
  
%  Example2SubModelSelectionLM <- data.frame(K = rep(c(0,SpatialLMBs),4),
%                            #Criteria = as.factor(c(rep('AIC',length(SpatialGamKs)+1),
%                            #                       rep('BIC',length(SpatialGamKs)+1),
%                            Criteria = as.factor(c(rep('Adjusted R Squared',length(SpatialGamKs)+1),
%                                                   rep('K-Fold Cross Validation',length(SpatialGamKs)+1),
%                                                   rep('Nugget p-value',length(SpatialGamKs)+1),
%                                                   rep('Slope p-value',length(SpatialGamKs)+1))),
%                            #Score = c(c(AIC(nulllm.model),unlist(lapply(Example2SubModelsLM,AIC))),
%                             #         c(BIC(nulllm.model),unlist(lapply(Example2SubModelsLM,BIC))),
%                            Score = c(Example2SubLMAdjR2 ,
%                                      Example2SubKFoldCVLM,
%                                      Example2SubVariogramSelectionLM[,'NuggetP'],
%                                      Example2SubVariogramSelectionLM[,'SlopeP'])
%                            )
%@

%\begin{figure}\centering
%<<Example2SubModelSelectionLM,fig=TRUE,echo=FALSE, width=8, height=6>>=
%ggplot(Example2SubModelSelectionLM, aes(K,Score)) + 
%    geom_point(aes(colour = Criteria),size=2) + 
%    geom_line(aes(colour = Criteria),size=1) + 
%    scale_colour_manual(values=cbPalette) + facet_wrap(~Criteria,nrow=3,scales="free_y") +
%    labs(title="Scores for LM smoothers", x ="Smoothing Parameter", y = "Score")
%@
%\caption{Selection criteria for linear regression fit basis splines models}\label{Example2SubModelSelectionLM}
%\end{figure}

\begin{figure}\centering
<<Example2SubModelSelectionGAM, fig=TRUE,echo=FALSE, width=8, height=6>>=
ggplot(Example2SubModelSelectionGAM, aes(K,Score)) + 
    geom_point(aes(colour = Criteria),size=2) + 
    geom_line(aes(colour = Criteria),size=1) + 
    scale_colour_manual(values=cbPalette) + facet_wrap(~Criteria, nrow=3, scales="free_y") +
    labs(title="Scores for GAM smoothers", x ="Smoothing Parameter", y = "Score")
    
@
\caption{Selection criteria for GAM fit basis splines models. Compare this with Figure \ref{Example2FullModelSelectionGAM}}
\label{Example2SubModelSelectionGAM}
\end{figure}

<<Example2SubExample1SmoothedHarvestPassesSpatialGAMSummaries, include=FALSE, echo=FALSE, eval=FALSE>>=
summary(Example2SubNullGAM)
summary(Example2SubDefaultGAM)
summary(Example2SubGAMModels[[8]])
summary(Example2SubGAMModels[[9]])

gam.check(Example2SubDefaultGAM)
gam.check(Example2SubGAMModels[[8]])
gam.check(Example2SubGAMModels[[9]])
@

%<<Example2SubSpatialLMSummaries, eval=FALSE,echo=FALSE, include=FALSE>>=
%summary(Example2SubNullLM)
%summary(Example2SubLMModels[[6]])
%summary(Example2SubLMModels[[7]])
%@

\begin{figure}\centering
<<Example2SubVariogramsGAMPlot, fig=TRUE,echo=FALSE, width=8, height=6>>=
Example2SubPredictionGAMVariogramPars[,"Source"] <- "Response Surface"
Example2SubResidualsGAMVariogramPars[,"Source"] <- "Residuals"
side.dat <- rbind(Example2SubPredictionGAMVariogramPars,Example2SubResidualsGAMVariogramPars)
side.dat[,'K'] <- factor(side.dat[,'K'])

ggplot(side.dat, aes(Distance,Gamma)) + 
    geom_point(aes(colour = K), size=2) + 
    geom_smooth(aes(group = K, color=K), se=FALSE,alpha=0.5) +
    scale_colour_manual(values=c(cbPalette, cbPalette)) + 
    labs(title=paste('Variograms, GAM, Model 1'), x ="Distance (m)", y = "Covariance") +
    facet_wrap(~ Source, scales = "free")
@
\caption{Variograms for GAM basis spline models fit to data from Example 2. Compare this with Figure \ref{Example2FullVariogramsGAMPlot}}
\label{Example2SubVariogramsGAMPlot}
\end{figure}

%\begin{figure}\centering
%<<Example2SubVariogramsLMPlot,fig=TRUE,echo=FALSE, width=8, height=6>>=
%Example2SubPredictionLMVariogramPars$Source <- "Smoothed Yield"
%Example2SubResidualsLMVariogramPars$Source <- "Residuals"
%side.dat <- rbind(Example2SubPredictionLMVariogramPars,Example2SubResidualsLMVariogramPars)
%side.dat[,'K'] <- factor(side.dat[,'K'])
%ggplot(side.dat, aes(Distance,Gamma)) + 
%    geom_point(aes(colour = K),size=2) + 
%    geom_smooth(aes(group = K,color=K), se=FALSE,alpha=0.5) +
%    scale_colour_manual(values=c(cbPalette,cbPalette)) + 
%    labs(title=paste('Variograms, LM, Example 1'), x ="Distance (m)", y = "Covariance") +
%    facet_wrap(~ Source, scales = "free")
%@
%\caption{Variograms for linear regression basis spline models fit to data from Example 1}\label{Example2SubVariogramsLMPlot}
%\end{figure}

<<Example2SubExtractCoefficents, include=FALSE, echo=FALSE>>=
Coef <- c()
Source <- c()
K = c()
UB <- c()
LB <- c()

totaldf <- dim(Example2Sub.dat)[1]
for(i in 1:length(SpatialGamKs)) {
  coefstbl <- summary(Example2SubGAMModels[[i]])[["p.table"]]
  dftbl <- summary(Example2SubGAMModels[[i]])[["s.table"]]
  crit.t <- qt(1-0.025, totaldf - dftbl[1,2])
  UB <- c(UB, coefstbl[2,1]+crit.t*coefstbl[2,2])
  LB <- c(LB, coefstbl[2,1]-crit.t*coefstbl[2,2])
  Coef <- c(Coef, coefstbl[2,1])
  K <- c(K,SpatialGamKs[i])
}
Plot2 <- data.frame(Coef=Coef, K=K,UB=UB,LB=LB)
@

\begin{figure}\centering
<<Example2SubCoefficents, fig=TRUE,echo=FALSE, width=8, height=3>>=
ggplot(Plot2, aes(K,Coef)) + 
      geom_ribbon(aes(ymin = LB, ymax = UB),col=cbPalette[1],fill=cbPalette[2]) +
      geom_point(size=2) + 
      geom_line(linewidth=1) + 
      scale_colour_manual(values=cbPalette) + 
      labs(title="GAM Coefficients, Example 2", x ="Smoothing Parameter", y = "Product Effect Estimate")
@
\caption{Coefficients from \texttt{mcgv} and response surface models fit to a subset of Example 2. Compare this with Figure \ref{Example2FullCoefficents}. Note that this figure more consistently associates a significant negative value with fungicide application, when found using response surface models of increasing complexity, fit to a subregion of the cropland. We can attribute this to better model fitting when the region to be fit is more rectangular, although we might instead attribute this to having excluded a portion of the field were the product was more effective. However, from Figure \ref{Example2Differences} we found positive effects of fungicide in the region from roughly 300m to 550m \texttt{Northing}, while the data used to generate these estimates was from 200-500m \texttt{Norhting} (Figure \ref{Example2SubMaps})}\label{Example2SubCoefficents}
\end{figure}

We spend only a little discussion on the analysis of the subset of Example 2. From Figures \ref{Example2SubGAMPredictions}, \ref{Example2SubVariogramsGAMPlot} and \ref{Example2SubModelSelectionGAM}, the analysis of the subset of Example 2 is not greatly different from the analysis of the full data set. However, it is worth noting that the estimated effect due to fungicide spraying in Figure \ref{Example2SubCoefficents} is consistently negative for \texttt{k>0}, and for the best models (\texttt{k=90}) the effect of fungicide spray was a loss of roughly 3 bu/acre.

\section{Spatial analysis summary}

In many cases, strip trials are executed as in Example 2, where the strips are not of uniform size. These types of trials are not amenable to the functional data analysis methods outlined in preceding chapters. Even simple univariate is suspect because of the lack of uniformity in experimental units. Spatial analysis in the form of fitting a response surface with additional fixed effects as described in this section allows us to recover information about treatments in such trials. As we see, model selection criteria such as adjusted $R^2$ or cross-validation may not help inform our choices about the degree of smoothing applicable to fit a response surface, but information, in the form of the semivariogram, about random field implied by a response surface can help guide our choices in fitting response surfaces. 

\chapter{Conclusions}\label{ConclusionsChapter}

In Example 1, we see a data set that is very amenable to functional data analysis or spatial response surface analysis. Not only is the data executed as a neat rectangular area with regularly sized strips, the effect size is quite large. In practice, a trial such as this may not need statistical analysis to provide convincing evidence of the benefit of one variety over another - the difference may be apparent to the farmer-collaborator on examination of the field during the growing season, and by inspection of yield monitor estimates as the harvester moves through different parts of the field. Example 1 does highlight an important consideration for on-farm strip trials. In the ideal case, treatments are independently randomized when being assigned to strips; but in practice this is not always practical. Traditional analysis such as paired sample $t$-tests will not be appropriate when strips are not properly randomized, but the researcher does not always have the option of randomizing treatments; it is frequently easier for the farmer-collaborator to apply strips in a systematic manner as illustrated by Examples 1 and 2. As we see in Example 2, treatment effects may be confounded with spatial patterns in the field, and provide a meaningfully different estimate of treatment effects (i.e. paired $t$-tests) than analysis that take into consideration spatial confounded (i.e. trend analysis).

In Example 2, we see an extreme case of problematic data - not only are the treated strips of different lengths, but the strips pass over areas where crops are not planted or are not viable. Simple analysis like pointwise $t$-tests are not appropriate for such strips, particularly do to the lack of replication over parts of the tested area. We could provide an analysis that excludes areas where strips are not sufficiently replicated, but that may be unsatisfactory to the farmer-collaborator; instead of analyzing only a portion of the treated field the collaborator may wish to have results that cover the entire treated area.

The two main approaches to analysis of on-farm strip trials - functional data analysis and spatial analysis - provide different understanding of the effects of treatment. Functional data analysis allows for some interpretation of results covering different parts of a field - retaining spatial information over the interaction of treatments with different production zones. Spatial analysis sacrifices a broader interpretation of the data, but does provide a single, easily interpreted result. However, spatial model selection criteria are ambiguous. Standard methods of model criticism such as adjusted $R^2$ or cross-validation do not apply easily to spatial models, so there is some judgement to be made on the part of the analyst.

That is, for Example 1, the answer to the question "Did the new product work well enough in the strip trial for the collaborator to adopt the new technology for application to a larger percent of his total acreage?" is quite obvious, even in the absence of statistical analysis. For Example 2, the answer is more ambiguous. For Example 2, at best fungicide spray had no meaningful effect, and in the worst case may have cost the farmer nearly 4 bu/acre.

\chapter{Further Work}

In this thesis, we present the data using two methodologies. The first is based on a classical paired $t$-test. In the presentation of the results of an experiment, we provide graphical equivalents of the mean of paired differences, Figures \ref{Example1Differences}, the associated $t$ statistic (Figure \ref{Example1T}) and associated probabilities (Figure \ref{Example1P}) The second proposed data presentation are single graphs with treatment effect and confidence intervals (e.g. Figures \ref{CIPlotExample1}). The first analysis presentation may be of interest to statisticians who tend to be more confortable with test statistics and $p$-values. However, the results of an analysis should be interpretable by non-statisticians, and the confidence interval graph may be more user friendly. To determine this requires working with end-users, which is beyond the scope of this paper.

In this paper, we also compare different computational engines for smoothing noisy data. We would prefer to use the \texttt{mgcv} library in that is is part of the standard R installation and so can be expected to be a stable implementation. However, in the case of pointwise trend analysis we observe unstable behavior of this library. Similar results can be found using the \texttt{brms} package, but this package depends on the \texttt{stan} library, so is not as user friendly, from the perspective of the coding expertise of typical agronomic researchers. The \texttt{brms} library also implements what is at heart a Bayesian methodology, while most agronomic researchers are trained in frequentist methods, making the output of \texttt{brms} analysis less easily interpreted by classically trained agronomists. However, the \texttt{brms} package makes producing confidence interval plots relatively simple, so if confidence interval plot are preferred by end-users, the arguments against \texttt{brms} are less tenable. If not, the limitations of \texttt{mgcv} warrant further study. This further study may include implementing the algorithms provided by \texttt{mgcv}, as described in \cite{wood-2017}.

The paired $t$-test assumes independence among the pairs experimental units. For this to be achieved in on-farm strip trials, the farmer or researcher must devise maps ahead of product application. In practice, though, the strips are frequently executed in a systematic alternating treatment pattern - indeed, both examples of on-farm strip trials used in this thesis are systematically arranged. We can overcome some of the limitations using trend analysis as demonstrated in this paper, but further work is required to advance the design of strip trials to compare with the sophistication of small-plot trials, where the researcher has more control of the arrangement of treatments. This work will require collaboration with existing and on-going on farm strip trial programs, such as the South Dakota Soybean On-Farm Research Program (\texttt{https://onfarmresearch.sdsoybean.org}).

In addition to pointwise functional data analysis, we propose a response surface analysis of strip trials. The functional data analysis retains some spatial information, by providing estimates and statistics for a set of strips, and the spatial variance along the length of a cropland is retained in the analysis. This requires several analysis steps, including the smoothing of individual harvest passes. In this paper, we strive to use the default smoothing functions provided by \texttt{mgcv} and ignore the potential benefits of fitting each harvest pass individually prior to using the pass as a functional data unit. It may be that more precise comparisons can be made between experimental treatments if smoothing functions for harvest passes are fine-tuned for each pass, individually. We propose an alternative analysis method - response surface - that provides a single estimate for treatment effect summarized over an entire cropland, but we also provide methods based on spatial properties of cropland - the semivariogram - to fine-tune response surface models. We have yet to provide analogous spatial adjustments for functional data models. 

One of our strip trials, Example 2, is a poor candidate for functional data analysis largely due to the lack of uniformity in the size of treated strips (it is also secondarily unsuitable due to missing yield points due to unplantable regions in the cropland). We present response surface analysis as on option to provide an alternative to functional data analysis for non-uniform data. However, this recommendation is based only on the single example. We would like to find further examples of strip trials that are not uniform (as in Example 1) and further explore the behavior of response surface models when yield data is not in the form of a filled rectangle.

Finally, this paper assumes that the underlying data generating process - the yield of crop plants in a planted strip - is continuous; that is, there are no "jumps" in yield by position. This may not always be true. There can be planting errors that leave small unplanted gaps in the otherwise continuous orderly arrangement of crop plants. There may also be gaps introduced in harvesting, if the harvester is stopped and restarted in the middle of a harvest strip. Storath and Weinmann \cite{storath-11-2023} recently proposed a methodology for fitting splines to discontinuous data; these methods have not been explored for fitting to harvest data.


\renewcommand\bibname{References}
\clearpage
\bibliographystyle{plain}
%\bibliographystyle{plainyr} % to sort references chronologically
%% To get bib in PDF bookmarks:
\phantomsection
%% if you put "Chapters" and "Appendices" labels in TOC:
\addcontentsline{toc}{part}{\bibname}

\bibliography{biblio}

\appendix
\clearpage 
\phantomsection
\addcontentsline{toc}{part}{Appendices}

\end{document}

%\begin{figure}\centering
%<<,fig=TRUE,echo=FALSE, width=8, height=6>>=

%@
%\caption{}\label{}
%\end{figure}

